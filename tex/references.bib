@article{peng2018sfv,
  title     = {{SFV: Reinforcement Learning of Physical Skills from Videos}},
  author    = {Peng, Xue Bin and Kanazawa, Angjoo and Malik, Jitendra and Abbeel, Pieter and Levine, Sergey},
  year      = 2018,
  month     = nov,
  journal   = {ACM Trans. Graph.},
  publisher = {ACM},
  address   = {New York, NY, USA},
  volume    = 37,
  number    = 6,
  articleno = 178,
  numpages  = 14,
  keywords  = {physics-based character animation, computer vision, video imitation, reinforcement learning, motion reconstruction}
}
@article{bertinetto2016siamfc,
  title         = {{Fully-convolutional siamese networks for object tracking}},
  author        = {Bertinetto, Luca and Valmadre, Jack and Henriques, Jo{\~{a}}o F. and Vedaldi, Andrea and Torr, Philip H.S.},
  year          = 2016,
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume        = {9914 LNCS},
  pages         = {850--865},
  doi           = {10.1007/978-3-319-48881-3_56},
  isbn          = 9783319488806,
  issn          = 16113349,
  abstract      = {The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object's appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.},
  archiveprefix = {arXiv},
  arxivid       = {1606.09549},
  eprint        = {1606.09549},
  keywords      = {Deep-learning,Object-tracking,Siamese-network,Similarity-learning},
  pmid          = 4520227
}
@article{bertinetto2016oneshot,
  title         = {{Learning feed-forward one-shot learners}},
  author        = {Bertinetto, Luca and Henriques, Jo{\~{a}}o F. and Valmadre, Jack and Torr, Philip H.S. and Vedaldi, Andrea},
  year          = 2016,
  journal       = {Advances in Neural Information Processing Systems},
  number        = {Nips},
  pages         = {523--531},
  issn          = 10495258,
  abstract      = {One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.},
  archiveprefix = {arXiv},
  arxivid       = {1606.05233},
  eprint        = {1606.05233}
}
@article{bawley2016simple,
  title         = {{Simple online and realtime tracking}},
  author        = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  year          = 2016,
  journal       = {Proceedings - International Conference on Image Processing, ICIP},
  volume        = {2016-Augus},
  pages         = {3464--3468},
  doi           = {10.1109/ICIP.2016.7533003},
  isbn          = 9781467399616,
  issn          = 15224880,
  abstract      = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9{\%}. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
  archiveprefix = {arXiv},
  arxivid       = {1602.00763},
  eprint        = {1602.00763},
  keywords      = {Computer Vision,Data Association,Detection,Multiple Object Tracking}
}
@article{bodla2017softnms,
  title         = {{Soft-NMS - Improving Object Detection with One Line of Code}},
  author        = {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S.},
  year          = 2017,
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  volume        = {2017-October},
  pages         = {5562--5570},
  doi           = {10.1109/ICCV.2017.593},
  isbn          = 9781538610329,
  issn          = 15505499,
  abstract      = {Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-NMS obtains consistent improvements for the coco-style mAP metric on standard datasets like PASCAL VOC2007 (1.7{\%} for both R-FCN and Faster-RCNN) and MS-COCO (1.3{\%} for R-FCN and 1.1{\%} for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves state-of-the-art in object detection from 39.8{\%} to 40.9{\%} with a single model. Further, the computational complexity of Soft-NMS is the same as traditional NMS and hence it can be efficiently implemented. Since Soft-NMS does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for Soft-NMS is publicly available on GitHub http://bit.ly/2nJLNMu.},
  archiveprefix = {arXiv},
  arxivid       = {1704.04503},
  eprint        = {1704.04503}
}
@article{comaniciu2003kernel,
  title   = {{Kernel-based object tracking}},
  author  = {Comaniciu, D. and Ramesh, V. and Meer, P.},
  year    = 2003,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume  = 25,
  number  = 5,
  pages   = {564--577},
  doi     = {10.1109/TPAMI.2003.1195991}
}
@misc{webgradaccumulation,
  title        = {{Gradient Accumulation}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html}}
}
@misc{webhomographyrankinggithub,
  title        = {{Homography Ranking}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://github.com/mondrasovic/homography_ranking}}
}
@misc{webcompcarsdataset,
  title        = {{CompCars}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{http://ai.stanford.edu/~jkrause/cars/car_dataset.html}}
}
@article{cortes1995support,
  title     = {{Support Vector Networks}},
  author    = {Cortes, C. and Vapnik, V.},
  year      = 1995,
  journal   = {Machine Learning},
  pages     = {273--297},
  added-at  = {2015-09-17T16:32:09.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/22b1eb8bea07ae0156a53a4e9c6eac1df/nosebrain},
  interhash = {c223c465141618ad63aac5a6132280f7},
  intrahash = {2b1eb8bea07ae0156a53a4e9c6eac1df},
  keywords  = {classification margin soft support svm vector},
  timestamp = {2015-09-17T17:15:55.000+0200}
}
@article{wen2020uadetrac,
  title   = {{ UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking}},
  author  = {Longyin Wen and Dawei Du and Zhaowei Cai and Zhen Lei and Ming{-}Ching Chang and others},
  year    = 2020,
  journal = {Computer Vision and Image Understanding}
}
@article{davis2016prcurves,
  title   = {{The relationship between Precision-Recall and ROC curves}},
  author  = {Jesse Davis and Mark H. Goadrich},
  year    = 2006,
  journal = {Proceedings of the 23rd international conference on Machine learning}
}
@article{finn2017oneshotimitation,
  title         = {{One-Shot Visual Imitation Learning via Meta-Learning}},
  author        = {Chelsea Finn and Tianhe Yu and Tianhao Zhang and Pieter Abbeel and Sergey Levine},
  year          = 2017,
  journal       = {CoRR},
  volume        = {abs/1709.04905},
  url           = {http://arxiv.org/abs/1709.04905},
  archiveprefix = {arXiv},
  eprint        = {1709.04905},
  timestamp     = {Mon, 13 Aug 2018 16:47:36 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1709-04905.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{dendorfer2019cvpr,
  title      = {{CVPR19 Tracking and Detection Challenge: How crowded can it get?}},
  author     = {Patrick Dendorfer and Seyed Hamid Rezatofighi and Anton Milan and Javen Shi and Daniel Cremers and others},
  year       = 2019,
  journal    = {CoRR},
  volume     = {abs/1906.04567},
  url        = {http://arxiv.org/abs/1906.04567},
  eprinttype = {arXiv},
  eprint     = {1906.04567},
  timestamp  = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1906-04567.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@article{everingham2010pascalvoc,
  title   = {{The Pascal Visual Object Classes (VOC) Challenge}},
  author  = {Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},
  year    = 2010,
  month   = jun,
  journal = {International Journal of Computer Vision},
  volume  = 88,
  number  = 2,
  pages   = {303--338}
}
@book{forsyth2012computer,
  title     = {{Computer Vision - A Modern Approach, Second Edition.}},
  author    = {Forsyth, David A. and Ponce, Jean},
  year      = 2012,
  publisher = {Pitman},
  pages     = {1--791},
  isbn      = {978-0-273-76414-4},
  added-at  = {2015-03-23T00:00:00.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/24e26ef656dc22317d8ed6d0cfad2d3bc/dblp},
  ee        = {http://vig.pearsoned.com/store/product/1,1207,store-12521_isbn-013608592X,00.html},
  interhash = {a4dfa38179479650cae77eb72b893bb6},
  intrahash = {4e26ef656dc22317d8ed6d0cfad2d3bc},
  keywords  = {dblp},
  timestamp = {2015-06-18T09:49:18.000+0200}
}
@book{franoischollet2017learning,
  title     = {{Deep Learning with Python }},
  author    = {Chollet, François},
  year      = 2017,
  month     = nov,
  publisher = {Manning},
  isbn      = 9781617294433,
  added-at  = {2018-08-01T08:16:18.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/231f94815ebbd65d3a31e4a69e818573e/jaeschke},
  interhash = {cfbfd3f93853a469e5e6978f61a74a0a},
  intrahash = {31f94815ebbd65d3a31e4a69e818573e},
  keywords  = {ai deeplearning ml},
  timestamp = {2018-08-01T08:16:18.000+0200}
}
@inproceedings{gabriel2003sotamot,
  title     = {{The state of the art in multiple object tracking under occlusion in video sequences}},
  author    = {Gabriel, Pierre F and Verly, Jacques G and Piater, Justus H and Genon, Andr{\'e}},
  year      = 2003,
  booktitle = {Advanced Concepts for Intelligent Vision Systems},
  pages     = {166--173}
}
@inproceedings{geiger2012cvpr,
  title     = {{Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite}},
  author    = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
  year      = 2012,
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@book{goodfellow2016dl,
  title     = {{Deep Learning}},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  year      = 2016,
  publisher = {MIT Press}
}
@article{goodfellow2014gans,
  title         = {{Generative adversarial nets}},
  author        = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and others},
  year          = 2014,
  journal       = {Advances in Neural Information Processing Systems},
  volume        = 3,
  number        = {January},
  pages         = {2672--2680},
  doi           = {10.3156/jsoft.29.5_177_2},
  issn          = 10495258,
  abstract      = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to {\textless}sup{\textgreater}1{\textless}/sup{\textgreater}/{\textless}inf{\textgreater}2{\textless}/inf{\textgreater} everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  arxivid       = {1406.2661},
  eprint        = {1406.2661}
}
@misc{webgot10kdataset,
  title        = {{GOT-10k}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{http://got-10k.aitestunion.com/}}
}
@misc{guo2019siamcar,
  title         = {{SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking}},
  author        = {Dongyan Guo and Jun Wang and Ying Cui and Zhenhua Wang and Shengyong Chen},
  year          = 2019,
  eprint        = {1911.07241},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{hadsell2006dimreduction,
  title    = {{Dimensionality reduction by learning an invariant mapping}},
  author   = {Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
  year     = 2006,
  journal  = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume   = 2,
  pages    = {1735--1742},
  doi      = {10.1109/CVPR.2006.100},
  isbn     = {0769525970},
  issn     = 10636919,
  abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that "similar" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distance measure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular ILE. {\textcopyright} 2006 IEEE.}
}
@article{he2018twofoldsiam,
  title         = {{A Twofold Siamese Network for Real-Time Object Tracking}},
  author        = {He, Anfeng and Luo, Chong and Tian, Xinmei and Zeng, Wenjun},
  year          = 2018,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages         = {4834--4843},
  doi           = {10.1109/CVPR.2018.00508},
  isbn          = 9781538664209,
  issn          = 10636919,
  abstract      = {Observing that Semantic features learned in an image classification task and Appearance features learned in a similarity matching task complement each other, we build a twofold Siamese network, named SA-Siam, for real-time object tracking. SA-Siam is composed of a semantic branch and an appearance branch. Each branch is a similaritylearning Siamese network. An important design choice in SA-Siam is to separately train the two branches to keep the heterogeneity of the two types of features. In addition, we propose a channel attention mechanism for the semantic branch. Channel-wise weights are computed according to the channel activations around the target position. While the inherited architecture from SiamFC [3] allows our tracker to operate beyond real-time, the twofold design and the attention mechanism significantly improve the tracking performance. The proposed SA-Siam outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks.},
  archiveprefix = {arXiv},
  arxivid       = {1802.08817},
  eprint        = {1802.08817}
}
@article{held2016goturn,
  title         = {{Learning to Track at 100 FPS with Deep}},
  author        = {Held, David and Thrun, Sebastian and Savarese, Silvio},
  year          = 2016,
  journal       = {Computer Vision – ECCV 2016 Lecture Notes in Computer Science},
  pages         = {749--765},
  url           = {http://davheld.github.io/GOTURN/GOTURN.html},
  abstract      = {Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance. Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training. We propose a method for offline training of neural networks that can track novel objects at test-time at 100 fps. Our tracker is significantly faster than previous methods that use neural networks for tracking, which are typically very slow to run and not practical for real-time applications. Our tracker uses a simple feed-forward network with no online training required. The tracker learns a generic relationship between object motion and appearance and can be used to track novel objects that do not appear in the training set. We test our network on a standard tracking benchmark to demonstrate our tracker's state-of-the-art performance. Further, our performance improves as we add more videos to our offline training set. To the best of our knowledge, our tracker 1 is the first neural-network tracker that learns to track generic objects at 100 fps.},
  archiveprefix = {arXiv},
  arxivid       = {1604.01802},
  eprint        = {1604.01802},
  keywords      = {deep learning,machine learning,neural networks,tracking}
}
@misc{hermans2017triplet,
  title         = {{In Defense of the Triplet Loss for Person Re-Identification}},
  author        = {Alexander Hermans and Lucas Beyer and Bastian Leibe},
  year          = 2017,
  eprint        = {1703.07737},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{hochreiter1997lstm,
  title    = {{Long Short-Term Memory}},
  author   = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
  year     = 1997,
  journal  = {Neural Computation},
  volume   = 9,
  number   = 8,
  pages    = {1735--1780},
  doi      = {10.1162/neco.1997.9.8.1735},
  issn     = {08997667},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  pmid     = 9377276
}
@article{hosang2017learningnms,
  title         = {{Learning non-maximum suppression}},
  author        = {Hosang, Jan and Benenson, Rodrigo and Schiele, Bernt},
  year          = 2017,
  journal       = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume        = {2017-January},
  pages         = {6469--6477},
  doi           = {10.1109/CVPR.2017.685},
  isbn          = 9781538604571,
  abstract      = {Object detectors have hugely profited from moving towards an end-to-end learning paradigm: proposals, features, and the classifier becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard NMS algorithm is still fully hand-crafted, suspiciously simple, and - being based on greedy clustering with a fixed distance threshold - forces a trade-off between recall and precision. We propose a new network architecture designed to perform NMS, using only boxes and their score. We report experiments for person detection on PETS and for general object categories on the COCO dataset. Our approach shows promise providing improved localization and occlusion handling.},
  archiveprefix = {arXiv},
  arxivid       = {1705.02950},
  eprint        = {1705.02950}
}
@article{huang2017speedacctradeoff,
  title         = {{Speed/accuracy trade-offs for modern convolutional object detectors}},
  author        = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and others},
  year          = 2017,
  journal       = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume        = {2017-Janua},
  pages         = {3296--3305},
  doi           = {10.1109/CVPR.2017.351},
  isbn          = 9781538604571,
  abstract      = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
  archiveprefix = {arXiv},
  arxivid       = {1611.10012},
  eprint        = {1611.10012}
}
@article{huang2021got10k,
  title   = {{GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild}},
  author  = {Huang, Lianghua and Zhao, Xin and Huang, Kaiqi},
  year    = 2021,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume  = 43,
  number  = 5,
  pages   = {1562--1577},
  doi     = {10.1109/TPAMI.2019.2957464}
}
@misc{chen2019rotbboxes,
  title         = {{Fast Visual Object Tracking with Rotated Bounding Boxes}},
  author        = {Bao Xin Chen and John K. Tsotsos},
  year          = 2019,
  eprint        = {1907.03892},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{deng2009imagenet,
  title     = {{ImageNet: A Large-Scale Hierarchical Image Database}},
  author    = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and others},
  year      = 2009,
  booktitle = {CVPR09},
  bibsource = {http://www.image-net.org/papers/deng2009imagenet.bib}
}
@article{jalal2012sotavot,
  title   = {{The State-of-the-Art in Visual Object Tracking.}},
  author  = {Jalal, Anand and Singh, Vrijendra},
  year    = 2012,
  month   = {01},
  journal = {Informatica (Slovenia)},
  volume  = 36,
  pages   = {227--248},
  issn    = {03505596}
}
@article{jiyan2007robustocclusion,
  title    = {{Robust occlusion handling in object tracking}},
  author   = {Jiyan, Pan and Bo, Hu},
  year     = 2007,
  journal  = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  number   = {June 2007},
  doi      = {10.1109/CVPR.2007.383453},
  isbn     = 1424411807,
  issn     = 10636919,
  abstract = {In object tracking, occlusions significantly undermine the performance of tracking algorithms. Unlike the existing methods that solely depend on the observed target appearance to detect occluders, we propose an algorithm that progressively analyzes the occlusion situation by exploiting the spatiotemporal context information, which is further double checked by the reference target and motion constraints. This strategy enables our proposed algorithm to make a clearer distinction between the target and occluders than existing approaches. To further improve the tracking performance, we rectify the occlusion-interfered erroneous target location by employing a variant-mask template matching operation. As a result, correct target location can always be obtained regardless of the occlusion situation. Using these techniques, the robustness of tracking under occlusions is significantly promoted. Experimental results have confirmed the effectiveness of our proposed algorithm. {\textcopyright} 2007 IEEE.}
}
@article{kalman1960linearfilter,
  title    = {{A new approach to linear filtering and prediction problems}},
  author   = {Kalman, R. E.},
  year     = 1960,
  journal  = {Journal of Fluids Engineering, Transactions of the ASME},
  volume   = 82,
  number   = 1,
  pages    = {35--45},
  doi      = {10.1115/1.3662552},
  issn     = {1528901X},
  abstract = {The classical filtering and prediction problem is re-examined using the Bode-Sliannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinitememory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix. {\textcopyright} 1960 by ASME.}
}
@article{karras2020stylegan,
  title         = {{A Style-Based Generator Architecture for Generative Adversarial Networks}},
  author        = {Karras, Tero and Laine, Samuli and Aila, Timo},
  year          = 2020,
  pages         = {4396--4405},
  doi           = {10.1109/cvpr.2019.00453},
  url           = {http://arxiv.org/abs/1812.04948},
  abstract      = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  archiveprefix = {arXiv},
  arxivid       = {1812.04948},
  eprint        = {1812.04948}
}
@misc{webkittiobjdetectiondataset,
  title        = {{KITTI Object Detection}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d}}
}
@misc{webkittiobjtrackingdataset,
  title        = {{KITTI Object Tracking}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{http://www.cvlibs.net/datasets/kitti/eval_tracking.php}}
}
@inproceedings{koch2015siameseoneshot,
  title  = {{Siamese Neural Networks for One-Shot Image Recognition}},
  author = {Gregory R. Koch},
  year   = 2015
}
@article{kristan2018vot18,
  title    = {{VOT2018 results}},
  author   = {Kristan, Matej and Leonardis, Ale{\v{s}} and Matas, Jiř{\'{i}} and Felsberg, Michael and Pflugfelder, Roman and others},
  year     = 2018,
  journal  = {Chinese Academy of Sciences},
  volume   = 26,
  number   = 1,
  pages    = {1--15},
  url      = {http://vision.fe.uni-lj.si/cvbase06/},
  abstract = {The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative. Results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis and a "real-time" experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. A long-term tracking sub-challenge has been introduced to the set of standard VOT sub-challenges. The new subchallenge focuses on long-term tracking properties, namely coping with target disappearance and reappearance. A new dataset has been compiled and a performance evaluation methodology that focuses on long-term tracking capabilities has been adopted. The VOT toolkit has been updated to support both standard short-term and the new long-term tracking subchallenges. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the track-ers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website 60 .}
}
@misc{kristan2019motyolovot19,
  title  = {{The Seventh Visual Object Tracking VOT2019 Challenge Results}},
  author = {Matej Kristan and Jiri Matas and Ales Leonardis and Michael Felsberg and Roman Pflugfelder and others},
  year   = 2019
}
@article{krizhevsky2012classification,
  title    = {{ImageNet classification with deep convolutional neural networks}},
  author   = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year     = 2012,
  journal  = {Advances in Neural Information Processing Systems},
  volume   = 2,
  pages    = {1097--1105},
  isbn     = 9781627480031,
  issn     = 10495258,
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.}
}
@article{kuhn1995hungarian,
  title   = {{The Hungarian method for the assignment problem}},
  author  = {H. W. Kuhn and Bryn Yaw},
  year    = 1955,
  journal = {Naval Res. Logist. Quart},
  pages   = {83--97}
}
@article{kuma2019vehiclereid,
  title         = {{Vehicle Re-identification: An Efficient Baseline Using Triplet Embedding}},
  author        = {Kuma, Ratnesh and Weill, Edwin and Aghdasi, Farzin and Sriram, Parthasarathy},
  year          = 2019,
  journal       = {Proceedings of the International Joint Conference on Neural Networks},
  volume        = {2019-July},
  doi           = {10.1109/IJCNN.2019.8852059},
  isbn          = 9781728119854,
  abstract      = {In this paper we tackle the problem of vehicle re-identification in a camera network utilizing triplet embeddings. Re-identification is the problem of matching appearances of objects across different cameras. With the proliferation of surveillance cameras enabling smart and safer cities, there is an ever-increasing need to re-identify vehicles across cameras. Typical challenges arising in smart city scenarios include variations of viewpoints, illumination and self occlusions. Most successful approaches for re-identification involve (deep) learning an embedding space such that the vehicles of same identities are projected closer to one another, compared to the vehicles representing different identities. Popular loss functions for learning an embedding (space) include contrastive or triplet loss. In this paper we provide an extensive evaluation of these losses applied to vehicle re-identification and demonstrate that using the best practices for learning embeddings outperform most of the previous approaches proposed in the vehicle re-identification literature. Compared to most existing state-of-the-art approaches, our approach is simpler and more straightforward for training utilizing only identity-level annotations, along with one of the smallest published embedding dimensions for efficient inference. Furthermore in this work we introduce a formal evaluation of a triplet sampling variant (batch sample) into the re-identification literature.},
  archiveprefix = {arXiv},
  arxivid       = {1901.01015},
  eprint        = {1901.01015}
}
@misc{lealtaixe2017tracking,
  title         = {{Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking}},
  author        = {Laura Leal-Taixé and Anton Milan and Konrad Schindler and Daniel Cremers and Ian Reid and others},
  year          = 2017,
  eprint        = {1704.02781},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{li2018siamrpn,
  title    = {{High Performance Visual Tracking with Siamese Region Proposal Network}},
  author   = {Li, Bo and Yan, Junjie and Wu, Wei and Zhu, Zheng and Hu, Xiaolin},
  year     = 2018,
  journal  = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages    = {8971--8980},
  doi      = {10.1109/CVPR.2018.00935},
  isbn     = 9781538664209,
  issn     = 10636919,
  abstract = {Visual object tracking has been a fundamental topic in recent years and many deep learning based trackers have achieved state-of-the-art performance on multiple benchmarks. However, most of these trackers can hardly get top performance with real-time speed. In this paper, we propose the Siamese region proposal network (Siamese-RPN) which is end-to-end trained off-line with large-scale image pairs. Specifically, it consists of Siamese subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. In the inference phase, the proposed framework is formulated as a local one-shot detection task. We can pre-compute the template branch of the Siamese subnetwork and formulate the correlation layers as trivial convolution layers to perform online tracking. Benefit from the proposal refinement, traditional multi-scale test and online fine-tuning can be discarded. The Siamese-RPN runs at 160 FPS while achieving leading performance in VOT2015, VOT2016 and VOT2017 real-time challenges.}
}
@article{lin2014mscoco,
  title         = {{Microsoft COCO: Common objects in context}},
  author        = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and others},
  year          = 2014,
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume        = {8693 LNCS},
  number        = {PART 5},
  pages         = {740--755},
  doi           = {10.1007/978-3-319-10602-1_48},
  issn          = 16113349,
  abstract      = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
  archiveprefix = {arXiv},
  arxivid       = {1405.0312},
  eprint        = {1405.0312}
}
@article{lin2014netinnet,
  title         = {{Network in network}},
  author        = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  year          = 2014,
  journal       = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
  pages         = {1--10},
  abstract      = {We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1312.4400v3},
  eprint        = {arXiv:1312.4400v3}
}
@article{liu2016ssd,
  title         = {{SSD: Single shot multibox detector}},
  author        = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and others},
  year          = 2016,
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume        = {9905 LNCS},
  pages         = {21--37},
  doi           = {10.1007/978-3-319-46448-0_2},
  isbn          = 9783319464473,
  issn          = 16113349,
  abstract      = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300 input, SSD achieves 74.3{\%} mAP on VOC2007 test at 59 FPS on a webnvidia Titan X and for 512 × 512 input, SSD achieves 76.9{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/ tree/ssd.},
  archiveprefix = {arXiv},
  arxivid       = {1512.02325},
  eprint        = {1512.02325},
  keywords      = {Convolutional neural network,Real-time object detection}
}
@inproceedings{liu2016deepreldist,
  title     = {{Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles}},
  author    = {Liu, Hongye and Tian, Yonghong and Wang, Yaowei and Pang, Lu and Huang, Tiejun},
  year      = 2016,
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {2167--2175}
}
@article{liu2018provid,
  title     = {{PROVID: Progressive and Multimodal Vehicle Reidentification for Large-Scale Urban Surveillance}},
  author    = {Liu, Xinchen and Liu, Wu and Mei, Tao and Ma, Huadong},
  year      = 2018,
  journal   = {IEEE Transactions on Multimedia},
  publisher = {IEEE},
  volume    = 20,
  number    = 3,
  pages     = {645--658},
  doi       = {10.1109/TMM.2017.2751966},
  issn      = 15209210,
  abstract  = {Compared with person reidentification, which has attracted concentrated attention, vehicle reidentification is an important yet frontier problem in video surveillance and has been neglected by the multimedia and vision communities. Since most existing approaches mainly consider the general vehicle appearance for reidentification while overlooking the distinct vehicle identifier, such as the license plate number, they attain suboptimal performance. In this paper, we propose PROVID, a PROgressive Vehicle re-IDentification framework based on deep neural networks. In particular, our framework not only utilizes the multimodality data in large-scale video surveillance, such as visual features, license plates, camera locations, and contextual information, but also considers vehicle reidentification in two progressive procedures: coarse-To-fine search in the feature domain, and near-To-distant search in the physical space. Furthermore, to evaluate our progressive search framework and facilitate related research, we construct the VeRi dataset, which is the most comprehensive dataset from real-world surveillance videos. It not only provides large numbers of vehicles with varied labels and sufficient cross-camera recurrences but also contains license plate numbers and contextual information. Extensive experiments on the VeRi dataset demonstrate both the accuracy and efficiency of our progressive vehicle reidentification framework.},
  keywords  = {Progressive search,contextual information,deep learning,license plate verification,vehicle re-identification}
}
@article{lou2019embreid,
  title     = {{Embedding Adversarial Learning for Vehicle Re-Identification}},
  author    = {Lou, Yihang and Bai, Yan and Liu, Jun and Wang, Shiqi and Duan, Ling Yu},
  year      = 2019,
  journal   = {IEEE Transactions on Image Processing},
  publisher = {IEEE},
  volume    = 28,
  number    = 8,
  pages     = {3794--3807},
  doi       = {10.1109/TIP.2019.2902112},
  issn      = 19410042,
  abstract  = {The high similarities of different real-world vehicles and great diversities of the acquisition views pose grand challenges to vehicle re-identification (ReID), which traditionally maps the vehicle images into a high-dimensional embedding space for distance optimization, vehicle discrimination, and identification. To improve the discriminative capability and robustness of the ReID algorithm, we propose a novel end-to-end embedding adversarial learning network (EALN) that is capable of generating samples localized in the embedding space. Instead of selecting abundant hard negatives from the training set, which is extremely difficult if not impossible, with our embedding adversarial learning scheme, the automatically generated hard negative samples in the specified embedding space can greatly improve the capability of the network for discriminating similar vehicles. Moreover, the more challenging cross-view vehicle ReID problem, which requires the ReID algorithm to be robust with different query views, can also benefit from such a scheme based on the artificially generated cross-view samples. We demonstrate the promise of EALN through extensive experiments and show the effectiveness of hard negative and cross-view generation in facilitating vehicle ReID based on the comparisons with the state-of-the-art schemes.},
  keywords  = {Vehicle Re-Identification,cross-view,embedding adversarial learning,generative adversarial network,hard negatives}
}
@article{lowel1999objrecognition,
  title    = {{Object recognition from local scale-invariant features}},
  author   = {Lowe, David G.},
  year     = 1999,
  journal  = {Proceedings of the IEEE International Conference on Computer Vision},
  volume   = 2,
  pages    = {1150--1157},
  doi      = {10.1109/iccv.1999.790410},
  abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.}
}
@article{manmatha2017samplingmatters,
  title         = {{Sampling Matters in Deep Embedding Learning}},
  author        = {Manmatha, R. and Wu, Chao Yuan and Smola, Alexander J. and Krahenbuhl, Philipp},
  year          = 2017,
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  volume        = {2017-Octob},
  pages         = {2859--2867},
  doi           = {10.1109/ICCV.2017.309},
  isbn          = 9781538610329,
  issn          = 15505499,
  abstract      = {Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.},
  archiveprefix = {arXiv},
  arxivid       = {1706.07567},
  eprint        = {1706.07567}
}
@misc{webmotchallenge,
  title        = {{MOT challenge}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://webmotchallenge.net/}}
}
@misc{webmscocodataset,
  title        = {{MS COCO}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://cocodataset.org/}}
}
@inproceedings{ng2002discriminative,
  title     = {{On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes}},
  author    = {Ng, Andrew Y and Jordan, Michael I},
  year      = 2002,
  booktitle = {Advances in neural information processing systems},
  pages     = {841--848}
}
@misc{websensors,
  title        = {{websensors}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://www.mdpi.com/journal/websensors}}
}
@misc{webmistconf,
  title        = {{MiST}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://mist-klak.webnode.sk/}}
}
@misc{webhomographybasiccode,
  title        = {{Homography - basic concepts}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{http://man.hubwiz.com/docset/OpenCV.docset/Contents/Resources/Documents/d9/dab/tutorial_homography.html}}
}
@misc{webpymotmetrics,
  title        = {{webpymotmetrics}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://github.com/cheind/py-motmetrics}}
}
@misc{websiammotforkgithub,
  title        = {{SiamMOT - GitHub (forked project)}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://github.com/mondrasovic/siam-mot}}
}
@misc{websiammotoriggithub,
  title        = {{SiamMOT - GitHub (original project)}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://github.com/amazon-research/siam-mot}}
}
@misc{webnvidia,
  title        = {{webnvidia}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://www.webnvidia.com/en-us/}}
}
@article{wu2015otb,
  title   = {{Object Tracking Benchmark}},
  author  = {Wu, Yi and Lim, Jongwoo and Yang, Ming-Hsuan},
  year    = 2015,
  month   = {09},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume  = 37,
  pages   = {1--1},
  doi     = {10.1109/TPAMI.2014.2388226}
}
@article{ondrasovic2021siamese,
  title   = {{Siamese Visual Object Tracking: A Survey}},
  author  = {Ondrašovič, Milan and Tarábek, Peter},
  year    = 2021,
  journal = {IEEE Access},
  volume  = 9,
  pages   = {110149--110172},
  doi     = {10.1109/ACCESS.2021.3101988}
}
@article{bradski2000opencv,
  title                = {{The OpenCV Library}},
  author               = {Bradski, G.},
  year                 = 2000,
  journal              = {Dr. Dobb's Journal of Software Tools},
  citeulike-article-id = 2236121,
  keywords             = {bibtex-import},
  posted-at            = {2008-01-15 19:21:54},
  priority             = 4
}
@inproceedings{barath2016novel,
  title     = {Novel Ways to Estimate Homography from Local Affine Transformations},
  author    = {Daniel Barath and Levente Hajder},
  year      = 2016,
  booktitle = {Proceedings of the 11th Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications {(VISIGRAPP} 2016) - Volume 3},
  pages     = {434--445},
  doi       = {10.5220/0005674904320443},
  url       = {https://doi.org/10.5220/0005674904320443},
  editor    = {Nadia Magnenat{-}Thalmann and Paul Richard and Lars Linsen and Alexandru C. Telea and Sebastiano Battiato and Francisco H. Imai and Jos{\'{e}} Braz},
  timestamp = {Wed, 17 Mar 2021 18:00:56 +0100},
  biburl    = {https://dblp.org/rec/conf/visapp/BarathH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@phdthesis{beck2016planar,
  title  = {Planar Homography Estimation from Traffic Streams via Energy Functional Minimization},
  author = {Beck, Graham and others},
  year   = 2016,
  school = {Johns Hopkins University}
}
@article{abdel2015direct,
  title   = {Direct Linear Transformation from Comparator Coordinates into Object Space Coordinates in Close-Range Photogrammetry*},
  author  = {Y.I Abdel-Aziz and H.M. Karara and Michael Hauck},
  year    = 2015,
  journal = {Photogrammetric Engineering \& Remote Sensing},
  volume  = 81,
  number  = 2,
  pages   = {103--107},
  doi     = {https://doi.org/10.14358/PERS.81.2.103},
  issn    = {0099-1112},
  url     = {https://www.sciencedirect.com/science/article/pii/S0099111215303086}
}
@article{mcconnell1986osti,
  title        = {{Method of and apparatus for pattern recognition}},
  author       = {McConnell, R K},
  year         = 1986,
  month        = 1,
  abstractnote = {This patent describes a method of recognizing a pattern in a test object. The method consists of: specifying properties characteristic of the pattern; specifying discrete ranges of values of the properties; measuring the values of the properties in the test object; arranging the measured values in at least one test histogram; determining a reference set of values of the properties and arranging the set as at least a first reference histogram; and comparing the test and reference histograms by determination of the value of a function which provides a measure of the amount of information necessary to express the at least one test histogram in terms of the optimum code for describing at least the first reference histogram.},
  place        = {United States}
}
@article{parkhi2015deepface,
  title    = {{Deep Face Recognition}},
  author   = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew},
  year     = 2015,
  number   = {Section 3},
  pages    = {41.1--41.12},
  doi      = {10.5244/c.29.41},
  abstract = {The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M im- ages, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present meth- ods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.}
}
@misc{webpkuvehicledataset,
  title        = {{PKU VehicleID}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://www.pkuml.org/resources/pku-vehicleid.html}}
}
@article{redmon2016yolo,
  title         = {{You only look once: Unified, real-time object detection}},
  author        = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year          = 2016,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {2016-Decem},
  pages         = {779--788},
  doi           = {10.1109/CVPR.2016.91},
  isbn          = 9781467388504,
  issn          = 10636919,
  abstract      = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  archiveprefix = {arXiv},
  arxivid       = {1506.02640},
  eprint        = {1506.02640}
}
@article{redmon2017yolo9000,
  title         = {{YOLO9000: Better, faster, stronger}},
  author        = {Redmon, Joseph and Farhadi, Ali},
  year          = 2017,
  journal       = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume        = {2017-January},
  pages         = {6517--6525},
  doi           = {10.1109/CVPR.2017.690},
  isbn          = 9781538604571,
  abstract      = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
  archiveprefix = {arXiv},
  arxivid       = {1612.08242},
  eprint        = {1612.08242}
}
@misc{redmon2018yolov3,
  title         = {{YOLOv3: An Incremental Improvement}},
  author        = {Joseph Redmon and Ali Farhadi},
  year          = 2018,
  eprint        = {1804.02767},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{ren2017fasterrcnn,
  title         = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
  author        = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year          = 2017,
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume        = 39,
  number        = 6,
  pages         = {1137--1149},
  doi           = {10.1109/TPAMI.2016.2577031},
  issn          = {01628828},
  abstract      = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arXiv},
  arxivid       = {1506.01497},
  eprint        = {1506.01497},
  file          = {:E$\backslash$:/Data/Downloads/1506.01497.pdf:pdf},
  keywords      = {Object detection,convolutional neural network,region proposal},
  pmid          = 27295650
}
@article{rosenblatt1958perceptron,
  title   = {{The perceptron: a probabilistic model for information storage and organization in the brain.}},
  author  = {Frank F. Rosenblatt},
  year    = 1958,
  journal = {Psychological review},
  volume  = {65 6},
  pages   = {386--408}
}
@incollection{rumelhart1986backprop,
  title                = {{Learning Internal Representations by Error Propagation}},
  author               = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year                 = 1986,
  booktitle            = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, {V}olume 1: {F}oundations},
  publisher            = {MIT Press},
  address              = {Cambridge, MA},
  pages                = {318--362},
  added-at             = {2008-02-26T11:58:58.000+0100},
  biburl               = {https://www.bibsonomy.org/bibtex/27c3d39c519530239660d33e66493ade1/schaul},
  citeulike-article-id = 2378884,
  description          = {idsia},
  editor               = {Rumelhart, David E. and Mcclelland, James L.},
  interhash            = {dd8485b30b80c7f35263bcb21ed81c1f},
  intrahash            = {7c3d39c519530239660d33e66493ade1},
  keywords             = {nn},
  priority             = 2,
  timestamp            = {2008-02-26T12:02:43.000+0100}
}
@book{salton1983introduction,
  title     = {{Introduction to modern information retrieval}},
  author    = {Salton, Gerard and McGill, Michael},
  year      = 1983,
  publisher = {McGraw-Hill},
  address   = {New York, NY},
  added-at  = {2018-11-04T16:45:24.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/27fc601c5b5346f8939027357c972c5b4/lepsky},
  interhash = {90e5e9500c919499099da9517aa8163e},
  intrahash = {7fc601c5b5346f8939027357c972c5b4},
  keywords  = {information_retrieval},
  timestamp = {2018-11-07T09:14:29.000+0100}
}
@article{schroff2015facenet,
  title         = {{FaceNet: A unified embedding for face recognition and clustering}},
  author        = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year          = 2015,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {07-12-June},
  pages         = {815--823},
  doi           = {10.1109/CVPR.2015.7298682},
  isbn          = 9781467369640,
  issn          = 10636919,
  abstract      = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63{\%}. On YouTube Faces DB it achieves 95.12{\%}. Our system cuts the error rate in comparison to the best published result [15] by 30{\%} on both datasets.},
  archiveprefix = {arXiv},
  arxivid       = {1503.03832},
  eprint        = {1503.03832}
}
@article{simonyan2015verydeepcnn,
  title         = {{Very deep convolutional networks for large-scale image recognition}},
  author        = {Simonyan, Karen and Zisserman, Andrew},
  year          = 2015,
  journal       = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
  pages         = {1--14},
  abstract      = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  arxivid       = {1409.1556}
}
@article{szegedy2015inception,
  title         = {{Going deeper with convolutions}},
  author        = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and others},
  year          = 2015,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {07-12-June},
  pages         = {1--9},
  doi           = {10.1109/CVPR.2015.7298594},
  isbn          = 9781467369640,
  issn          = 10636919,
  abstract      = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archiveprefix = {arXiv},
  arxivid       = {1409.4842},
  eprint        = {1409.4842}
}
@article{taigman2014deepface,
  title         = {{DeepFace: Closing the gap to human-level performance in face verification}},
  author        = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  year          = 2014,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages         = {1701--1708},
  doi           = {10.1109/CVPR.2014.220},
  isbn          = 9781479951178,
  issn          = 10636919,
  abstract      = {In modern face recognition, the conventional pipeline consists of four stages: detect ={\textgreater} align ={\textgreater} represent ={\textgreater} classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35{\%} on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27{\%}, closely approaching human-level performance.},
  archiveprefix = {arXiv},
  arxivid       = {1501.05703},
  eprint        = {1501.05703},
  pmid          = 21646680
}
@article{tan2019motyolo,
  title     = {{A Multiple Object Tracking Algorithm Based on YOLO Detection}},
  author    = {Tan, Li and Dong, Xu and Ma, Yuxi and Yu, Chongchong},
  year      = 2019,
  journal   = {Proceedings - 2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics, CISP-BMEI 2018},
  publisher = {IEEE},
  pages     = {1--5},
  doi       = {10.1109/CISP-BMEI.2018.8633009},
  isbn      = 9781538676042,
  abstract  = {In order to further improve the accuracy and the efficiency of multi-target tracking, a multi-target tracking algorithm based on YOLO is proposed. Firstly, the video stream is detected by YOLO algorithm for multi-target detection. After obtaining the target size, position and other information, the depth feature extraction is performed, the noise data of the unrelated regions in the image is removed, and the complexity of calculation and time of feature extraction are reduced. LSTM (long short-term memory) obtains the temporal relationship between frames and frames. Finally, the Euclidean distance is used to measure the similarity so as to achieve target matching and association and complete the tracking of multiple targets in the video stream. Experiments on the open target tracking data set MOT-16 and MSR Data Set show that the proposed algorithm is workable on multi-target tracking.},
  keywords  = {convolution neural network,multi-target detection,multi-target tracking,recurrent neural network}
}
@article{tang2019cityflow,
  title         = {{Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification}},
  author        = {Tang, Zheng and Naphade, Milind and Liu, Ming Yu and Yang, Xiaodong and Birchfield, Stan and others},
  year          = 2019,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {2019-June},
  pages         = {8789--8798},
  doi           = {10.1109/CVPR.2019.00900},
  isbn          = 9781728132938,
  issn          = 10636919,
  abstract      = {Urban traffic optimization using traffic cameras as websensors is driving the need to advance state-of-the-art multi-target multi-camera (MTMC) tracking. This work introduces CityFlow, a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID). We conducted an extensive experimental evaluation of baselines/state-of-the-art approaches in MTMC tracking, multi-target single-camera (MTSC) tracking, object detection, and image-based ReID on this dataset, analyzing the impact of different network architectures, loss functions, spatio-temporal models and their combinations on task effectiveness. An evaluation server is launched with the release of our benchmark at the 2019 AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to compare the performance of their newest techniques. We expect this dataset to catalyze research in this field, propel the state-of-the-art forward, and lead to deployed traffic optimization(s) in the real world.},
  archiveprefix = {arXiv},
  arxivid       = {1903.09254},
  eprint        = {1903.09254},
  keywords      = {Big Data,Datasets and Evaluation,Large Scale Methods,Motion and Tracking}
}
@article{tao2016sint,
  title         = {{Siamese instance search for tracking}},
  author        = {Tao, Ran and Gavves, Efstratios and Smeulders, Arnold W.M.},
  year          = 2016,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {2016-December},
  pages         = {1420--1429},
  doi           = {10.1109/CVPR.2016.158},
  isbn          = 9781467388504,
  issn          = 10636919,
  abstract      = {In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-theart tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function. The strength of the matching function comes from being extensively trained generically, i.e., without any data of the target, using a Siamese deep neural network, which we design for tracking. Once learned, the matching function is used as is, without any adapting, to track previously unseen targets. It turns out that the learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-theart performance. Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot.},
  archiveprefix = {arXiv},
  arxivid       = {1605.05863},
  eprint        = {1605.05863}
}
@article{tian2019fcos,
  title         = {{FCOS: Fully convolutional one-stage object detection}},
  author        = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
  year          = 2019,
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  volume        = {2019-Octob},
  pages         = {9626--9635},
  doi           = {10.1109/ICCV.2019.00972},
  isbn          = 9781728148038,
  issn          = 15505499,
  abstract      = {We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7{\%} in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: Https://tinyurl.com/FCOSv1.},
  archiveprefix = {arXiv},
  arxivid       = {1904.01355},
  eprint        = {1904.01355}
}
@misc{webcnnarchitecture,
  title        = {{Convolutional Neural Network Architecture}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://ch.mathworks.com/fr/solutions/deep-learning/convolutional-neural-network.html}}
}
@misc{webfcarchitecture,
  title        = {{Fully Connected Neural Network Architecture}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://ch.mathworks.com/fr/solutions/deep-learning/convolutional-neural-network.html}}
}
@misc{webuadetracdataset,
  title        = {{UA-DETRAC dataset}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://detrac-db.rit.albany.edu/}}
}
@misc{webveridataset,
  title        = {{VeRI-776}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://vehiclereid.github.io/VeRi/}}
}
@misc{webvot2019dataset,
  title        = {{VOT 2019}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://www.webvotchallenge.net/vot2019/dataset.html}}
}
@misc{webvotchallenge,
  title        = {{VOT challenge}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://www.webvotchallenge.net/}}
}
@misc{wang2020yolov4,
  title         = {{YOLOv4: Optimal Speed and Accuracy of Object Detection}},
  author        = {Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
  year          = 2020,
  eprint        = {2004.10934},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{wang2015votcnn,
  title    = {{Visual tracking with fully convolutional networks}},
  author   = {Wang, Lijun and Ouyang, Wanli and Wang, Xiaogang and Lu, Huchuan},
  year     = 2015,
  journal  = {Proceedings of the IEEE International Conference on Computer Vision},
  volume   = {2015 Inter},
  pages    = {3119--3127},
  doi      = {10.1109/ICCV.2015.357},
  isbn     = 9781467383912,
  issn     = 15505499,
  abstract = {We propose a new approach for general object tracking with fully convolutional neural network. Instead of treating convolutional neural network (CNN) as a black-box feature extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and classification task on ImageNet. The discoveries motivate the design of our tracking system. It is found that convolutional layers in different levels characterize the target from different perspectives. A top layer encodes more semantic features and serves as a category detector, while a lower layer carries more discriminative information and can better separate the target from distracters with similar appearance. Both layers are jointly used with a switch mechanism during tracking. It is also found that for a tracking target, only a subset of neurons are relevant. A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy and improve tracking accuracy. Extensive evaluation on the widely used tracking benchmark [36] shows that the proposed tacker outperforms the state-of-the-art significantly.}
}
@article{wang2019siammask,
  title         = {{Fast online object tracking and segmentation: A unifying approach}},
  author        = {Wang, Qiang and Zhang, Li and Bertinetto, Luca and Hu, Weiming and Torr, Philip H.S.},
  year          = 2019,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {2019-June},
  pages         = {1328--1338},
  doi           = {10.1109/CVPR.2019.00142},
  isbn          = 9781728132938,
  issn          = 10636919,
  abstract      = {In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017.},
  archiveprefix = {arXiv},
  arxivid       = {1812.05050},
  eprint        = {1812.05050},
  keywords      = {Deep Learning,Motion and Tracking,Vision Applications and Systems}
}
@article{wong2019yolonano,
  title   = {{YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection}},
  author  = {Alexander Wong and Mahmoud Famouri and Mohammad Javad Shafiee and Francis Li and Brendan Chwyl and others},
  year    = 2019,
  journal = {2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)},
  pages   = {22--25}
}
@article{wren1997pfinder,
  title     = {{Pfinder: Real-time tracking of the human body}},
  author    = {Wren, Christopher Richard and Azarbayejani, Ali and Darrell, Trevor and Pentland, Alex Paul},
  year      = 1997,
  journal   = {IEEE Transactions on pattern analysis and machine intelligence},
  publisher = {IEEE},
  volume    = 19,
  number    = 7,
  pages     = {780--785}
}
@inproceedings{yan2017exploiting,
  title     = {{Exploiting Multi-grain Ranking Constraints for Precisely Searching Visually-similar Vehicles}},
  author    = {Yan, Ke and Tian, Yonghong and Wang, Yaowei and Zeng, Wei and Huang, Tiejun},
  year      = 2017,
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  pages     = {562--570},
  doi       = {10.1109/ICCV.2017.68}
}
@article{yang2015compcars,
  title    = {{A large-scale car dataset for fine-grained categorization and verification}},
  author   = {Yang, Linjie and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
  year     = 2015,
  journal  = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume   = {07-12-June-2015},
  pages    = {3973--3981},
  doi      = {10.1109/CVPR.2015.7299023},
  isbn     = 9781467369640,
  issn     = 10636919,
  abstract = {This paper aims to highlight vision related tasks centered around 'car', which has been largely neglected by vision community in comparison to other objects. We show that there are still many interesting car-related problems and applications, which are not yet well explored and researched. To facilitate future car-related research, in this paper we present our on-going effort in collecting a large-scale dataset, 'CompCars', that covers not only different car views, but also their different internal and external parts, and rich attributes. Importantly, the dataset is constructed with a cross-modality nature, containing a surveillance-nature set and a web-nature set. We further demonstrate a few important applications exploiting the dataset, namely car model classification, car model verification, and attribute prediction. We also discuss specific challenges of the car-related problems and other potential applications that worth further investigations. The latest dataset can be downloaded at http://mmlab.ie.cuhk.edu.hk/ datasets/comp-cars/index.html.}
}
@article{yang2016encoderdecoder,
  title         = {{Object contour detection with a fully convolutional encoder-decoder network}},
  author        = {Yang, Jimei and Price, Brian and Cohen, Scott and Lee, Honglak and Yang, Ming Hsuan},
  year          = 2016,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {2016-Decem},
  pages         = {193--202},
  doi           = {10.1109/CVPR.2016.28},
  isbn          = 9781467388504,
  issn          = 10636919,
  abstract      = {We develop a deep learning algorithm for contour detection with a fully convolutional encoder-decoder network. Different from previous low-level edge detection, our algorithm focuses on detecting higher-level object contours. Our network is trained end-to-end on PASCAL VOC with refined ground truth from inaccurate polygon annotations, yielding much higher precision in object contour detection than previous methods. We find that the learned model generalizes well to unseen object classes from the same supercategories on MS COCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning. By combining with the multiscale combinatorial grouping algorithm, our method can generate high-quality segmented object proposals, which significantly advance the state-of-the-art on PASCAL VOC (improving average recall from 0.62 to 0.67) with a relatively small amount of candidates ({\~{}}1660 per image).},
  archiveprefix = {arXiv},
  arxivid       = {1603.04530},
  eprint        = {1603.04530}
}
@article{yu2016unitbox,
  title         = {{UnitBox: An advanced object detection network}},
  author        = {Yu, Jiahui and Jiang, Yuning and Wang, Zhangyang and Cao, Zhimin and Huang, Thomas},
  year          = 2016,
  journal       = {MM 2016 - Proceedings of the 2016 ACM Multimedia Conference},
  pages         = {516--520},
  doi           = {10.1145/2964284.2967274},
  isbn          = 9781450336031,
  abstract      = {In present object detection systems, the deep convolutional neural networks (CNNs) are utilized to predict bounding boxes of object candidates, and have gained performance advantages over the traditional region proposal methods. However, existing deep CNN methods assume the object bounds to be four independent variables, which could be regressed by the ℓ2 loss separately. Such an oversimplified assumption is contrary to the well-received observation, that those variables are correlated, resulting to less accurate localization. To address the issue, we firstly introduce a novel Intersection over Union (IoU) loss function for bounding box prediction, which regresses the four bounds of a predicted box as a whole unit. By taking the advantages of IoU loss and deep fully convolutional networks, the UnitBox is introduced, which performs accurate and efficient localization, shows robust to objects of varied shapes and scales, and converges fast. We apply UnitBox on face detection task and achieve the best performance among all published methods on the FDDB benchmark.},
  archiveprefix = {arXiv},
  arxivid       = {1608.01471},
  eprint        = {1608.01471},
  keywords      = {Bounding Box Prediction,IoU Loss,Object Detection}
}
@misc{zhang2017deeprlintracking,
  title         = {{Deep Reinforcement Learning for Visual Object Tracking in Videos}},
  author        = {Da Zhang and Hamid Maei and Xin Wang and Yuan-Fang Wang},
  year          = 2017,
  eprint        = {1701.08936},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{geetha2013automatic,
  title   = {Automatic rectification of perspective distortion from a single image using plane homography},
  author  = {Geetha Kiran, A and Murali, S},
  year    = 2013,
  journal = {J. Comput. Sci. Appl},
  volume  = 3,
  number  = 5,
  pages   = {47--58}
}
@article{bousaid2020perspective,
  title     = {Perspective distortion modeling for image measurements},
  author    = {Bousaid, Alexandre and Theodoridis, Theodoros and Nefti-Meziani, Samia and Davis, Steve},
  year      = 2020,
  journal   = {IEEE Access},
  publisher = {IEEE},
  volume    = 8,
  pages     = {15322--15331},
  doi       = {10.1109/ACCESS.2020.2966716}
}
@book{hartley2003multiple,
  title     = {Multiple View Geometry in Computer Vision},
  author    = {Hartley, Richard and Zisserman, Andrew},
  year      = 2003,
  publisher = {Cambridge University Press},
  address   = {USA},
  isbn      = {0521540518},
  edition   = 2,
  abstract  = {From the Publisher:A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Recent major developments in the theory and practice of scene reconstruction are described in detail in a unified framework. The book covers the geometric principles and how to represent objects algebraically so they can be computed and applied. The authors provide comprehensive background material and explain how to apply the methods and implement the algorithms directly.}
}
@article{hartley1997defense,
  title     = {In defense of the eight-point algorithm},
  author    = {Hartley, Richard I},
  year      = 1997,
  journal   = {IEEE Transactions on pattern analysis and machine intelligence},
  publisher = {IEEE},
  volume    = 19,
  number    = 6,
  pages     = {580--593},
  doi       = {10.1109/34.601246}
}
@article{lu2005perspective,
  title     = {Perspective rectification of document images using fuzzy set and morphological operations},
  author    = {Lu, Shijian and Chen, Ben M and Ko, Chi Chung},
  year      = 2005,
  journal   = {Image and Vision Computing},
  publisher = {Elsevier},
  volume    = 23,
  number    = 5,
  pages     = {541--553}
}
@inproceedings{miao2006perspective,
  title        = {Perspective rectification of document images based on morphology},
  author       = {Miao, Ligang and Peng, Silong},
  year         = 2006,
  booktitle    = {2006 International Conference on Computational Intelligence and Security},
  volume       = 2,
  pages        = {1805--1808},
  doi          = {10.1109/ICCIAS.2006.295374},
  organization = {IEEE}
}
@article{adel2014image,
  title   = {Image stitching based on feature extraction techniques: a survey},
  author  = {Adel, Ebtsam and Elmogy, Mohammed and Elbakry, Hazem},
  year    = 2014,
  journal = {International Journal of Computer Applications},
  volume  = 99,
  number  = 6,
  pages   = {1--8},
  doi     = {10.5120/17374-7818}
}
@inproceedings{gao2011constructing,
  title        = {Constructing image panoramas using dual-homography warping},
  author       = {Gao, Junhong and Kim, Seon Joo and Brown, Michael S},
  year         = 2011,
  booktitle    = {CVPR 2011},
  pages        = {49--56},
  doi          = {10.1109/CVPR.2011.5995433},
  organization = {IEEE}
}
@inproceedings{liu2015smooth,
  title        = {Smooth Globally Warp Locally: Video Stabilization Using Homography Fields},
  author       = {W. X. {Liu} and T. {Chin}},
  year         = 2015,
  booktitle    = {2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA)},
  pages        = {1--8},
  doi          = {10.1109/DICTA.2015.7371309},
  organization = {IEEE}
}
@article{mariyanayagam2018poseestim,
  title         = {Pose estimation of a single circle using default intrinsic calibration},
  author        = {Damien Mariyanayagam and Pierre Gurdjos and Sylvie Chambon and Florent Brunet and Vincent Charvillat},
  year          = 2018,
  journal       = {CoRR},
  volume        = {abs/1804.04922},
  url           = {http://arxiv.org/abs/1804.04922},
  archiveprefix = {arXiv},
  eprint        = {1804.04922},
  timestamp     = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1804-04922.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{arrospide2010homography,
  title     = {Homography-based ground plane detection using a single on-board camera},
  author    = {Arr{\'o}spide, Jon and Salgado, Luis and Nieto, Marcos and Mohedano, Ra{\'u}l},
  year      = 2010,
  journal   = {IET Intelligent Transport Systems},
  publisher = {IET},
  volume    = 4,
  number    = 2,
  pages     = {149--160},
  doi       = {10.1049/iet-its.2009.0073}
}
@inproceedings{luo2010low,
  title        = {Low-cost implementation of bird's-eye view system for camera-on-vehicle},
  author       = {Luo, Lin-Bo and Koh, In-Sung and Min, Kyeong-Yuk and Wang, Jun and Chong, Jong-Wha},
  year         = 2010,
  booktitle    = {2010 Digest of Technical Papers International Conference on Consumer Electronics (ICCE)},
  pages        = {311--312},
  doi          = {10.1109/ICCE.2010.5418845},
  organization = {IEEE}
}
@article{zhang2000flexible,
  title     = {A flexible new technique for camera calibration},
  author    = {Zhang, Zhengyou},
  year      = 2000,
  journal   = {IEEE Transactions on pattern analysis and machine intelligence},
  publisher = {IEEE},
  volume    = 22,
  number    = 11,
  pages     = {1330--1334},
  doi       = {10.1109/34.888718}
}
@article{zhang2016flexible,
  title     = {A Flexible Online Camera Calibration Using Line Segments},
  author    = {Zhang, Yueqiang and Zhou, Langming and Liu, Haibo and Shang, Yang},
  year      = 2016,
  month     = {Jan},
  day       = {06},
  journal   = {Journal of websensors},
  publisher = {Hindawi Publishing Corporation},
  volume    = 2016,
  doi       = {10.1155/2016/2802343},
  issn      = {1687-725X},
  url       = {https://doi.org/10.1155/2016/2802343}
}
@article{osuna2016multiobjective,
  title     = {A Multiobjective Approach to Homography Estimation},
  author    = {Osuna-Enciso, Valent{\'i}n and Cuevas, Erik and Oliva, Diego and Z{\'u}{\~{n}}iga, Virgilio and P{\'e}rez-Cisneros, Marco and Zald{\'i}var, Daniel},
  year      = 2015,
  month     = {Dec},
  day       = 28,
  journal   = {Computational Intelligence and Neuroscience},
  publisher = {Hindawi Publishing Corporation},
  volume    = 2016,
  pages     = 3629174,
  doi       = {10.1155/2016/3629174},
  issn      = {1687-5265},
  url       = {https://doi.org/10.1155/2016/3629174}
}
@inproceedings{mou2013robust,
  title        = {Robust homography estimation based on non-linear least squares optimization},
  author       = {Mou, Wei and Wang, Han and Seet, Gerald and Zhou, Lubing},
  year         = 2013,
  booktitle    = {2013 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  pages        = {372--377},
  doi          = {10.1109/ROBIO.2013.6739487},
  organization = {IEEE}
}
@article{fischler1981ransac,
  title      = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
  author     = {Fischler, Martin A. and Bolles, Robert C.},
  year       = 1981,
  month      = jun,
  journal    = {Commun. ACM},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = 24,
  number     = 6,
  pages      = {381–395},
  doi        = {10.1145/358669.358692},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/358669.358692},
  issue_date = {June 1981},
  numpages   = 15,
  keywords   = {location determination, image matching, scene analysis, model fitting, camera calibration, automated cartography}
}
@inproceedings{bose2004groundplane,
  title     = {Ground Plane Rectification by Tracking Moving Objects},
  author    = {Biswajit Bose and Eric Grimson},
  year      = 2004,
  booktitle = {IEEE International Workshop on Visual Surveillance and PETS}
}
@article{ondrasovic2021homography,
  title          = {Homography Ranking Based on Multiple Groups of Point Correspondences},
  author         = {Ondra{\v{s}}ovi{\v{c}}, Milan and Tar{\'a}bek, Peter},
  year           = 2021,
  journal        = {websensors},
  volume         = 21,
  number         = 17,
  doi            = {10.3390/s21175752},
  issn           = {1424-8220},
  url            = {https://www.mdpi.com/1424-8220/21/17/5752},
  article-number = 5752,
  pubmedid       = 34502643
}
@inproceedings{ondrasovic2020foundations,
  title     = {Foundations for homography estimation in presence of redundant point correspondencies},
  author    = {Ondra{\v{s}}ovi{\v{c}}, Milan and Tar{\'a}bek, Peter},
  year      = 2020,
  booktitle = {Mathematics in science and technologies - proceedings of the MIST conference 2020},
  number    = {1. vydanie},
  pages     = {52--57}
}
@inproceedings{zhang2012homographytrack,
  title     = {Accurate Object Tracking Based on Homography Matrix},
  author    = {Zhang, Miaohui and Hou, Yandong and Hu, Zhentao},
  year      = 2012,
  booktitle = {2012 International Conference on Computer Science and Service System},
  pages     = {2310--2312},
  doi       = {10.1109/CSSS.2012.573}
}
@article{Mei2009,
  title   = {Efficient Homography-Based Tracking and 3-D Reconstruction for Single-Viewpoint websensors},
  author  = {Mei, Christopher and Benhimane, Selim and Malis, Ezio and Rives, Patrick},
  year    = 2009,
  month   = {01},
  journal = {Robotics, IEEE Transactions on},
  volume  = 24,
  pages   = {1352--1364},
  doi     = {10.1109/TRO.2008.2007941}
}
@article{bay2008speeded,
  title     = {Speeded-up robust features (SURF)},
  author    = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Van Gool, Luc},
  year      = 2008,
  journal   = {Computer vision and image understanding},
  publisher = {Elsevier},
  volume    = 110,
  number    = 3,
  pages     = {346--359},
  doi       = {https://doi.org/10.1016/j.cviu.2007.09.014}
}
@book{bradski2008learning,
  title     = {Learning OpenCV: Computer vision with the OpenCV library},
  author    = {Bradski, Gary and Kaehler, Adrian},
  year      = 2008,
  publisher = {" O'Reilly Media, Inc."}
}
@article{bernardin2008clearmot,
  title   = {Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics},
  author  = {Bernardin, Keni and Stiefelhagen, Rainer},
  year    = 2008,
  month   = {May},
  day     = 18,
  journal = {EURASIP Journal on Image and Video Processing},
  volume  = 2008,
  number  = 1,
  pages   = 246309,
  doi     = {10.1155/2008/246309},
  issn    = {1687-5281},
  url     = {https://doi.org/10.1155/2008/246309}
}
@article{munkres1957assignment,
  title     = {{Algorithms for the Assignment and Transportation Problems}},
  author    = {Munkres, James R.},
  year      = 1957,
  month     = {March},
  journal   = {Journal of the Society for Industrial and Applied Mathematics},
  volume    = 5,
  number    = 1,
  pages     = {32--38},
  added-at  = {2011-12-12T19:01:11.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/2bae4738783565247e1a2e7134bfef57f/gergie},
  groups    = {public},
  interhash = {41b44368bfc8a78101a53b094b6efffc},
  intrahash = {bae4738783565247e1a2e7134bfef57f},
  timestamp = {2011-12-12T19:01:11.000+0100},
  username  = {gergie}
}
@misc{shuai2021siammot,
  title         = {SiamMOT: Siamese Multi-Object Tracking},
  author        = {Bing Shuai and Andrew Berneshawi and Xinyu Li and Davide Modolo and Joseph Tighe},
  year          = 2021,
  eprint        = {2105.11595},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@incollection{paszke2019pytorch,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year      = 2019,
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  pages     = {8024--8035},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@misc{danelljan2019atom,
  title         = {ATOM: Accurate Tracking by Overlap Maximization},
  author        = {Martin Danelljan and Goutam Bhat and Fahad Shahbaz Khan and Michael Felsberg},
  year          = 2019,
  eprint        = {1811.07628},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{lin2018focal,
  title         = {Focal Loss for Dense Object Detection},
  author        = {Tsung-Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollár},
  year          = 2018,
  eprint        = {1708.02002},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{girshick2015fast,
  title         = {Fast R-CNN},
  author        = {Ross Girshick},
  year          = 2015,
  eprint        = {1504.08083},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{bergmann2019tracking,
  title     = {Tracking Without Bells and Whistles},
  author    = {Bergmann, Philipp and Meinhardt, Tim and Leal-Taixe, Laura},
  year      = 2019,
  month     = {Oct},
  journal   = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher = {IEEE},
  doi       = {10.1109/iccv.2019.00103},
  url       = {http://dx.doi.org/10.1109/ICCV.2019.00103}
}
@misc{wojke2017simple,
  title         = {Simple Online and Realtime Tracking with a Deep Association Metric},
  author        = {Nicolai Wojke and Alex Bewley and Dietrich Paulus},
  year          = 2017,
  eprint        = {1703.07402},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{zhou2020tracking,
  title         = {Tracking Objects as Points},
  author        = {Xingyi Zhou and Vladlen Koltun and Philipp Krähenbühl},
  year          = 2020,
  eprint        = {2004.01177},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{ioffe2015batchnorm,
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author        = {Sergey Ioffe and Christian Szegedy},
  year          = {2015},
  eprint        = {1502.03167},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{wu2018groupnorm,
  title         = {Group Normalization},
  author        = {Yuxin Wu and Kaiming He},
  year          = {2018},
  eprint        = {1803.08494},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{zhou2020batchgroupnorm,
  title         = {Batch Group Normalization},
  author        = {Xiao-Yun Zhou and Jiacheng Sun and Nanyang Ye and Xu Lan and Qijun Luo and Bo-Lin Lai and Pedro Esperanca and Guang-Zhong Yang and Zhenguo Li},
  year          = {2020},
  eprint        = {2012.02782},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{he2015resnet,
  title         = {Deep Residual Learning for Image Recognition},
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year          = {2015},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{lin2017fpn,
  title         = {Feature Pyramid Networks for Object Detection},
  author        = {Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
  year          = {2017},
  eprint        = {1612.03144},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{yu2019dla,
  title         = {Deep Layer Aggregation},
  author        = {Fisher Yu and Dequan Wang and Evan Shelhamer and Trevor Darrell},
  year          = {2019},
  eprint        = {1707.06484},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{huang2018densenet,
  title         = {Densely Connected Convolutional Networks},
  author        = {Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
  year          = {2018},
  eprint        = {1608.06993},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{pflugfelder2018indepth,
      title={An In-Depth Analysis of Visual Tracking with Siamese Neural Networks}, 
      author={Roman Pflugfelder},
      year={2018},
      eprint={1707.00569},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
