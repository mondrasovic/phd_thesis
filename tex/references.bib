@article{Kalman1960,
abstract = {The classical filtering and prediction problem is re-examined using the Bode-Sliannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinitememory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix. {\textcopyright} 1960 by ASME.},
author = {Kalman, R. E.},
doi = {10.1115/1.3662552},
issn = {1528901X},
journal = {Journal of Fluids Engineering, Transactions of the ASME},
number = {1},
pages = {35--45},
title = {{A new approach to linear filtering and prediction problems}},
volume = {82},
year = {1960}
}

@article{objecttrackingbenchmark,
author = {Wu, Yi and Lim, Jongwoo and Yang, Ming-Hsuan},
year = {2015},
month = {09},
pages = {1-1},
title = {Object Tracking Benchmark},
volume = {37},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2014.2388226}
}

@misc{welch1995introduction,
  title={An introduction to the Kalman filter},
  author={Welch, Greg and Bishop, Gary and others},
  year={1995},
  publisher={Citeseer}
}

@article{isard1998condensation,
  title={Condensation—conditional density propagation for visual tracking},
  author={Isard, Michael and Blake, Andrew},
  journal={International journal of computer vision},
  volume={29},
  number={1},
  pages={5--28},
  year={1998},
  publisher={Springer}
}

@InProceedings{particle_filters_for_vot,
author="Wang, Fasheng",
editor="Shen, Gang
and Huang, Xiong",
title="Particle Filters for Visual Tracking",
booktitle="Advanced Research on Computer Science and Information Engineering",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="107--112",
abstract="Particle filter has grown to be a standard tool for solving visual tracking problems in real world applications. This paper discusses in detail the application of particle filter in visual tracking, including single object and multiple objects tracking. Choosing a good proposal distribution for the tracking algorithm in particle filtering framework is the main focus of this paper. We also discussed the contributions related to dealing with occlusion, interaction, illumination change using improved particle filters. A conclusion is drawn in section 4.",
isbn="978-3-642-21402-8"
}

@incollection{doucet2001introduction,
  title={An introduction to sequential Monte Carlo methods},
  author={Doucet, Arnaud and De Freitas, Nando and Gordon, Neil},
  booktitle={Sequential Monte Carlo methods in practice},
  pages={3--14},
  year={2001},
  publisher={Springer}
}

@article{Redmon2016,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {779--788},
title = {{You only look once: Unified, real-time object detection}},
volume = {2016-Decem},
year = {2016}
}

@article{Redmon2017,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
doi = {10.1109/CVPR.2017.690},
eprint = {1612.08242},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {6517--6525},
title = {{YOLO9000: Better, faster, stronger}},
volume = {2017-January},
year = {2017}
}

@misc{Redmon2018,
      title={YOLOv3: An Incremental Improvement}, 
      author={Joseph Redmon and Ali Farhadi},
      year={2018},
      eprint={1804.02767},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{Chen2019,
      title={Fast Visual Object Tracking with Rotated Bounding Boxes}, 
      author={Bao Xin Chen and John K. Tsotsos},
      year={2019},
      eprint={1907.03892},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{He2018,
abstract = {Observing that Semantic features learned in an image classification task and Appearance features learned in a similarity matching task complement each other, we build a twofold Siamese network, named SA-Siam, for real-time object tracking. SA-Siam is composed of a semantic branch and an appearance branch. Each branch is a similaritylearning Siamese network. An important design choice in SA-Siam is to separately train the two branches to keep the heterogeneity of the two types of features. In addition, we propose a channel attention mechanism for the semantic branch. Channel-wise weights are computed according to the channel activations around the target position. While the inherited architecture from SiamFC [3] allows our tracker to operate beyond real-time, the twofold design and the attention mechanism significantly improve the tracking performance. The proposed SA-Siam outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks.},
archivePrefix = {arXiv},
arxivId = {1802.08817},
author = {He, Anfeng and Luo, Chong and Tian, Xinmei and Zeng, Wenjun},
doi = {10.1109/CVPR.2018.00508},
eprint = {1802.08817},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {4834--4843},
title = {{A Twofold Siamese Network for Real-Time Object Tracking}},
year = {2018}
}

@article{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.},
author = {Lowe, David G.},
doi = {10.1109/iccv.1999.790410},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1150--1157},
title = {{Object recognition from local scale-invariant features}},
volume = {2},
year = {1999}
}

@article{Wong2019,
  title={YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection},
  author={Alexander Wong and Mahmoud Famouri and Mohammad Javad Shafiee and Francis Li and Brendan Chwyl and Jonathan Chung},
  journal={2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)},
  year={2019},
  pages={22-25}
}

@misc{Wang,
      title={YOLOv4: Optimal Speed and Accuracy of Object Detection}, 
      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
      year={2020},
      eprint={2004.10934},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{osti_6007283,
title = {Method of and apparatus for pattern recognition},
author = {McConnell, R K},
abstractNote = {This patent describes a method of recognizing a pattern in a test object. The method consists of: specifying properties characteristic of the pattern; specifying discrete ranges of values of the properties; measuring the values of the properties in the test object; arranging the measured values in at least one test histogram; determining a reference set of values of the properties and arranging the set as at least a first reference histogram; and comparing the test and reference histograms by determination of the value of a function which provides a measure of the amount of information necessary to express the at least one test histogram in terms of the optimum code for describing at least the first reference histogram.},
doi = {},
journal = {},
place = {United States},
year = {1986},
month = {1}
}

@article{Held2016,
abstract = {Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance. Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training. We propose a method for offline training of neural networks that can track novel objects at test-time at 100 fps. Our tracker is significantly faster than previous methods that use neural networks for tracking, which are typically very slow to run and not practical for real-time applications. Our tracker uses a simple feed-forward network with no online training required. The tracker learns a generic relationship between object motion and appearance and can be used to track novel objects that do not appear in the training set. We test our network on a standard tracking benchmark to demonstrate our tracker's state-of-the-art performance. Further, our performance improves as we add more videos to our offline training set. To the best of our knowledge, our tracker 1 is the first neural-network tracker that learns to track generic objects at 100 fps.},
archivePrefix = {arXiv},
arxivId = {1604.01802},
author = {Held, David and Thrun, Sebastian and Savarese, Silvio},
eprint = {1604.01802},
journal = {Computer Vision – ECCV 2016 Lecture Notes in Computer Science},
keywords = {deep learning,machine learning,neural networks,tracking},
pages = {749--765},
title = {{Learning to Track at 100 FPS with Deep}},
url = {http://davheld.github.io/GOTURN/GOTURN.html},
year = {2016}
}

@article{Lin2014NetworkInNetwork,
abstract = {We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.4400v3},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {arXiv:1312.4400v3},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
pages = {1--10},
title = {{Network in network}},
year = {2014}
}

@article{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}

@article{Bertinetto2016,
abstract = {The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object's appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.},
archivePrefix = {arXiv},
arxivId = {1606.09549},
author = {Bertinetto, Luca and Valmadre, Jack and Henriques, Jo{\~{a}}o F. and Vedaldi, Andrea and Torr, Philip H.S.},
doi = {10.1007/978-3-319-48881-3_56},
eprint = {1606.09549},
isbn = {9783319488806},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Deep-learning,Object-tracking,Siamese-network,Similarity-learning},
pages = {850--865},
pmid = {4520227},
title = {{Fully-convolutional siamese networks for object tracking}},
volume = {9914 LNCS},
year = {2016}
}

@misc{Guo2019,
      title={SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking}, 
      author={Dongyan Guo and Jun Wang and Ying Cui and Zhenhua Wang and Shengyong Chen},
      year={2019},
      eprint={1911.07241},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Ren2017,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1109/TPAMI.2016.2577031},
eprint = {1506.01497},
file = {:E$\backslash$:/Data/Downloads/1506.01497.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Object detection,convolutional neural network,region proposal},
number = {6},
pages = {1137--1149},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
volume = {39},
year = {2017}
}

@article{Yu2016,
abstract = {In present object detection systems, the deep convolutional neural networks (CNNs) are utilized to predict bounding boxes of object candidates, and have gained performance advantages over the traditional region proposal methods. However, existing deep CNN methods assume the object bounds to be four independent variables, which could be regressed by the ℓ2 loss separately. Such an oversimplified assumption is contrary to the well-received observation, that those variables are correlated, resulting to less accurate localization. To address the issue, we firstly introduce a novel Intersection over Union (IoU) loss function for bounding box prediction, which regresses the four bounds of a predicted box as a whole unit. By taking the advantages of IoU loss and deep fully convolutional networks, the UnitBox is introduced, which performs accurate and efficient localization, shows robust to objects of varied shapes and scales, and converges fast. We apply UnitBox on face detection task and achieve the best performance among all published methods on the FDDB benchmark.},
archivePrefix = {arXiv},
arxivId = {1608.01471},
author = {Yu, Jiahui and Jiang, Yuning and Wang, Zhangyang and Cao, Zhimin and Huang, Thomas},
doi = {10.1145/2964284.2967274},
eprint = {1608.01471},
isbn = {9781450336031},
journal = {MM 2016 - Proceedings of the 2016 ACM Multimedia Conference},
keywords = {Bounding Box Prediction,IoU Loss,Object Detection},
pages = {516--520},
title = {{UnitBox: An advanced object detection network}},
year = {2016}
}

@article{Huang2017,
abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
archivePrefix = {arXiv},
arxivId = {1611.10012},
author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
doi = {10.1109/CVPR.2017.351},
eprint = {1611.10012},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {3296--3305},
title = {{Speed/accuracy trade-offs for modern convolutional object detectors}},
volume = {2017-Janua},
year = {2017}
}

@article{Tian2019,
abstract = {We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7{\%} in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: Https://tinyurl.com/FCOSv1.},
archivePrefix = {arXiv},
arxivId = {1904.01355},
author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
doi = {10.1109/ICCV.2019.00972},
eprint = {1904.01355},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {9626--9635},
title = {{FCOS: Fully convolutional one-stage object detection}},
volume = {2019-Octob},
year = {2019}
}

@misc{opticalflowimage,
  title = {{Optical Flow Illustration}},
  howpublished = {\url{https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html}},
  note = {Accessed: 2020-09-07}
}

@article{Shi1994,
abstract = {No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. These methods are based on a new tracking algorithm that extends previous Newton-Raphson style search methods to work under affine image transformations. We test performance with several simulations and experiments},
author = {Shi, Jianbo and Tomasi, Carlo},
doi = {10.1109/CVPR.1994.323794},
isbn = {0-8186-5825-8},
issn = {1063-6919},
journal = {Image (Rochester, N.Y.)},
pages = {593--600},
pmid = {11968495},
title = {{Good Features}},
year = {1994}
}

@misc{Leal-Taixe2017,
      title={Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking}, 
      author={Laura Leal-Taixé and Anton Milan and Konrad Schindler and Daniel Cremers and Ian Reid and Stefan Roth},
      year={2017},
      eprint={1704.02781},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Yang2016,
abstract = {We develop a deep learning algorithm for contour detection with a fully convolutional encoder-decoder network. Different from previous low-level edge detection, our algorithm focuses on detecting higher-level object contours. Our network is trained end-to-end on PASCAL VOC with refined ground truth from inaccurate polygon annotations, yielding much higher precision in object contour detection than previous methods. We find that the learned model generalizes well to unseen object classes from the same supercategories on MS COCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning. By combining with the multiscale combinatorial grouping algorithm, our method can generate high-quality segmented object proposals, which significantly advance the state-of-the-art on PASCAL VOC (improving average recall from 0.62 to 0.67) with a relatively small amount of candidates ({\~{}}1660 per image).},
archivePrefix = {arXiv},
arxivId = {1603.04530},
author = {Yang, Jimei and Price, Brian and Cohen, Scott and Lee, Honglak and Yang, Ming Hsuan},
doi = {10.1109/CVPR.2016.28},
eprint = {1603.04530},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {193--202},
title = {{Object contour detection with a fully convolutional encoder-decoder network}},
volume = {2016-Decem},
year = {2016}
}

@article{Wang2019,
abstract = {In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017.},
archivePrefix = {arXiv},
arxivId = {1812.05050},
author = {Wang, Qiang and Zhang, Li and Bertinetto, Luca and Hu, Weiming and Torr, Philip H.S.},
doi = {10.1109/CVPR.2019.00142},
eprint = {1812.05050},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Deep Learning,Motion and Tracking,Vision Applications and Systems},
pages = {1328--1338},
title = {{Fast online object tracking and segmentation: A unifying approach}},
volume = {2019-June},
year = {2019}
}

@article{Wang2015,
abstract = {We propose a new approach for general object tracking with fully convolutional neural network. Instead of treating convolutional neural network (CNN) as a black-box feature extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and classification task on ImageNet. The discoveries motivate the design of our tracking system. It is found that convolutional layers in different levels characterize the target from different perspectives. A top layer encodes more semantic features and serves as a category detector, while a lower layer carries more discriminative information and can better separate the target from distracters with similar appearance. Both layers are jointly used with a switch mechanism during tracking. It is also found that for a tracking target, only a subset of neurons are relevant. A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy and improve tracking accuracy. Extensive evaluation on the widely used tracking benchmark [36] shows that the proposed tacker outperforms the state-of-the-art significantly.},
author = {Wang, Lijun and Ouyang, Wanli and Wang, Xiaogang and Lu, Huchuan},
doi = {10.1109/ICCV.2015.357},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3119--3127},
title = {{Visual tracking with fully convolutional networks}},
volume = {2015 Inter},
year = {2015}
}


@article{Hosang2017,
abstract = {Object detectors have hugely profited from moving towards an end-to-end learning paradigm: proposals, features, and the classifier becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard NMS algorithm is still fully hand-crafted, suspiciously simple, and - being based on greedy clustering with a fixed distance threshold - forces a trade-off between recall and precision. We propose a new network architecture designed to perform NMS, using only boxes and their score. We report experiments for person detection on PETS and for general object categories on the COCO dataset. Our approach shows promise providing improved localization and occlusion handling.},
archivePrefix = {arXiv},
arxivId = {1705.02950},
author = {Hosang, Jan and Benenson, Rodrigo and Schiele, Bernt},
doi = {10.1109/CVPR.2017.685},
eprint = {1705.02950},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {6469--6477},
title = {{Learning non-maximum suppression}},
volume = {2017-January},
year = {2017}
}

@article{Bodla2017,
abstract = {Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-NMS obtains consistent improvements for the coco-style mAP metric on standard datasets like PASCAL VOC2007 (1.7{\%} for both R-FCN and Faster-RCNN) and MS-COCO (1.3{\%} for R-FCN and 1.1{\%} for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves state-of-the-art in object detection from 39.8{\%} to 40.9{\%} with a single model. Further, the computational complexity of Soft-NMS is the same as traditional NMS and hence it can be efficiently implemented. Since Soft-NMS does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for Soft-NMS is publicly available on GitHub http://bit.ly/2nJLNMu.},
archivePrefix = {arXiv},
arxivId = {1704.04503},
author = {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S.},
doi = {10.1109/ICCV.2017.593},
eprint = {1704.04503},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {5562--5570},
title = {{Soft-NMS - Improving Object Detection with One Line of Code}},
volume = {2017-October},
year = {2017}
}

@article{cortes1995support,
  added-at = {2015-09-17T16:32:09.000+0200},
  author = {Cortes, C. and Vapnik, V.},
  biburl = {https://www.bibsonomy.org/bibtex/22b1eb8bea07ae0156a53a4e9c6eac1df/nosebrain},
  interhash = {c223c465141618ad63aac5a6132280f7},
  intrahash = {2b1eb8bea07ae0156a53a4e9c6eac1df},
  journal = {Machine Learning},
  keywords = {classification margin soft support svm vector},
  pages = {273-297},
  timestamp = {2015-09-17T17:15:55.000+0200},
  title = {Support Vector Networks},
  year = 1995
}

@INPROCEEDINGS{JiaDeng2009,  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},   title={ImageNet: A large-scale hierarchical image database},   year={2009},  volume={},  number={},  pages={248-255},  doi={10.1109/CVPR.2009.5206848}}

@inproceedings{ng2002discriminative,
  title={On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes},
  author={Ng, Andrew Y and Jordan, Michael I},
  booktitle={Advances in neural information processing systems},
  pages={841--848},
  year={2002}
}

@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--14},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}

@article{Liu2016,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300 input, SSD achieves 74.3{\%} mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/ tree/ssd.},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network,Real-time object detection},
pages = {21--37},
title = {{SSD: Single shot multibox detector}},
volume = {9905 LNCS},
year = {2016}
}

@article{Arulampalam2007,
abstract = {Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or “particle”) representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example.},
author = {Arulampalam, M. Sanjeev and Maskell, Simon and Gordon, Neil and Clapp, Tim},
doi = {10.1109/9780470544198.ch73},
isbn = {9780470544198},
journal = {Bayesian Bounds for Parameter Estimation and Nonlinear Filtering/Tracking},
keywords = {Approximation algorithms,Approximation methods,Bayesian methods,Filtering algorithms,Particle filters},
number = {2},
pages = {723--737},
publisher = {IEEE},
title = {{A tutorial on particle filters for online nonlinear/nongaussian bayesian tracking}},
volume = {50},
year = {2007}
}

@phdthesis{bergman1999recursive,
  title={Recursive Bayesian estimation: Navigation and tracking applications},
  author={Bergman, Niclas},
  year={1999},
  school={Link{\"o}ping University}
}

@mastersthesis{stordal2008sequential,
  title={Sequential Monte Carlo Methods for Bayesian Filtering},
  author={Stordal, Andreas St{\o}rksen},
  year={2008},
  school={The University of Bergen}
}

@article{Kunsch2013,
abstract = {This is a short review of Monte Carlo methods for approximating filter distributions in state space models. The basic algorithm and different strategies to reduce imbalance of the weights are discussed. Finally, methods for more difficult problems like smoothing and parameter estimation and applications outside the state space model context are presented. {\textcopyright} 2013 ISI/BS.},
archivePrefix = {arXiv},
arxivId = {1309.7807},
author = {K{\"{u}}nsch, Hans R.},
doi = {10.3150/12-BEJSP07},
eprint = {1309.7807},
issn = {13507265},
journal = {Bernoulli},
keywords = {Ensemble Kalman filter,Importance sampling and resampling,Sequential Monte Carlo,Smoothing algorithm,State space models},
number = {4},
pages = {1391--1403},
title = {{Particle filters}},
volume = {19},
year = {2013}
}

@article{mihaylova2007object,
  title={Object tracking by particle filtering techniques in video sequences},
  author={Mihaylova, Lyudmila and Brasnett, Paul and Canagarajah, Nishan and Bull, David},
  journal={Advances and challenges in multisensor data and information processing},
  volume={8},
  pages={260--268},
  year={2007},
  publisher={Citeseer}
}

@incollection{kim2018introduction,
  title={Introduction to Kalman filter and its applications},
  author={Kim, Youngjoo and Bang, Hyochoong},
  booktitle={Introduction and Implementations of the Kalman Filter},
  year={2018},
  publisher={IntechOpen}
}

@ARTICLE{broida1986estimation,  author={Broida, Ted J. and Chellappa, Rama},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Estimation of Object Motion Parameters from Noisy Images},   year={1986},  volume={PAMI-8},  number={1},  pages={90-99},  doi={10.1109/TPAMI.1986.4767755}}

@ARTICLE{Kuhn55thehungarian,
    author = {H. W. Kuhn and Bryn Yaw},
    title = {The Hungarian method for the assignment problem},
    journal = {Naval Res. Logist. Quart},
    year = {1955},
    pages = {83--97}
}

@ARTICLE{comaniciu2003kernel, 
author={Comaniciu, D. and Ramesh, V. and Meer, P.},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Kernel-based object tracking},   year={2003},  volume={25},  number={5},  pages={564-577},  doi={10.1109/TPAMI.2003.1195991}
}

@inproceedings{gabriel2003state,
  title={The state of the art in multiple object tracking under occlusion in video sequences},
  author={Gabriel, Pierre F and Verly, Jacques G and Piater, Justus H and Genon, Andr{\'e}},
  booktitle={Advanced Concepts for Intelligent Vision Systems},
  pages={166--173},
  year={2003}
}

@article{wren1997pfinder,
  title={Pfinder: Real-time tracking of the human body},
  author={Wren, Christopher Richard and Azarbayejani, Ali and Darrell, Trevor and Pentland, Alex Paul},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={19},
  number={7},
  pages={780--785},
  year={1997},
  publisher={IEEE}
}

@article{Jalal2012,
author = {Jalal, Anand and Singh, Vrijendra},
year = {2012},
month = {01},
pages = {227-248},
title = {The State-of-the-Art in Visual Object Tracking.},
volume = {36},
journal = {Informatica (Slovenia)},
issn = {03505596}
}

@book{forsyth2002computer,
  added-at = {2015-03-23T00:00:00.000+0100},
  author = {Forsyth, David A. and Ponce, Jean},
  biburl = {https://www.bibsonomy.org/bibtex/24e26ef656dc22317d8ed6d0cfad2d3bc/dblp},
  ee = {http://vig.pearsoned.com/store/product/1,1207,store-12521_isbn-013608592X,00.html},
  interhash = {a4dfa38179479650cae77eb72b893bb6},
  intrahash = {4e26ef656dc22317d8ed6d0cfad2d3bc},
  isbn = {978-0-273-76414-4},
  keywords = {dblp},
  pages = {1-791},
  publisher = {Pitman},
  timestamp = {2015-06-18T09:49:18.000+0200},
  title = {Computer Vision - A Modern Approach, Second Edition.},
  year = 2012
}

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}

@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1097--1105},
title = {{ImageNet classification with deep convolutional neural networks}},
volume = {2},
year = {2012}
}

@book{franoischollet2017learning,
  added-at = {2018-08-01T08:16:18.000+0200},
  author = {Chollet, François},
  biburl = {https://www.bibsonomy.org/bibtex/231f94815ebbd65d3a31e4a69e818573e/jaeschke},
  interhash = {cfbfd3f93853a469e5e6978f61a74a0a},
  intrahash = {31f94815ebbd65d3a31e4a69e818573e},
  isbn = {9781617294433},
  keywords = {ai deeplearning ml},
  month = nov,
  publisher = {Manning},
  timestamp = {2018-08-01T08:16:18.000+0200},
  title = {Deep Learning with Python },
  year = 2017
}


@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {08997667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}

@incollection{rumelhart:errorpropnonote,
  added-at = {2008-02-26T11:58:58.000+0100},
  address = {Cambridge, MA},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  biburl = {https://www.bibsonomy.org/bibtex/27c3d39c519530239660d33e66493ade1/schaul},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, {V}olume 1: {F}oundations},
  citeulike-article-id = {2378884},
  description = {idsia},
  editor = {Rumelhart, David E. and Mcclelland, James L.},
  interhash = {dd8485b30b80c7f35263bcb21ed81c1f},
  intrahash = {7c3d39c519530239660d33e66493ade1},
  keywords = {nn},
  pages = {318--362},
  priority = {2},
  publisher = {MIT Press},
  timestamp = {2008-02-26T12:02:43.000+0100},
  title = {Learning Internal Representations by Error Propagation},
  year = 1986
}

@article{Tang2019,
abstract = {Urban traffic optimization using traffic cameras as sensors is driving the need to advance state-of-the-art multi-target multi-camera (MTMC) tracking. This work introduces CityFlow, a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID). We conducted an extensive experimental evaluation of baselines/state-of-the-art approaches in MTMC tracking, multi-target single-camera (MTSC) tracking, object detection, and image-based ReID on this dataset, analyzing the impact of different network architectures, loss functions, spatio-temporal models and their combinations on task effectiveness. An evaluation server is launched with the release of our benchmark at the 2019 AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to compare the performance of their newest techniques. We expect this dataset to catalyze research in this field, propel the state-of-the-art forward, and lead to deployed traffic optimization(s) in the real world.},
archivePrefix = {arXiv},
arxivId = {1903.09254},
author = {Tang, Zheng and Naphade, Milind and Liu, Ming Yu and Yang, Xiaodong and Birchfield, Stan and Wang, Shuo and Kumar, Ratnesh and Anastasiu, David and Hwang, Jenq Neng},
doi = {10.1109/CVPR.2019.00900},
eprint = {1903.09254},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Big Data,Datasets and Evaluation,Large Scale Methods,Motion and Tracking},
pages = {8789--8798},
title = {{Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification}},
volume = {2019-June},
year = {2019}
}

@article{Leal-Taixe2016,
abstract = {This paper introduces a novel approach to the task of data association within the context of pedestrian tracking, by introducing a two-stage learning scheme to match pairs of detections. First, a Siamese convolutional neural network (CNN) is trained to learn descriptors encoding local spatio-temporal structures between the two input image patches, aggregating pixel values and optical flow information. Second, a set of contextual features derived from the position and size of the compared input patches are combined with the CNN output by means of a gradient boosting classifier to generate the final matching probability. This learning approach is validated by using a linear programming based multi-person tracker showing that even a simple and efficient tracker may outperform much more complex models when fed with our learned matching probabilities. Results on publicly available sequences show that our method meets state-of-the-art standards in multiple people tracking.},
archivePrefix = {arXiv},
arxivId = {1604.07866},
author = {Leal-Taixe, Laura and Canton-Ferrer, Cristian and Schindler, Konrad},
doi = {10.1109/CVPRW.2016.59},
eprint = {1604.07866},
isbn = {9781467388504},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {418--425},
title = {{Learning by Tracking: Siamese CNN for Robust Target Association}},
year = {2016}
}

@inproceedings{Lucas1981,
author = {Lucas, Bruce D. and Kanade, Takeo},
title = {An Iterative Image Registration Technique with an Application to Stereo Vision},
year = {1981},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the 7th International Joint Conference on Artificial Intelligence - Volume 2},
pages = {674–679},
numpages = {6},
location = {Vancouver, BC, Canada},
series = {IJCAI'81}
}

@InProceedings{GunnarFarneback,
author="Farneback, Gunnar",
editor="Bigun, Josef
and Gustavsson, Tomas",
title="Two-Frame Motion Estimation Based on Polynomial Expansion",
booktitle="Image Analysis",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="363--370",
abstract="This paper presents a novel two-frame motion estimation algorithm. The first step is to approximate each neighborhood of both frames by quadratic polynomials, which can be done efficiently using the polynomial expansion transform. From observing how an exact polynomial transforms under translation a method to estimate displacement fields from the polynomial expansion coefficients is derived and after a series of refinements leads to a robust algorithm. Evaluation on the Yosemite sequence shows good results.",
isbn="978-3-540-45103-7"
}

@article{
2018-TOG-SFV,
author = {Peng, Xue Bin and Kanazawa, Angjoo and Malik, Jitendra and Abbeel, Pieter and Levine, Sergey},
title = {SFV: Reinforcement Learning of Physical Skills from Videos},
journal = {ACM Trans. Graph.},
volume = {37},
number = {6},
month = nov,
year = {2018},
articleno = {178},
numpages = {14},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {physics-based character animation, computer vision, video imitation, reinforcement learning, motion reconstruction}
}

@article{DBLP:journals/corr/abs-1709-04905,
  author    = {Chelsea Finn and
               Tianhe Yu and
               Tianhao Zhang and
               Pieter Abbeel and
               Sergey Levine},
  title     = {One-Shot Visual Imitation Learning via Meta-Learning},
  journal   = {CoRR},
  volume    = {abs/1709.04905},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.04905},
  archivePrefix = {arXiv},
  eprint    = {1709.04905},
  timestamp = {Mon, 13 Aug 2018 16:47:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1709-04905.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Schroff2015,
abstract = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63{\%}. On YouTube Faces DB it achieves 95.12{\%}. Our system cuts the error rate in comparison to the best published result [15] by 30{\%} on both datasets.},
archivePrefix = {arXiv},
arxivId = {1503.03832},
author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
doi = {10.1109/CVPR.2015.7298682},
eprint = {1503.03832},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {815--823},
title = {{FaceNet: A unified embedding for face recognition and clustering}},
volume = {07-12-June},
year = {2015}
}

@article{Dosovitskiy2015,
abstract = {Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.},
archivePrefix = {arXiv},
arxivId = {1504.06852},
author = {Dosovitskiy, Alexey and Fischery, Philipp and Ilg, Eddy and Hausser, Philip and Hazirbas, Caner and Golkov, Vladimir and Smagt, Patrick Van Der and Cremers, Daniel and Brox, Thomas},
doi = {10.1109/ICCV.2015.316},
eprint = {1504.06852},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2758--2766},
title = {{FlowNet: Learning optical flow with convolutional networks}},
volume = {2015 Inter},
year = {2015}
}

@incollection{fleet2006optical,
  title={Optical flow estimation},
  author={Fleet, David and Weiss, Yair},
  booktitle={Handbook of mathematical models in computer vision},
  pages={237--257},
  year={2006},
  publisher={Springer}
}

@article{Taigman2014,
abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ={\textgreater} align ={\textgreater} represent ={\textgreater} classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35{\%} on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27{\%}, closely approaching human-level performance.},
archivePrefix = {arXiv},
arxivId = {1501.05703},
author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
doi = {10.1109/CVPR.2014.220},
eprint = {1501.05703},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1701--1708},
pmid = {21646680},
title = {{DeepFace: Closing the gap to human-level performance in face verification}},
year = {2014}
}

@article{Jiyan2007,
abstract = {In object tracking, occlusions significantly undermine the performance of tracking algorithms. Unlike the existing methods that solely depend on the observed target appearance to detect occluders, we propose an algorithm that progressively analyzes the occlusion situation by exploiting the spatiotemporal context information, which is further double checked by the reference target and motion constraints. This strategy enables our proposed algorithm to make a clearer distinction between the target and occluders than existing approaches. To further improve the tracking performance, we rectify the occlusion-interfered erroneous target location by employing a variant-mask template matching operation. As a result, correct target location can always be obtained regardless of the occlusion situation. Using these techniques, the robustness of tracking under occlusions is significantly promoted. Experimental results have confirmed the effectiveness of our proposed algorithm. {\textcopyright} 2007 IEEE.},
author = {Jiyan, Pan and Bo, Hu},
doi = {10.1109/CVPR.2007.383453},
isbn = {1424411807},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {June 2007},
title = {{Robust occlusion handling in object tracking}},
year = {2007}
}

@article{Hadsell2006,
abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that "similar" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distance measure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular ILE. {\textcopyright} 2006 IEEE.},
author = {Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
doi = {10.1109/CVPR.2006.100},
isbn = {0769525970},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1735--1742},
title = {{Dimensionality reduction by learning an invariant mapping}},
volume = {2},
year = {2006}
}

@article{Kuma2019,
abstract = {In this paper we tackle the problem of vehicle re-identification in a camera network utilizing triplet embeddings. Re-identification is the problem of matching appearances of objects across different cameras. With the proliferation of surveillance cameras enabling smart and safer cities, there is an ever-increasing need to re-identify vehicles across cameras. Typical challenges arising in smart city scenarios include variations of viewpoints, illumination and self occlusions. Most successful approaches for re-identification involve (deep) learning an embedding space such that the vehicles of same identities are projected closer to one another, compared to the vehicles representing different identities. Popular loss functions for learning an embedding (space) include contrastive or triplet loss. In this paper we provide an extensive evaluation of these losses applied to vehicle re-identification and demonstrate that using the best practices for learning embeddings outperform most of the previous approaches proposed in the vehicle re-identification literature. Compared to most existing state-of-the-art approaches, our approach is simpler and more straightforward for training utilizing only identity-level annotations, along with one of the smallest published embedding dimensions for efficient inference. Furthermore in this work we introduce a formal evaluation of a triplet sampling variant (batch sample) into the re-identification literature.},
archivePrefix = {arXiv},
arxivId = {1901.01015},
author = {Kuma, Ratnesh and Weill, Edwin and Aghdasi, Farzin and Sriram, Parthasarathy},
doi = {10.1109/IJCNN.2019.8852059},
eprint = {1901.01015},
isbn = {9781728119854},
journal = {Proceedings of the International Joint Conference on Neural Networks},
title = {{Vehicle Re-identification: An Efficient Baseline Using Triplet Embedding}},
volume = {2019-July},
year = {2019}
}

@article{Li2019,
abstract = {Feature Pyramid Networks (FPN) is a popular feature extraction. However, FPN and its variants do not investigate the influence of resolution information and semantic information in the object detection. Thus, FPN and its variants cannot detect some objects on challenging images. In this paper, based on FPN, we propose to use gaussian kernel function to assign different weight values to semantic information and resolution information for different images in the object detection. The proposed method, is called a Weighted Feature Pyramid Network (WFPN), and shows significant improvement over the traditional feature pyramids in several applications. Using WFPN in Faster R-CNN system, the proposed method achieves better performance on the PASCAL detection benchmark.},
archivePrefix = {arXiv},
arxivId = {arXiv:1612.03144v2},
author = {Li, Xiaohan and Lai, Taotao and Wang, Shuaiyu and Chen, Quan and Yang, Changcai and Chen, Riqing},
doi = {10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00217},
eprint = {arXiv:1612.03144v2},
file = {:C$\backslash$:/Users/Milan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2019 - Weighted feature pyramid networks for object detection.pdf:pdf},
isbn = {9781728143286},
journal = {Proceedings - 2019 IEEE Intl Conf on Parallel and Distributed Processing with Applications, Big Data and Cloud Computing, Sustainable Computing and Communications, Social Computing and Networking, ISPA/BDCloud/SustainCom/SocialCom 2019},
keywords = {Gaussian kernel function,Resolution information,Semantic information,Weighted feature pyramid networks},
pages = {1500--1504},
title = {{Weighted feature pyramid networks for object detection}},
year = {2019}
}

@article{Li2018,
abstract = {Visual object tracking has been a fundamental topic in recent years and many deep learning based trackers have achieved state-of-the-art performance on multiple benchmarks. However, most of these trackers can hardly get top performance with real-time speed. In this paper, we propose the Siamese region proposal network (Siamese-RPN) which is end-to-end trained off-line with large-scale image pairs. Specifically, it consists of Siamese subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. In the inference phase, the proposed framework is formulated as a local one-shot detection task. We can pre-compute the template branch of the Siamese subnetwork and formulate the correlation layers as trivial convolution layers to perform online tracking. Benefit from the proposal refinement, traditional multi-scale test and online fine-tuning can be discarded. The Siamese-RPN runs at 160 FPS while achieving leading performance in VOT2015, VOT2016 and VOT2017 real-time challenges.},
author = {Li, Bo and Yan, Junjie and Wu, Wei and Zhu, Zheng and Hu, Xiaolin},
doi = {10.1109/CVPR.2018.00935},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8971--8980},
title = {{High Performance Visual Tracking with Siamese Region Proposal Network}},
year = {2018}
}

@article{Bertinetto2016OneShot,
abstract = {One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.},
archivePrefix = {arXiv},
arxivId = {1606.05233},
author = {Bertinetto, Luca and Henriques, Jo{\~{a}}o F. and Valmadre, Jack and Torr, Philip H.S. and Vedaldi, Andrea},
eprint = {1606.05233},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {523--531},
title = {{Learning feed-forward one-shot learners}},
year = {2016}
}

@article{Tan2019,
abstract = {In order to further improve the accuracy and the efficiency of multi-target tracking, a multi-target tracking algorithm based on YOLO is proposed. Firstly, the video stream is detected by YOLO algorithm for multi-target detection. After obtaining the target size, position and other information, the depth feature extraction is performed, the noise data of the unrelated regions in the image is removed, and the complexity of calculation and time of feature extraction are reduced. LSTM (long short-term memory) obtains the temporal relationship between frames and frames. Finally, the Euclidean distance is used to measure the similarity so as to achieve target matching and association and complete the tracking of multiple targets in the video stream. Experiments on the open target tracking data set MOT-16 and MSR Data Set show that the proposed algorithm is workable on multi-target tracking.},
author = {Tan, Li and Dong, Xu and Ma, Yuxi and Yu, Chongchong},
doi = {10.1109/CISP-BMEI.2018.8633009},
isbn = {9781538676042},
journal = {Proceedings - 2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics, CISP-BMEI 2018},
keywords = {convolution neural network,multi-target detection,multi-target tracking,recurrent neural network},
pages = {1--5},
publisher = {IEEE},
title = {{A Multiple Object Tracking Algorithm Based on YOLO Detection}},
year = {2019}
}

@article{opencv_library,
author = {Bradski, G.},
citeulike-article-id = {2236121},
journal = {Dr. Dobb's Journal of Software Tools},
keywords = {bibtex-import},
posted-at = {2008-01-15 19:21:54},
priority = {4},
title = {{The OpenCV Library}},
year = {2000}
}

@misc{Hermans2017,
      title={In Defense of the Triplet Loss for Person Re-Identification}, 
      author={Alexander Hermans and Lucas Beyer and Bastian Leibe},
      year={2017},
      eprint={1703.07737},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Liu2018,
abstract = {Compared with person reidentification, which has attracted concentrated attention, vehicle reidentification is an important yet frontier problem in video surveillance and has been neglected by the multimedia and vision communities. Since most existing approaches mainly consider the general vehicle appearance for reidentification while overlooking the distinct vehicle identifier, such as the license plate number, they attain suboptimal performance. In this paper, we propose PROVID, a PROgressive Vehicle re-IDentification framework based on deep neural networks. In particular, our framework not only utilizes the multimodality data in large-scale video surveillance, such as visual features, license plates, camera locations, and contextual information, but also considers vehicle reidentification in two progressive procedures: coarse-To-fine search in the feature domain, and near-To-distant search in the physical space. Furthermore, to evaluate our progressive search framework and facilitate related research, we construct the VeRi dataset, which is the most comprehensive dataset from real-world surveillance videos. It not only provides large numbers of vehicles with varied labels and sufficient cross-camera recurrences but also contains license plate numbers and contextual information. Extensive experiments on the VeRi dataset demonstrate both the accuracy and efficiency of our progressive vehicle reidentification framework.},
author = {Liu, Xinchen and Liu, Wu and Mei, Tao and Ma, Huadong},
doi = {10.1109/TMM.2017.2751966},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Progressive search,contextual information,deep learning,license plate verification,vehicle re-identification},
number = {3},
pages = {645--658},
publisher = {IEEE},
title = {{PROVID: Progressive and Multimodal Vehicle Reidentification for Large-Scale Urban Surveillance}},
volume = {20},
year = {2018}
}

@article{Yang2015,
abstract = {This paper aims to highlight vision related tasks centered around 'car', which has been largely neglected by vision community in comparison to other objects. We show that there are still many interesting car-related problems and applications, which are not yet well explored and researched. To facilitate future car-related research, in this paper we present our on-going effort in collecting a large-scale dataset, 'CompCars', that covers not only different car views, but also their different internal and external parts, and rich attributes. Importantly, the dataset is constructed with a cross-modality nature, containing a surveillance-nature set and a web-nature set. We further demonstrate a few important applications exploiting the dataset, namely car model classification, car model verification, and attribute prediction. We also discuss specific challenges of the car-related problems and other potential applications that worth further investigations. The latest dataset can be downloaded at http://mmlab.ie.cuhk.edu.hk/ datasets/comp-cars/index.html.},
author = {Yang, Linjie and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
doi = {10.1109/CVPR.2015.7299023},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3973--3981},
title = {{A large-scale car dataset for fine-grained categorization and verification}},
volume = {07-12-June-2015},
year = {2015}
}

@inproceedings{liu2016deep,
  title={Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles},
  author={Liu, Hongye and Tian, Yonghong and Wang, Yaowei and Pang, Lu and Huang, Tiejun},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2167--2175},
  year={2016}
}

@INPROCEEDINGS{yan2017exploiting,  author={Yan, Ke and Tian, Yonghong and Wang, Yaowei and Zeng, Wei and Huang, Tiejun},  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},   title={Exploiting Multi-grain Ranking Constraints for Precisely Searching Visually-similar Vehicles},   year={2017},  volume={},  number={},  pages={562-570},  doi={10.1109/ICCV.2017.68}}

@article{CVIU_UA-DETRAC,
             author    = {Longyin Wen and Dawei Du and Zhaowei Cai and Zhen Lei and Ming{-}Ching Chang and
               Honggang Qi and Jongwoo Lim and Ming{-}Hsuan Yang and Siwei Lyu},
             title     = { {UA-DETRAC:} {A} New Benchmark and Protocol for Multi-Object Detection and Tracking},
             journal   = {Computer Vision and Image Understanding},
             year      = {2020}
             } 

@misc {Kristan2019a,
	year = {2019},
	author = {Matej Kristan and Jiri Matas and Ales Leonardis and Michael Felsberg and Roman Pflugfelder and Joni-Kristian Kamarainen and Luka Cehovin Zajc and Ondrej Drbohlav and Alan Lukezic and Amanda Berg and Abdelrahman Eldesokey and Jani Kapyla and Gustavo Fernandez},
	title = {The Seventh Visual Object Tracking VOT2019 Challenge Results}
}

@article{Kristan2018,
abstract = {The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative. Results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis and a "real-time" experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. A long-term tracking sub-challenge has been introduced to the set of standard VOT sub-challenges. The new subchallenge focuses on long-term tracking properties, namely coping with target disappearance and reappearance. A new dataset has been compiled and a performance evaluation methodology that focuses on long-term tracking capabilities has been adopted. The VOT toolkit has been updated to support both standard short-term and the new long-term tracking subchallenges. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the track-ers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website 60 .},
author = {Kristan, Matej and Leonardis, Ale{\v{s}} and Matas, Jiř{\'{i}} and Felsberg, Michael and Pflugfelder, Roman and Zajc, Luka{\v{C}}ehovinLukaLuka{\v{C}}ehovin and Voj{\'{i}}, Tom{\'{a}}{\v{s}} and Bhat, Goutam and Luke{\v{z}}i{\v{c}}, Alan and Eldesokey, Abdelrahman and Fern{\'{a}}ndez, Gustavo and Garc{\'{i}}a-Mart{\'{i}}n, Alvaro and Iglesias-Arias, Alvaro and {Aydin Alatan}, A and Gonz{\'{a}}lez-Garc{\'{i}}a, Abel and Petrosino, Alfredo and Memarmoghadam, Alireza and Vedaldi, Andrea and Muhi{\v{c}}, Andrej and He, Anfeng and Smeulders, Arnold and Perera, Asanka G and Li, Bo and Chen, Boyu and Kim, Changick and Xu, Changsheng and Xiong, Changzhen and Tian, Cheng and Luo, Chong and Sun, Chong and Hao, Cong and Kim, Daijin and Mishra, Deepak and Chen, Deming and Wang, Dong and Wee, Dongyoon and Gavves, Efstratios and Gundogdu, Erhan and Velasco-Salido, Erik and {Shahbaz Khan}, Fahad and Yang, Fan and Zhao, Fei and Li, Feng and Battistone, Francesco and {De Ath}, George and {K S Subrahmanyam}, Gorthi R and Bastos, Guilherme and Ling, Haibin and {Kiani Galoogahi}, Hamed and Lee, Hankyeol and Li, Haojie and Zhao, Haojie and Fan, Heng and Zhang, Honggang and Possegger, Horst and Li, Houqiang and Lu, Huchuan and Zhi, Hui and Li, Huiyun and Lee, Hyemin and {Jin Chang}, Hyung and Drummond, Isabela and Valmadre, Jack and {Spencer Martin}, Jaime and Chahl, Javaan and {Young Choi}, Jin and Li, Jing and Wang, Jinqiao and Qi, Jinqing and Sung, Jinyoung and Johnander, Joakim and Henriques, Joao and Choi, Jongwon and van de Weijer, Joost and {Rodr{\'{i}}guez Herranz}, Jorge and Mart{\'{i}}nez, Jos{\'{e}} M and Kittler, Josef and Zhuang, Junfei and Gao, Junyu and Grm, Klemen and Zhang, Lichao and Wang, Lijun and Yang, Lingxiao and Rout, Litu and Si, Liu and Bertinetto, Luca and Chu, Lutao and Che, Manqiang and {Edoardo Maresca}, Mario and Danelljan, Martin and Yang, Ming-Hsuan and Abdelpakey, Mohamed and Shehata, Mohamed and Kang, Myunggu and Lee, Namhoon and Wang, Ning and Miksik, Ondrej and Moallem, P and Vicente-Mo{\~{n}}ivar, Pablo and Senna, Pedro and Li, Peixia and Torr, Philip and {Mariam Raju}, Priya and Ruihe, Qian and Wang, Qiang and Zhou, Qin and Guo, Qing and Mart{\'{i}}n-Nieto, Rafael and {Krishna Gorthi}, Rama and Tao, Ran and Bowden, Richard and Everson, Richard and Wang, Runling and Yun, Sangdoo and Choi, Seokeon and Vivas, Sergio and Bai, Shuai and Huang, Shuangping and Wu, Sihang and Hadfield, Simon and Wang, Siwen and Golodetz, Stuart and Ming, Tang and Xu, Tianyang and Zhang, Tianzhu and Fischer, Tobias and Santopietro, Vincenzo and Struc, VitomiřVitomiř and Wei, Wang and Zuo, Wangmeng and Feng, Wei and Wu, Wei and Zou, Wei and Hu, Weiming and Zhou, Wengang and Zeng, Wenjun and Zhang, Xiaofan and Wu, Xiaohe and Wu, Xiao-Jun and Tian, Xinmei and Li, Yan and Lu, Yan and {Wei Law}, Yee and Wu, Yi and Demiris, Yiannis and Yang, Yicai and Jiao, Yifan and Li, Yuhong and Zhang, Yunhua and Sun, Yuxuan and Zhang, Zheng and Zhu, Zheng and Feng, Zhen-Hua and Wang, Zhihui and He, Zhiqun},
journal = {Chinese Academy of Sciences},
number = {1},
pages = {1--15},
title = {{VOT2018 results}},
url = {http://vision.fe.uni-lj.si/cvbase06/},
volume = {26},
year = {2018}
}

@article{Tao2016,
abstract = {In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-theart tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function. The strength of the matching function comes from being extensively trained generically, i.e., without any data of the target, using a Siamese deep neural network, which we design for tracking. Once learned, the matching function is used as is, without any adapting, to track previously unseen targets. It turns out that the learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-theart performance. Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot.},
archivePrefix = {arXiv},
arxivId = {1605.05863},
author = {Tao, Ran and Gavves, Efstratios and Smeulders, Arnold W.M.},
doi = {10.1109/CVPR.2016.158},
eprint = {1605.05863},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1420--1429},
title = {{Siamese instance search for tracking}},
volume = {2016-December},
year = {2016}
}

@misc{votchallenge,
  title = {{VOT challenge}},
  howpublished = {\url{https://www.votchallenge.net/}},
  note = {Accessed: 2020-09-07}
}

@misc{motchallenge,
  title = {{MOT challenge}},
  howpublished = {\url{https://motchallenge.net/}},
  note = {Accessed: 2020-09-07}
}

@misc{uadetracdataset,
  title = {{MOT challenge}},
  howpublished = {\url{https://detrac-db.rit.albany.edu/}},
  note = {Accessed: 2020-09-07}
}



@article{Dendorfer2019,
  author    = {Patrick Dendorfer and
               Seyed Hamid Rezatofighi and
               Anton Milan and
               Javen Shi and
               Daniel Cremers and
               Ian D. Reid and
               Stefan Roth and
               Konrad Schindler and
               Laura Leal{-}Taix{\'{e}}},
  title     = {{CVPR19} Tracking and Detection Challenge: How crowded can it get?},
  journal   = {CoRR},
  volume    = {abs/1906.04567},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.04567},
  eprinttype = {arXiv},
  eprint    = {1906.04567},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-04567.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Bewley2016,
abstract = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9{\%}. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
archivePrefix = {arXiv},
arxivId = {1602.00763},
author = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
doi = {10.1109/ICIP.2016.7533003},
eprint = {1602.00763},
isbn = {9781467399616},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {Computer Vision,Data Association,Detection,Multiple Object Tracking},
pages = {3464--3468},
title = {{Simple online and realtime tracking}},
volume = {2016-Augus},
year = {2016}
}

@misc{Zhang2017,
      title={Deep Reinforcement Learning for Visual Object Tracking in Videos}, 
      author={Da Zhang and Hamid Maei and Xin Wang and Yuan-Fang Wang},
      year={2017},
      eprint={1701.08936},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Rosenblatt1958ThePA,
title={The perceptron: a probabilistic model for information storage and organization in the brain.},
author={Frank F. Rosenblatt},
journal={Psychological review},
year={1958},
volume={65 6},
pages={386-408}
}

@book{Goodfellow-et-al-2016,
title={Deep Learning},
author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
publisher={MIT Press},
year={2016}
}

@Article{Everingham10,
   author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.",
   title = "The Pascal Visual Object Classes (VOC) Challenge",
   journal = "International Journal of Computer Vision",
   volume = "88",
   year = "2010",
   number = "2",
   month = jun,
   pages = "303--338",
}

@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {1405.0312},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}

@book{James2013,
  added-at = {2019-10-12T20:03:56.000+0200},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/2444186c86d18bddb4433c12fa126f6be/lopusz_kdd},
  interhash = {b3febabdc45a8629023cee7323dfbd86},
  intrahash = {444186c86d18bddb4433c12fa126f6be},
  keywords = {general_machine_learning},
  publisher = {Springer},
  timestamp = {2019-10-12T23:45:37.000+0200},
  title = {An Introduction to Statistical Learning: with Applications in R },
  url = {https://faculty.marshall.usc.edu/gareth-james/ISL/},
  year = 2013
}

@article{Davis2016,
  title={The relationship between Precision-Recall and ROC curves},
  author={Jesse Davis and Mark H. Goadrich},
  journal={Proceedings of the 23rd international conference on Machine learning},
  year={2006}
}

@book{salton_introduction_1983,
  added-at = {2018-11-04T16:45:24.000+0100},
  address = {New York, NY},
  author = {Salton, Gerard and McGill, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/27fc601c5b5346f8939027357c972c5b4/lepsky},
  interhash = {90e5e9500c919499099da9517aa8163e},
  intrahash = {7fc601c5b5346f8939027357c972c5b4},
  keywords = {information_retrieval},
  publisher = {McGraw-Hill},
  timestamp = {2018-11-07T09:14:29.000+0100},
  title = {Introduction to modern information retrieval},
  year = 1983
}

@inproceedings{Koch2011,
  title={Siamese Neural Networks for One-Shot Image Recognition},
  author={Gregory R. Koch},
  year={2015}
}

@article{Karras2020,
abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
archivePrefix = {arXiv},
arxivId = {1812.04948},
author = {Karras, Tero and Laine, Samuli and Aila, Timo},
doi = {10.1109/cvpr.2019.00453},
eprint = {1812.04948},
pages = {4396--4405},
title = {{A Style-Based Generator Architecture for Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1812.04948},
year = {2020}
}

@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to {\textless}sup{\textgreater}1{\textless}/sup{\textgreater}/{\textless}inf{\textgreater}2{\textless}/inf{\textgreater} everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.3156/jsoft.29.5_177_2},
eprint = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {2672--2680},
title = {{Generative adversarial nets}},
volume = {3},
year = {2014}
}

@article{Lou2019,
abstract = {The high similarities of different real-world vehicles and great diversities of the acquisition views pose grand challenges to vehicle re-identification (ReID), which traditionally maps the vehicle images into a high-dimensional embedding space for distance optimization, vehicle discrimination, and identification. To improve the discriminative capability and robustness of the ReID algorithm, we propose a novel end-to-end embedding adversarial learning network (EALN) that is capable of generating samples localized in the embedding space. Instead of selecting abundant hard negatives from the training set, which is extremely difficult if not impossible, with our embedding adversarial learning scheme, the automatically generated hard negative samples in the specified embedding space can greatly improve the capability of the network for discriminating similar vehicles. Moreover, the more challenging cross-view vehicle ReID problem, which requires the ReID algorithm to be robust with different query views, can also benefit from such a scheme based on the artificially generated cross-view samples. We demonstrate the promise of EALN through extensive experiments and show the effectiveness of hard negative and cross-view generation in facilitating vehicle ReID based on the comparisons with the state-of-the-art schemes.},
author = {Lou, Yihang and Bai, Yan and Liu, Jun and Wang, Shiqi and Duan, Ling Yu},
doi = {10.1109/TIP.2019.2902112},
issn = {19410042},
journal = {IEEE Transactions on Image Processing},
keywords = {Vehicle Re-Identification,cross-view,embedding adversarial learning,generative adversarial network,hard negatives},
number = {8},
pages = {3794--3807},
publisher = {IEEE},
title = {{Embedding Adversarial Learning for Vehicle Re-Identification}},
volume = {28},
year = {2019}
}


@article{Parkhi2015,
abstract = {The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M im- ages, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present meth- ods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.},
author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew},
doi = {10.5244/c.29.41},
number = {Section 3},
pages = {41.1--41.12},
title = {{Deep Face Recognition}},
year = {2015}
}

@article{Krause2013,
abstract = {While 3D object representations are being revived in the context of multi-view object class detection and scene understanding, they have not yet attained wide-spread use in fine-grained categorization. State-of-the-art approaches achieve remarkable performance when training data is plentiful, but they are typically tied to flat, 2D representations that model objects as a collection of unconnected views, limiting their ability to generalize across viewpoints. In this paper, we therefore lift two state-of-the-art 2D object representations to 3D, on the level of both local feature appearance and location. In extensive experiments on existing and newly proposed datasets, we show our 3D object representations outperform their state-of-the-art 2D counterparts for fine-grained categorization and demonstrate their efficacy for estimating 3D geometry from images via ultra-wide baseline matching and 3D reconstruction. {\textcopyright} 2013 IEEE.},
author = {Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
doi = {10.1109/ICCVW.2013.77},
isbn = {9781479930227},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {554--561},
title = {{3D object representations for fine-grained categorization}},
year = {2013}
}

@misc{nvidia,
  title = {{NVidia}},
  howpublished = {\url{https://www.nvidia.com/en-us/}},
  note = {Accessed: 2020-09-07}
}

@misc{veridataset,
  title = {{VeRI-776}},
  howpublished = {\url{https://vehiclereid.github.io/VeRi/}},
  note = {Accessed: 2020-09-07}
}

@misc{mscocodataset,
  title = {{MS COCO}},
  howpublished = {\url{https://cocodataset.org/}},
  note = {Accessed: 2020-09-07}
}

@misc{vot2019dataset,
  title = {{VOT 2019}},
  howpublished = {\url{https://www.votchallenge.net/vot2019/dataset.html}},
  note = {Accessed: 2020-09-07}
}

@misc{pkuvehicleiddataset,
  title = {{PKU VehicleID}},
  howpublished = {\url{https://www.pkuml.org/resources/pku-vehicleid.html}},
  note = {Accessed: 2020-09-07}
}

@misc{compcarsdataset,
  title = {{CompCars}},
  howpublished = {\url{http://ai.stanford.edu/~jkrause/cars/car_dataset.html}},
  note = {Accessed: 2020-09-07}
}

@misc{kittiobjectdetectiondataset,
  title = {{KITTI Object Detection}},
  howpublished = {\url{http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d}},
  note = {Accessed: 2020-09-07}
}

@misc{kittiobjecttrackingdataset,
  title = {{KITTI Object Tracking}},
  howpublished = {\url{http://www.cvlibs.net/datasets/kitti/eval_tracking.php}},
  note = {Accessed: 2020-09-07}
}

@misc{typicalfcnarchitecture,
  title = {{Fully Connected Neural Network Architecture}},
  howpublished = {\url{https://ch.mathworks.com/fr/solutions/deep-learning/convolutional-neural-network.html}},
  note = {Accessed: 2020-09-07}
}

@misc{typicalcnnarchitecture,
  title = {{Convolutional Neural Network Architecture}},
  howpublished = {\url{https://ch.mathworks.com/fr/solutions/deep-learning/convolutional-neural-network.html}},
  note = {Accessed: 2020-09-07}
}

@INPROCEEDINGS{Geiger2012CVPR,
  author = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
  title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2012}
}

@InProceedings{tmemotorwaydatasetpaper,
author = "Claudio Caraffi and Tomas Vojir and Jura Trefny and Jan Sochman and Jiri Matas",
title = "{A System for Real-time Detection and Tracking of Vehicles from a Single Car-mounted Camera}",
booktitle = "ITS Conference",
year = "2012",
pages = "975--982",
month = "Sep."
}

@article{Manmatha2017,
abstract = {Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.},
archivePrefix = {arXiv},
arxivId = {1706.07567},
author = {Manmatha, R. and Wu, Chao Yuan and Smola, Alexander J. and Krahenbuhl, Philipp},
doi = {10.1109/ICCV.2017.309},
eprint = {1706.07567},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2859--2867},
title = {{Sampling Matters in Deep Embedding Learning}},
volume = {2017-Octob},
year = {2017}
}
