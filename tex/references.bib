@article{peng2018sfv,
  title     = {{SFV: Reinforcement Learning of Physical Skills from Videos}},
  author    = {Peng, Xue Bin and Kanazawa, Angjoo and Malik, Jitendra and Abbeel, Pieter and Levine, Sergey},
  year      = 2018,
  month     = nov,
  journal   = {ACM Trans. Graph.},
  publisher = {ACM},
  address   = {New York, NY, USA},
  volume    = 37,
  number    = 6,
  articleno = 178,
  numpages  = 14,
  keywords  = {physics-based character animation, computer vision, video imitation, reinforcement learning, motion reconstruction}
}
@article{bertinetto2016siamfc,
  title         = {{Fully-convolutional siamese networks for object tracking}},
  author        = {Bertinetto, Luca and Valmadre, Jack and Henriques, Jo{\~{a}}o F. and Vedaldi, Andrea and Torr, Philip H.S.},
  year          = 2016,
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume        = {9914 LNCS},
  pages         = {850--865},
  doi           = {10.1007/978-3-319-48881-3_56},
  isbn          = 9783319488806,
  issn          = 16113349,
  abstract      = {The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object's appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.},
  archiveprefix = {arXiv},
  arxivid       = {1606.09549},
  eprint        = {1606.09549},
  keywords      = {Deep-learning,Object-tracking,Siamese-network,Similarity-learning},
  pmid          = 4520227
}
@article{bertinetto2016oneshot,
  title         = {{Learning feed-forward one-shot learners}},
  author        = {Bertinetto, Luca and Henriques, Jo{\~{a}}o F. and Valmadre, Jack and Torr, Philip H.S. and Vedaldi, Andrea},
  year          = 2016,
  journal       = {Advances in Neural Information Processing Systems},
  number        = {Nips},
  pages         = {523--531},
  issn          = 10495258,
  abstract      = {One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.},
  archiveprefix = {arXiv},
  arxivid       = {1606.05233},
  eprint        = {1606.05233}
}
@article{bawley2016simple,
  title         = {{Simple online and realtime tracking}},
  author        = {Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  year          = 2016,
  journal       = {Proceedings - International Conference on Image Processing, ICIP},
  volume        = {2016-Augus},
  pages         = {3464--3468},
  doi           = {10.1109/ICIP.2016.7533003},
  isbn          = 9781467399616,
  issn          = 15224880,
  abstract      = {This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9{\%}. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.},
  archiveprefix = {arXiv},
  arxivid       = {1602.00763},
  eprint        = {1602.00763},
  keywords      = {Computer Vision,Data Association,Detection,Multiple Object Tracking}
}
@misc{webhomographyrankinggithub,
  title        = {{Homography Ranking}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://github.com/mondrasovic/homography_ranking}}
}
@article{wen2020uadetrac,
  title   = {{ UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking}},
  author  = {Longyin Wen and Dawei Du and Zhaowei Cai and Zhen Lei and Ming{-}Ching Chang and others},
  year    = 2020,
  journal = {Computer Vision and Image Understanding}
}
@article{finn2017oneshotimitation,
  title         = {{One-Shot Visual Imitation Learning via Meta-Learning}},
  author        = {Chelsea Finn and Tianhe Yu and Tianhao Zhang and Pieter Abbeel and Sergey Levine},
  year          = 2017,
  journal       = {CoRR},
  volume        = {abs/1709.04905},
  url           = {http://arxiv.org/abs/1709.04905},
  archiveprefix = {arXiv},
  eprint        = {1709.04905},
  timestamp     = {Mon, 13 Aug 2018 16:47:36 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1709-04905.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{everingham2010pascalvoc,
  title   = {{The Pascal Visual Object Classes (VOC) Challenge}},
  author  = {Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},
  year    = 2010,
  month   = jun,
  journal = {International Journal of Computer Vision},
  volume  = 88,
  number  = 2,
  pages   = {303--338}
}
@book{forsyth2012computer,
  title     = {{Computer Vision - A Modern Approach, Second Edition.}},
  author    = {Forsyth, David A. and Ponce, Jean},
  year      = 2012,
  publisher = {Pitman},
  pages     = {1--791},
  isbn      = {978-0-273-76414-4},
  added-at  = {2015-03-23T00:00:00.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/24e26ef656dc22317d8ed6d0cfad2d3bc/dblp},
  ee        = {http://vig.pearsoned.com/store/product/1,1207,store-12521_isbn-013608592X,00.html},
  interhash = {a4dfa38179479650cae77eb72b893bb6},
  intrahash = {4e26ef656dc22317d8ed6d0cfad2d3bc},
  keywords  = {dblp},
  timestamp = {2015-06-18T09:49:18.000+0200}
}
@inproceedings{gabriel2003sotamot,
  title     = {{The state of the art in multiple object tracking under occlusion in video sequences}},
  author    = {Gabriel, Pierre F and Verly, Jacques G and Piater, Justus H and Genon, Andr{\'e}},
  year      = 2003,
  booktitle = {Advanced Concepts for Intelligent Vision Systems},
  pages     = {166--173}
}
@inproceedings{geiger2012cvpr,
  title     = {{Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite}},
  author    = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
  year      = 2012,
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@book{goodfellow2016dl,
  title     = {{Deep Learning}},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  year      = 2016,
  publisher = {MIT Press}
}
@misc{guo2019siamcar,
  title         = {{SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking}},
  author        = {Dongyan Guo and Jun Wang and Ying Cui and Zhenhua Wang and Shengyong Chen},
  year          = 2019,
  eprint        = {1911.07241},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{hadsell2006dimreduction,
  title    = {{Dimensionality reduction by learning an invariant mapping}},
  author   = {Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
  year     = 2006,
  journal  = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume   = 2,
  pages    = {1735--1742},
  doi      = {10.1109/CVPR.2006.100},
  isbn     = {0769525970},
  issn     = 10636919,
  abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that "similar" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distance measure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular ILE. {\textcopyright} 2006 IEEE.}
}
@article{he2018twofoldsiam,
  title         = {{A Twofold Siamese Network for Real-Time Object Tracking}},
  author        = {He, Anfeng and Luo, Chong and Tian, Xinmei and Zeng, Wenjun},
  year          = 2018,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages         = {4834--4843},
  doi           = {10.1109/CVPR.2018.00508},
  isbn          = 9781538664209,
  issn          = 10636919,
  abstract      = {Observing that Semantic features learned in an image classification task and Appearance features learned in a similarity matching task complement each other, we build a twofold Siamese network, named SA-Siam, for real-time object tracking. SA-Siam is composed of a semantic branch and an appearance branch. Each branch is a similaritylearning Siamese network. An important design choice in SA-Siam is to separately train the two branches to keep the heterogeneity of the two types of features. In addition, we propose a channel attention mechanism for the semantic branch. Channel-wise weights are computed according to the channel activations around the target position. While the inherited architecture from SiamFC [3] allows our tracker to operate beyond real-time, the twofold design and the attention mechanism significantly improve the tracking performance. The proposed SA-Siam outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks.},
  archiveprefix = {arXiv},
  arxivid       = {1802.08817},
  eprint        = {1802.08817}
}
@article{held2016goturn,
  title         = {{Learning to Track at 100 FPS with Deep}},
  author        = {Held, David and Thrun, Sebastian and Savarese, Silvio},
  year          = 2016,
  journal       = {Computer Vision – ECCV 2016 Lecture Notes in Computer Science},
  pages         = {749--765},
  url           = {http://davheld.github.io/GOTURN/GOTURN.html},
  abstract      = {Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance. Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training. We propose a method for offline training of neural networks that can track novel objects at test-time at 100 fps. Our tracker is significantly faster than previous methods that use neural networks for tracking, which are typically very slow to run and not practical for real-time applications. Our tracker uses a simple feed-forward network with no online training required. The tracker learns a generic relationship between object motion and appearance and can be used to track novel objects that do not appear in the training set. We test our network on a standard tracking benchmark to demonstrate our tracker's state-of-the-art performance. Further, our performance improves as we add more videos to our offline training set. To the best of our knowledge, our tracker 1 is the first neural-network tracker that learns to track generic objects at 100 fps.},
  archiveprefix = {arXiv},
  arxivid       = {1604.01802},
  eprint        = {1604.01802},
  keywords      = {deep learning,machine learning,neural networks,tracking}
}
@misc{hermans2017triplet,
  title         = {{In Defense of the Triplet Loss for Person Re-Identification}},
  author        = {Alexander Hermans and Lucas Beyer and Bastian Leibe},
  year          = 2017,
  eprint        = {1703.07737},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{hosang2017learningnms,
  title         = {{Learning non-maximum suppression}},
  author        = {Hosang, Jan and Benenson, Rodrigo and Schiele, Bernt},
  year          = 2017,
  journal       = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume        = {2017-January},
  pages         = {6469--6477},
  doi           = {10.1109/CVPR.2017.685},
  isbn          = 9781538604571,
  abstract      = {Object detectors have hugely profited from moving towards an end-to-end learning paradigm: proposals, features, and the classifier becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard NMS algorithm is still fully hand-crafted, suspiciously simple, and - being based on greedy clustering with a fixed distance threshold - forces a trade-off between recall and precision. We propose a new network architecture designed to perform NMS, using only boxes and their score. We report experiments for person detection on PETS and for general object categories on the COCO dataset. Our approach shows promise providing improved localization and occlusion handling.},
  archiveprefix = {arXiv},
  arxivid       = {1705.02950},
  eprint        = {1705.02950}
}
@article{huang2017speedacctradeoff,
  title         = {{Speed/accuracy trade-offs for modern convolutional object detectors}},
  author        = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and others},
  year          = 2017,
  journal       = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume        = {2017-Janua},
  pages         = {3296--3305},
  doi           = {10.1109/CVPR.2017.351},
  isbn          = 9781538604571,
  abstract      = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as "meta-architectures" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.},
  archiveprefix = {arXiv},
  arxivid       = {1611.10012},
  eprint        = {1611.10012}
}
@misc{chen2019rotbboxes,
  title         = {{Fast Visual Object Tracking with Rotated Bounding Boxes}},
  author        = {Bao Xin Chen and John K. Tsotsos},
  year          = 2019,
  eprint        = {1907.03892},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{deng2009imagenet,
  title     = {{ImageNet: A Large-Scale Hierarchical Image Database}},
  author    = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and others},
  year      = 2009,
  booktitle = {CVPR09},
  bibsource = {http://www.image-net.org/papers/deng2009imagenet.bib}
}
@article{jalal2012sotavot,
  title   = {{The State-of-the-Art in Visual Object Tracking.}},
  author  = {Jalal, Anand and Singh, Vrijendra},
  year    = 2012,
  month   = {01},
  journal = {Informatica (Slovenia)},
  volume  = 36,
  pages   = {227--248},
  issn    = {03505596}
}
@article{jiyan2007robustocclusion,
  title    = {{Robust occlusion handling in object tracking}},
  author   = {Jiyan, Pan and Bo, Hu},
  year     = 2007,
  journal  = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  number   = {June 2007},
  doi      = {10.1109/CVPR.2007.383453},
  isbn     = 1424411807,
  issn     = 10636919,
  abstract = {In object tracking, occlusions significantly undermine the performance of tracking algorithms. Unlike the existing methods that solely depend on the observed target appearance to detect occluders, we propose an algorithm that progressively analyzes the occlusion situation by exploiting the spatiotemporal context information, which is further double checked by the reference target and motion constraints. This strategy enables our proposed algorithm to make a clearer distinction between the target and occluders than existing approaches. To further improve the tracking performance, we rectify the occlusion-interfered erroneous target location by employing a variant-mask template matching operation. As a result, correct target location can always be obtained regardless of the occlusion situation. Using these techniques, the robustness of tracking under occlusions is significantly promoted. Experimental results have confirmed the effectiveness of our proposed algorithm. {\textcopyright} 2007 IEEE.}
}
@article{kalman1960linearfilter,
  title    = {{A new approach to linear filtering and prediction problems}},
  author   = {Kalman, R. E.},
  year     = 1960,
  journal  = {Journal of Fluids Engineering, Transactions of the ASME},
  volume   = 82,
  number   = 1,
  pages    = {35--45},
  doi      = {10.1115/1.3662552},
  issn     = {1528901X},
  abstract = {The classical filtering and prediction problem is re-examined using the Bode-Sliannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinitememory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix. {\textcopyright} 1960 by ASME.}
}
@inproceedings{koch2015siameseoneshot,
  title  = {{Siamese Neural Networks for One-Shot Image Recognition}},
  author = {Gregory R. Koch},
  year   = 2015
}
@article{krizhevsky2012classification,
  title    = {{ImageNet classification with deep convolutional neural networks}},
  author   = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year     = 2012,
  journal  = {Advances in Neural Information Processing Systems},
  volume   = 2,
  pages    = {1097--1105},
  isbn     = 9781627480031,
  issn     = 10495258,
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.}
}
@article{kuma2019vehiclereid,
  title         = {{Vehicle Re-identification: An Efficient Baseline Using Triplet Embedding}},
  author        = {Kuma, Ratnesh and Weill, Edwin and Aghdasi, Farzin and Sriram, Parthasarathy},
  year          = 2019,
  journal       = {Proceedings of the International Joint Conference on Neural Networks},
  volume        = {2019-July},
  doi           = {10.1109/IJCNN.2019.8852059},
  isbn          = 9781728119854,
  abstract      = {In this paper we tackle the problem of vehicle re-identification in a camera network utilizing triplet embeddings. Re-identification is the problem of matching appearances of objects across different cameras. With the proliferation of surveillance cameras enabling smart and safer cities, there is an ever-increasing need to re-identify vehicles across cameras. Typical challenges arising in smart city scenarios include variations of viewpoints, illumination and self occlusions. Most successful approaches for re-identification involve (deep) learning an embedding space such that the vehicles of same identities are projected closer to one another, compared to the vehicles representing different identities. Popular loss functions for learning an embedding (space) include contrastive or triplet loss. In this paper we provide an extensive evaluation of these losses applied to vehicle re-identification and demonstrate that using the best practices for learning embeddings outperform most of the previous approaches proposed in the vehicle re-identification literature. Compared to most existing state-of-the-art approaches, our approach is simpler and more straightforward for training utilizing only identity-level annotations, along with one of the smallest published embedding dimensions for efficient inference. Furthermore in this work we introduce a formal evaluation of a triplet sampling variant (batch sample) into the re-identification literature.},
  archiveprefix = {arXiv},
  arxivid       = {1901.01015},
  eprint        = {1901.01015}
}
@misc{lealtaixe2017tracking,
  title         = {{Tracking the Trackers: An Analysis of the State of the Art in Multiple Object Tracking}},
  author        = {Laura Leal-Taixé and Anton Milan and Konrad Schindler and Daniel Cremers and Ian Reid and others},
  year          = 2017,
  eprint        = {1704.02781},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{li2018siamrpn,
  title    = {{High Performance Visual Tracking with Siamese Region Proposal Network}},
  author   = {Li, Bo and Yan, Junjie and Wu, Wei and Zhu, Zheng and Hu, Xiaolin},
  year     = 2018,
  journal  = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages    = {8971--8980},
  doi      = {10.1109/CVPR.2018.00935},
  isbn     = 9781538664209,
  issn     = 10636919,
  abstract = {Visual object tracking has been a fundamental topic in recent years and many deep learning based trackers have achieved state-of-the-art performance on multiple benchmarks. However, most of these trackers can hardly get top performance with real-time speed. In this paper, we propose the Siamese region proposal network (Siamese-RPN) which is end-to-end trained off-line with large-scale image pairs. Specifically, it consists of Siamese subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. In the inference phase, the proposed framework is formulated as a local one-shot detection task. We can pre-compute the template branch of the Siamese subnetwork and formulate the correlation layers as trivial convolution layers to perform online tracking. Benefit from the proposal refinement, traditional multi-scale test and online fine-tuning can be discarded. The Siamese-RPN runs at 160 FPS while achieving leading performance in VOT2015, VOT2016 and VOT2017 real-time challenges.}
}
@article{lin2014mscoco,
  title         = {{Microsoft COCO: Common objects in context}},
  author        = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and others},
  year          = 2014,
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume        = {8693 LNCS},
  number        = {PART 5},
  pages         = {740--755},
  doi           = {10.1007/978-3-319-10602-1_48},
  issn          = 16113349,
  abstract      = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
  archiveprefix = {arXiv},
  arxivid       = {1405.0312},
  eprint        = {1405.0312}
}
@article{lin2014netinnet,
  title         = {{Network in network}},
  author        = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  year          = 2014,
  journal       = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
  pages         = {1--10},
  abstract      = {We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1312.4400v3},
  eprint        = {arXiv:1312.4400v3}
}
@article{liu2016ssd,
  title         = {{SSD: Single shot multibox detector}},
  author        = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and others},
  year          = 2016,
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume        = {9905 LNCS},
  pages         = {21--37},
  doi           = {10.1007/978-3-319-46448-0_2},
  isbn          = 9783319464473,
  issn          = 16113349,
  abstract      = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300 input, SSD achieves 74.3{\%} mAP on VOC2007 test at 59 FPS on a webnvidia Titan X and for 512 × 512 input, SSD achieves 76.9{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/ tree/ssd.},
  archiveprefix = {arXiv},
  arxivid       = {1512.02325},
  eprint        = {1512.02325},
  keywords      = {Convolutional neural network,Real-time object detection}
}
@article{liu2018provid,
  title     = {{PROVID: Progressive and Multimodal Vehicle Reidentification for Large-Scale Urban Surveillance}},
  author    = {Liu, Xinchen and Liu, Wu and Mei, Tao and Ma, Huadong},
  year      = 2018,
  journal   = {IEEE Transactions on Multimedia},
  publisher = {IEEE},
  volume    = 20,
  number    = 3,
  pages     = {645--658},
  doi       = {10.1109/TMM.2017.2751966},
  issn      = 15209210,
  abstract  = {Compared with person reidentification, which has attracted concentrated attention, vehicle reidentification is an important yet frontier problem in video surveillance and has been neglected by the multimedia and vision communities. Since most existing approaches mainly consider the general vehicle appearance for reidentification while overlooking the distinct vehicle identifier, such as the license plate number, they attain suboptimal performance. In this paper, we propose PROVID, a PROgressive Vehicle re-IDentification framework based on deep neural networks. In particular, our framework not only utilizes the multimodality data in large-scale video surveillance, such as visual features, license plates, camera locations, and contextual information, but also considers vehicle reidentification in two progressive procedures: coarse-To-fine search in the feature domain, and near-To-distant search in the physical space. Furthermore, to evaluate our progressive search framework and facilitate related research, we construct the VeRi dataset, which is the most comprehensive dataset from real-world surveillance videos. It not only provides large numbers of vehicles with varied labels and sufficient cross-camera recurrences but also contains license plate numbers and contextual information. Extensive experiments on the VeRi dataset demonstrate both the accuracy and efficiency of our progressive vehicle reidentification framework.},
  keywords  = {Progressive search,contextual information,deep learning,license plate verification,vehicle re-identification}
}
@article{manmatha2017samplingmatters,
  title         = {{Sampling Matters in Deep Embedding Learning}},
  author        = {Manmatha, R. and Wu, Chao Yuan and Smola, Alexander J. and Krahenbuhl, Philipp},
  year          = 2017,
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  volume        = {2017-Octob},
  pages         = {2859--2867},
  doi           = {10.1109/ICCV.2017.309},
  isbn          = 9781538610329,
  issn          = 15505499,
  abstract      = {Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.},
  archiveprefix = {arXiv},
  arxivid       = {1706.07567},
  eprint        = {1706.07567}
}
@misc{websiammotforkgithub,
  title        = {{SiamMOT - GitHub (forked project)}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://github.com/mondrasovic/siam-mot}}
}
@misc{websiammotoriggithub,
  title        = {{SiamMOT - GitHub (original project)}},
  note         = {Accessed: 2020-04-20},
  howpublished = {\url{https://github.com/amazon-research/siam-mot}}
}
@article{ondrasovic2021siamese,
  title   = {{Siamese Visual Object Tracking: A Survey}},
  author  = {Ondrašovič, Milan and Tarábek, Peter},
  year    = 2021,
  journal = {IEEE Access},
  volume  = 9,
  pages   = {110149--110172},
  doi     = {10.1109/ACCESS.2021.3101988}
}
@article{bradski2000opencv,
  title                = {{The OpenCV Library}},
  author               = {Bradski, G.},
  year                 = 2000,
  journal              = {Dr. Dobb's Journal of Software Tools},
  citeulike-article-id = 2236121,
  keywords             = {bibtex-import},
  posted-at            = {2008-01-15 19:21:54},
  priority             = 4
}
@inproceedings{barath2016novel,
  title     = {Novel Ways to Estimate Homography from Local Affine Transformations},
  author    = {Daniel Barath and Levente Hajder},
  year      = 2016,
  booktitle = {Proceedings of the 11th Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications {(VISIGRAPP} 2016) - Volume 3},
  pages     = {434--445},
  doi       = {10.5220/0005674904320443},
  url       = {https://doi.org/10.5220/0005674904320443},
  editor    = {Nadia Magnenat{-}Thalmann and Paul Richard and Lars Linsen and Alexandru C. Telea and Sebastiano Battiato and Francisco H. Imai and Jos{\'{e}} Braz},
  timestamp = {Wed, 17 Mar 2021 18:00:56 +0100},
  biburl    = {https://dblp.org/rec/conf/visapp/BarathH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@phdthesis{beck2016planar,
  title  = {Planar Homography Estimation from Traffic Streams via Energy Functional Minimization},
  author = {Beck, Graham and others},
  year   = 2016,
  school = {Johns Hopkins University}
}
@article{abdel2015direct,
  title   = {Direct Linear Transformation from Comparator Coordinates into Object Space Coordinates in Close-Range Photogrammetry*},
  author  = {Y.I Abdel-Aziz and H.M. Karara and Michael Hauck},
  year    = 2015,
  journal = {Photogrammetric Engineering \& Remote Sensing},
  volume  = 81,
  number  = 2,
  pages   = {103--107},
  doi     = {https://doi.org/10.14358/PERS.81.2.103},
  issn    = {0099-1112},
  url     = {https://www.sciencedirect.com/science/article/pii/S0099111215303086}
}
@article{parkhi2015deepface,
  title    = {{Deep Face Recognition}},
  author   = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew},
  year     = 2015,
  number   = {Section 3},
  pages    = {41.1--41.12},
  doi      = {10.5244/c.29.41},
  abstract = {The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M im- ages, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present meth- ods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.}
}
@article{redmon2016yolo,
  title         = {{You only look once: Unified, real-time object detection}},
  author        = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year          = 2016,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {2016-Decem},
  pages         = {779--788},
  doi           = {10.1109/CVPR.2016.91},
  isbn          = 9781467388504,
  issn          = 10636919,
  abstract      = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  archiveprefix = {arXiv},
  arxivid       = {1506.02640},
  eprint        = {1506.02640}
}
@article{redmon2017yolo9000,
  title         = {{YOLO9000: Better, faster, stronger}},
  author        = {Redmon, Joseph and Farhadi, Ali},
  year          = 2017,
  journal       = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  volume        = {2017-January},
  pages         = {6517--6525},
  doi           = {10.1109/CVPR.2017.690},
  isbn          = 9781538604571,
  abstract      = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
  archiveprefix = {arXiv},
  arxivid       = {1612.08242},
  eprint        = {1612.08242}
}
@misc{redmon2018yolov3,
  title         = {{YOLOv3: An Incremental Improvement}},
  author        = {Joseph Redmon and Ali Farhadi},
  year          = 2018,
  eprint        = {1804.02767},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{ren2017fasterrcnn,
  title         = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
  author        = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year          = 2017,
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume        = 39,
  number        = 6,
  pages         = {1137--1149},
  doi           = {10.1109/TPAMI.2016.2577031},
  issn          = {01628828},
  abstract      = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arXiv},
  arxivid       = {1506.01497},
  eprint        = {1506.01497},
  file          = {:E$\backslash$:/Data/Downloads/1506.01497.pdf:pdf},
  keywords      = {Object detection,convolutional neural network,region proposal},
  pmid          = 27295650
}
@incollection{rumelhart1986backprop,
  title                = {{Learning Internal Representations by Error Propagation}},
  author               = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year                 = 1986,
  booktitle            = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, {V}olume 1: {F}oundations},
  publisher            = {MIT Press},
  address              = {Cambridge, MA},
  pages                = {318--362},
  added-at             = {2008-02-26T11:58:58.000+0100},
  biburl               = {https://www.bibsonomy.org/bibtex/27c3d39c519530239660d33e66493ade1/schaul},
  citeulike-article-id = 2378884,
  description          = {idsia},
  editor               = {Rumelhart, David E. and Mcclelland, James L.},
  interhash            = {dd8485b30b80c7f35263bcb21ed81c1f},
  intrahash            = {7c3d39c519530239660d33e66493ade1},
  keywords             = {nn},
  priority             = 2,
  timestamp            = {2008-02-26T12:02:43.000+0100}
}
@book{salton1983introduction,
  title     = {{Introduction to modern information retrieval}},
  author    = {Salton, Gerard and McGill, Michael},
  year      = 1983,
  publisher = {McGraw-Hill},
  address   = {New York, NY},
  added-at  = {2018-11-04T16:45:24.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/27fc601c5b5346f8939027357c972c5b4/lepsky},
  interhash = {90e5e9500c919499099da9517aa8163e},
  intrahash = {7fc601c5b5346f8939027357c972c5b4},
  keywords  = {information_retrieval},
  timestamp = {2018-11-07T09:14:29.000+0100}
}
@article{schroff2015facenet,
  title         = {{FaceNet: A unified embedding for face recognition and clustering}},
  author        = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year          = 2015,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {07-12-June},
  pages         = {815--823},
  doi           = {10.1109/CVPR.2015.7298682},
  isbn          = 9781467369640,
  issn          = 10636919,
  abstract      = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63{\%}. On YouTube Faces DB it achieves 95.12{\%}. Our system cuts the error rate in comparison to the best published result [15] by 30{\%} on both datasets.},
  archiveprefix = {arXiv},
  arxivid       = {1503.03832},
  eprint        = {1503.03832}
}
@article{simonyan2015verydeepcnn,
  title         = {{Very deep convolutional networks for large-scale image recognition}},
  author        = {Simonyan, Karen and Zisserman, Andrew},
  year          = 2015,
  journal       = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
  pages         = {1--14},
  abstract      = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  arxivid       = {1409.1556}
}
@article{taigman2014deepface,
  title         = {{DeepFace: Closing the gap to human-level performance in face verification}},
  author        = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  year          = 2014,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages         = {1701--1708},
  doi           = {10.1109/CVPR.2014.220},
  isbn          = 9781479951178,
  issn          = 10636919,
  abstract      = {In modern face recognition, the conventional pipeline consists of four stages: detect ={\textgreater} align ={\textgreater} represent ={\textgreater} classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35{\%} on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27{\%}, closely approaching human-level performance.},
  archiveprefix = {arXiv},
  arxivid       = {1501.05703},
  eprint        = {1501.05703},
  pmid          = 21646680
}
@article{tang2019cityflow,
  title         = {{Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification}},
  author        = {Tang, Zheng and Naphade, Milind and Liu, Ming Yu and Yang, Xiaodong and Birchfield, Stan and others},
  year          = 2019,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {2019-June},
  pages         = {8789--8798},
  doi           = {10.1109/CVPR.2019.00900},
  isbn          = 9781728132938,
  issn          = 10636919,
  abstract      = {Urban traffic optimization using traffic cameras as sensors is driving the need to advance state-of-the-art multi-target multi-camera (MTMC) tracking. This work introduces CityFlow, a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID). We conducted an extensive experimental evaluation of baselines/state-of-the-art approaches in MTMC tracking, multi-target single-camera (MTSC) tracking, object detection, and image-based ReID on this dataset, analyzing the impact of different network architectures, loss functions, spatio-temporal models and their combinations on task effectiveness. An evaluation server is launched with the release of our benchmark at the 2019 AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to compare the performance of their newest techniques. We expect this dataset to catalyze research in this field, propel the state-of-the-art forward, and lead to deployed traffic optimization(s) in the real world.},
  archiveprefix = {arXiv},
  arxivid       = {1903.09254},
  eprint        = {1903.09254},
  keywords      = {Big Data,Datasets and Evaluation,Large Scale Methods,Motion and Tracking}
}
@article{tao2016sint,
  title         = {{Siamese instance search for tracking}},
  author        = {Tao, Ran and Gavves, Efstratios and Smeulders, Arnold W.M.},
  year          = 2016,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {2016-December},
  pages         = {1420--1429},
  doi           = {10.1109/CVPR.2016.158},
  isbn          = 9781467388504,
  issn          = 10636919,
  abstract      = {In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-theart tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function. The strength of the matching function comes from being extensively trained generically, i.e., without any data of the target, using a Siamese deep neural network, which we design for tracking. Once learned, the matching function is used as is, without any adapting, to track previously unseen targets. It turns out that the learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-theart performance. Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot.},
  archiveprefix = {arXiv},
  arxivid       = {1605.05863},
  eprint        = {1605.05863}
}
@article{tian2019fcos,
  title         = {{FCOS: Fully convolutional one-stage object detection}},
  author        = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
  year          = 2019,
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  volume        = {2019-Octob},
  pages         = {9626--9635},
  doi           = {10.1109/ICCV.2019.00972},
  isbn          = 9781728148038,
  issn          = 15505499,
  abstract      = {We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7{\%} in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: Https://tinyurl.com/FCOSv1.},
  archiveprefix = {arXiv},
  arxivid       = {1904.01355},
  eprint        = {1904.01355}
}
@misc{wang2020yolov4,
  title         = {{YOLOv4: Optimal Speed and Accuracy of Object Detection}},
  author        = {Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
  year          = 2020,
  eprint        = {2004.10934},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{wang2015votcnn,
  title    = {{Visual tracking with fully convolutional networks}},
  author   = {Wang, Lijun and Ouyang, Wanli and Wang, Xiaogang and Lu, Huchuan},
  year     = 2015,
  journal  = {Proceedings of the IEEE International Conference on Computer Vision},
  volume   = {2015 Inter},
  pages    = {3119--3127},
  doi      = {10.1109/ICCV.2015.357},
  isbn     = 9781467383912,
  issn     = 15505499,
  abstract = {We propose a new approach for general object tracking with fully convolutional neural network. Instead of treating convolutional neural network (CNN) as a black-box feature extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and classification task on ImageNet. The discoveries motivate the design of our tracking system. It is found that convolutional layers in different levels characterize the target from different perspectives. A top layer encodes more semantic features and serves as a category detector, while a lower layer carries more discriminative information and can better separate the target from distracters with similar appearance. Both layers are jointly used with a switch mechanism during tracking. It is also found that for a tracking target, only a subset of neurons are relevant. A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy and improve tracking accuracy. Extensive evaluation on the widely used tracking benchmark [36] shows that the proposed tacker outperforms the state-of-the-art significantly.}
}
@article{wang2019siammask,
  title         = {{Fast online object tracking and segmentation: A unifying approach}},
  author        = {Wang, Qiang and Zhang, Li and Bertinetto, Luca and Hu, Weiming and Torr, Philip H.S.},
  year          = 2019,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {2019-June},
  pages         = {1328--1338},
  doi           = {10.1109/CVPR.2019.00142},
  isbn          = 9781728132938,
  issn          = 10636919,
  abstract      = {In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017.},
  archiveprefix = {arXiv},
  arxivid       = {1812.05050},
  eprint        = {1812.05050},
  keywords      = {Deep Learning,Motion and Tracking,Vision Applications and Systems}
}
@article{wong2019yolonano,
  title   = {{YOLO Nano: a Highly Compact You Only Look Once Convolutional Neural Network for Object Detection}},
  author  = {Alexander Wong and Mahmoud Famouri and Mohammad Javad Shafiee and Francis Li and Brendan Chwyl and others},
  year    = 2019,
  journal = {2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)},
  pages   = {22--25}
}
@misc{dendorfer2020motchallenge,
  doi       = {10.48550/ARXIV.2010.07548},
  url       = {https://arxiv.org/abs/2010.07548},
  author    = {Dendorfer, Patrick and Ošep, Aljoša and Milan, Anton and Schindler, Konrad and Cremers, Daniel and Reid, Ian and Roth, Stefan and Leal-Taixé, Laura},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {MOTChallenge: A Benchmark for Single-Camera Multiple Target Tracking},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{yang2016encoderdecoder,
  title         = {{Object contour detection with a fully convolutional encoder-decoder network}},
  author        = {Yang, Jimei and Price, Brian and Cohen, Scott and Lee, Honglak and Yang, Ming Hsuan},
  year          = 2016,
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume        = {2016-Decem},
  pages         = {193--202},
  doi           = {10.1109/CVPR.2016.28},
  isbn          = 9781467388504,
  issn          = 10636919,
  abstract      = {We develop a deep learning algorithm for contour detection with a fully convolutional encoder-decoder network. Different from previous low-level edge detection, our algorithm focuses on detecting higher-level object contours. Our network is trained end-to-end on PASCAL VOC with refined ground truth from inaccurate polygon annotations, yielding much higher precision in object contour detection than previous methods. We find that the learned model generalizes well to unseen object classes from the same supercategories on MS COCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning. By combining with the multiscale combinatorial grouping algorithm, our method can generate high-quality segmented object proposals, which significantly advance the state-of-the-art on PASCAL VOC (improving average recall from 0.62 to 0.67) with a relatively small amount of candidates ({\~{}}1660 per image).},
  archiveprefix = {arXiv},
  arxivid       = {1603.04530},
  eprint        = {1603.04530}
}
@article{yu2016unitbox,
  title         = {{UnitBox: An advanced object detection network}},
  author        = {Yu, Jiahui and Jiang, Yuning and Wang, Zhangyang and Cao, Zhimin and Huang, Thomas},
  year          = 2016,
  journal       = {MM 2016 - Proceedings of the 2016 ACM Multimedia Conference},
  pages         = {516--520},
  doi           = {10.1145/2964284.2967274},
  isbn          = 9781450336031,
  abstract      = {In present object detection systems, the deep convolutional neural networks (CNNs) are utilized to predict bounding boxes of object candidates, and have gained performance advantages over the traditional region proposal methods. However, existing deep CNN methods assume the object bounds to be four independent variables, which could be regressed by the ℓ2 loss separately. Such an oversimplified assumption is contrary to the well-received observation, that those variables are correlated, resulting to less accurate localization. To address the issue, we firstly introduce a novel Intersection over Union (IoU) loss function for bounding box prediction, which regresses the four bounds of a predicted box as a whole unit. By taking the advantages of IoU loss and deep fully convolutional networks, the UnitBox is introduced, which performs accurate and efficient localization, shows robust to objects of varied shapes and scales, and converges fast. We apply UnitBox on face detection task and achieve the best performance among all published methods on the FDDB benchmark.},
  archiveprefix = {arXiv},
  arxivid       = {1608.01471},
  eprint        = {1608.01471},
  keywords      = {Bounding Box Prediction,IoU Loss,Object Detection}
}
@article{geetha2013automatic,
  title   = {Automatic rectification of perspective distortion from a single image using plane homography},
  author  = {Geetha Kiran, A and Murali, S},
  year    = 2013,
  journal = {J. Comput. Sci. Appl},
  volume  = 3,
  number  = 5,
  pages   = {47--58}
}
@article{bousaid2020perspective,
  title     = {Perspective distortion modeling for image measurements},
  author    = {Bousaid, Alexandre and Theodoridis, Theodoros and Nefti-Meziani, Samia and Davis, Steve},
  year      = 2020,
  journal   = {IEEE Access},
  publisher = {IEEE},
  volume    = 8,
  pages     = {15322--15331},
  doi       = {10.1109/ACCESS.2020.2966716}
}
@book{hartley2003multiple,
  title     = {Multiple View Geometry in Computer Vision},
  author    = {Hartley, Richard and Zisserman, Andrew},
  year      = 2003,
  publisher = {Cambridge University Press},
  address   = {USA},
  isbn      = {0521540518},
  edition   = 2,
  abstract  = {From the Publisher:A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Recent major developments in the theory and practice of scene reconstruction are described in detail in a unified framework. The book covers the geometric principles and how to represent objects algebraically so they can be computed and applied. The authors provide comprehensive background material and explain how to apply the methods and implement the algorithms directly.}
}
@article{hartley1997defense,
  title     = {In defense of the eight-point algorithm},
  author    = {Hartley, Richard I},
  year      = 1997,
  journal   = {IEEE Transactions on pattern analysis and machine intelligence},
  publisher = {IEEE},
  volume    = 19,
  number    = 6,
  pages     = {580--593},
  doi       = {10.1109/34.601246}
}
@article{lu2005perspective,
  title     = {Perspective rectification of document images using fuzzy set and morphological operations},
  author    = {Lu, Shijian and Chen, Ben M and Ko, Chi Chung},
  year      = 2005,
  journal   = {Image and Vision Computing},
  publisher = {Elsevier},
  volume    = 23,
  number    = 5,
  pages     = {541--553}
}
@inproceedings{miao2006perspective,
  title        = {Perspective rectification of document images based on morphology},
  author       = {Miao, Ligang and Peng, Silong},
  year         = 2006,
  booktitle    = {2006 International Conference on Computational Intelligence and Security},
  volume       = 2,
  pages        = {1805--1808},
  doi          = {10.1109/ICCIAS.2006.295374},
  organization = {IEEE}
}
@article{adel2014image,
  title   = {Image stitching based on feature extraction techniques: a survey},
  author  = {Adel, Ebtsam and Elmogy, Mohammed and Elbakry, Hazem},
  year    = 2014,
  journal = {International Journal of Computer Applications},
  volume  = 99,
  number  = 6,
  pages   = {1--8},
  doi     = {10.5120/17374-7818}
}
@inproceedings{gao2011constructing,
  title        = {Constructing image panoramas using dual-homography warping},
  author       = {Gao, Junhong and Kim, Seon Joo and Brown, Michael S},
  year         = 2011,
  booktitle    = {CVPR 2011},
  pages        = {49--56},
  doi          = {10.1109/CVPR.2011.5995433},
  organization = {IEEE}
}
@article{mariyanayagam2018poseestim,
  title         = {Pose estimation of a single circle using default intrinsic calibration},
  author        = {Damien Mariyanayagam and Pierre Gurdjos and Sylvie Chambon and Florent Brunet and Vincent Charvillat},
  year          = 2018,
  journal       = {CoRR},
  volume        = {abs/1804.04922},
  url           = {http://arxiv.org/abs/1804.04922},
  archiveprefix = {arXiv},
  eprint        = {1804.04922},
  timestamp     = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1804-04922.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}
@article{arrospide2010homography,
  title     = {Homography-based ground plane detection using a single on-board camera},
  author    = {Arr{\'o}spide, Jon and Salgado, Luis and Nieto, Marcos and Mohedano, Ra{\'u}l},
  year      = 2010,
  journal   = {IET Intelligent Transport Systems},
  publisher = {IET},
  volume    = 4,
  number    = 2,
  pages     = {149--160},
  doi       = {10.1049/iet-its.2009.0073}
}
@inproceedings{luo2010low,
  title        = {Low-cost implementation of bird's-eye view system for camera-on-vehicle},
  author       = {Luo, Lin-Bo and Koh, In-Sung and Min, Kyeong-Yuk and Wang, Jun and Chong, Jong-Wha},
  year         = 2010,
  booktitle    = {2010 Digest of Technical Papers International Conference on Consumer Electronics (ICCE)},
  pages        = {311--312},
  doi          = {10.1109/ICCE.2010.5418845},
  organization = {IEEE}
}
@article{zhang2000flexible,
  title     = {A flexible new technique for camera calibration},
  author    = {Zhang, Zhengyou},
  year      = 2000,
  journal   = {IEEE Transactions on pattern analysis and machine intelligence},
  publisher = {IEEE},
  volume    = 22,
  number    = 11,
  pages     = {1330--1334},
  doi       = {10.1109/34.888718}
}
@article{zhang2016flexible,
  title     = {A Flexible Online Camera Calibration Using Line Segments},
  author    = {Zhang, Yueqiang and Zhou, Langming and Liu, Haibo and Shang, Yang},
  year      = 2016,
  month     = {Jan},
  day       = {06},
  journal   = {Journal of Sensors},
  publisher = {Hindawi Publishing Corporation},
  volume    = 2016,
  doi       = {10.1155/2016/2802343},
  issn      = {1687-725X},
  url       = {https://doi.org/10.1155/2016/2802343}
}
@article{osuna2016multiobjective,
  title     = {A Multiobjective Approach to Homography Estimation},
  author    = {Osuna-Enciso, Valent{\'i}n and Cuevas, Erik and Oliva, Diego and Z{\'u}{\~{n}}iga, Virgilio and P{\'e}rez-Cisneros, Marco and Zald{\'i}var, Daniel},
  year      = 2015,
  month     = {Dec},
  day       = 28,
  journal   = {Computational Intelligence and Neuroscience},
  publisher = {Hindawi Publishing Corporation},
  volume    = 2016,
  pages     = 3629174,
  doi       = {10.1155/2016/3629174},
  issn      = {1687-5265},
  url       = {https://doi.org/10.1155/2016/3629174}
}
@inproceedings{mou2013robust,
  title        = {Robust homography estimation based on non-linear least squares optimization},
  author       = {Mou, Wei and Wang, Han and Seet, Gerald and Zhou, Lubing},
  year         = 2013,
  booktitle    = {2013 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  pages        = {372--377},
  doi          = {10.1109/ROBIO.2013.6739487},
  organization = {IEEE}
}
@article{fischler1981ransac,
  title      = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
  author     = {Fischler, Martin A. and Bolles, Robert C.},
  year       = 1981,
  month      = jun,
  journal    = {Commun. ACM},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = 24,
  number     = 6,
  pages      = {381–395},
  doi        = {10.1145/358669.358692},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/358669.358692},
  issue_date = {June 1981},
  numpages   = 15,
  keywords   = {location determination, image matching, scene analysis, model fitting, camera calibration, automated cartography}
}
@inproceedings{bose2004groundplane,
  title     = {Ground Plane Rectification by Tracking Moving Objects},
  author    = {Biswajit Bose and Eric Grimson},
  year      = 2004,
  booktitle = {IEEE International Workshop on Visual Surveillance and PETS}
}
@article{ondrasovic2021homography,
  title          = {Homography Ranking Based on Multiple Groups of Point Correspondences},
  author         = {Ondra{\v{s}}ovi{\v{c}}, Milan and Tar{\'a}bek, Peter},
  year           = 2021,
  journal        = {Sensors},
  volume         = 21,
  number         = 17,
  doi            = {10.3390/s21175752},
  issn           = {1424-8220},
  url            = {https://www.mdpi.com/1424-8220/21/17/5752},
  article-number = 5752,
  pubmedid       = 34502643
}
@inproceedings{ondrasovic2020foundations,
  title     = {Foundations for homography estimation in presence of redundant point correspondencies},
  author    = {Ondra{\v{s}}ovi{\v{c}}, Milan and Tar{\'a}bek, Peter},
  year      = 2020,
  booktitle = {Mathematics in science and technologies - proceedings of the MIST conference 2020},
  number    = {1. vydanie},
  pages     = {52--57}
}
@inproceedings{zhang2012homographytrack,
  title     = {Accurate Object Tracking Based on Homography Matrix},
  author    = {Zhang, Miaohui and Hou, Yandong and Hu, Zhentao},
  year      = 2012,
  booktitle = {2012 International Conference on Computer Science and Service System},
  pages     = {2310--2312},
  doi       = {10.1109/CSSS.2012.573}
}
@article{Mei2009,
  title   = {Efficient Homography-Based Tracking and 3-D Reconstruction for Single-Viewpoint Sensors},
  author  = {Mei, Christopher and Benhimane, Selim and Malis, Ezio and Rives, Patrick},
  year    = 2009,
  month   = {01},
  journal = {Robotics, IEEE Transactions on},
  volume  = 24,
  pages   = {1352--1364},
  doi     = {10.1109/TRO.2008.2007941}
}
@book{bradski2008learning,
  title     = {Learning OpenCV: Computer vision with the OpenCV library},
  author    = {Bradski, Gary and Kaehler, Adrian},
  year      = 2008,
  publisher = {" O'Reilly Media, Inc."}
}
@article{bernardin2008clearmot,
  title   = {Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics},
  author  = {Bernardin, Keni and Stiefelhagen, Rainer},
  year    = 2008,
  month   = {May},
  day     = 18,
  journal = {EURASIP Journal on Image and Video Processing},
  volume  = 2008,
  number  = 1,
  pages   = 246309,
  doi     = {10.1155/2008/246309},
  issn    = {1687-5281},
  url     = {https://doi.org/10.1155/2008/246309}
}
@article{munkres1957assignment,
  title     = {{Algorithms for the Assignment and Transportation Problems}},
  author    = {Munkres, James R.},
  year      = 1957,
  month     = {March},
  journal   = {Journal of the Society for Industrial and Applied Mathematics},
  volume    = 5,
  number    = 1,
  pages     = {32--38},
  added-at  = {2011-12-12T19:01:11.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/2bae4738783565247e1a2e7134bfef57f/gergie},
  groups    = {public},
  interhash = {41b44368bfc8a78101a53b094b6efffc},
  intrahash = {bae4738783565247e1a2e7134bfef57f},
  timestamp = {2011-12-12T19:01:11.000+0100},
  username  = {gergie}
}
@misc{shuai2021siammot,
  title         = {SiamMOT: Siamese Multi-Object Tracking},
  author        = {Bing Shuai and Andrew Berneshawi and Xinyu Li and Davide Modolo and Joseph Tighe},
  year          = 2021,
  eprint        = {2105.11595},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@incollection{paszke2019pytorch,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year      = 2019,
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  pages     = {8024--8035},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@misc{danelljan2019atom,
  title         = {ATOM: Accurate Tracking by Overlap Maximization},
  author        = {Martin Danelljan and Goutam Bhat and Fahad Shahbaz Khan and Michael Felsberg},
  year          = 2019,
  eprint        = {1811.07628},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{lin2018focal,
  title         = {Focal Loss for Dense Object Detection},
  author        = {Tsung-Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollár},
  year          = 2018,
  eprint        = {1708.02002},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{girshick2015fast,
  title         = {Fast R-CNN},
  author        = {Ross Girshick},
  year          = 2015,
  eprint        = {1504.08083},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{bergmann2019tracking,
  title     = {Tracking Without Bells and Whistles},
  author    = {Bergmann, Philipp and Meinhardt, Tim and Leal-Taixe, Laura},
  year      = 2019,
  month     = {Oct},
  journal   = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher = {IEEE},
  doi       = {10.1109/iccv.2019.00103},
  url       = {http://dx.doi.org/10.1109/ICCV.2019.00103}
}
@misc{wojke2017simple,
  title         = {Simple Online and Realtime Tracking with a Deep Association Metric},
  author        = {Nicolai Wojke and Alex Bewley and Dietrich Paulus},
  year          = 2017,
  eprint        = {1703.07402},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{zhou2020tracking,
  title         = {Tracking Objects as Points},
  author        = {Xingyi Zhou and Vladlen Koltun and Philipp Krähenbühl},
  year          = 2020,
  eprint        = {2004.01177},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{ioffe2015batchnorm,
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author        = {Sergey Ioffe and Christian Szegedy},
  year          = {2015},
  eprint        = {1502.03167},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{wu2018groupnorm,
  title         = {Group Normalization},
  author        = {Yuxin Wu and Kaiming He},
  year          = {2018},
  eprint        = {1803.08494},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{zhou2020batchgroupnorm,
  title         = {Batch Group Normalization},
  author        = {Xiao-Yun Zhou and Jiacheng Sun and Nanyang Ye and Xu Lan and Qijun Luo and Bo-Lin Lai and Pedro Esperanca and Guang-Zhong Yang and Zhenguo Li},
  year          = {2020},
  eprint        = {2012.02782},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{he2015resnet,
  title         = {Deep Residual Learning for Image Recognition},
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year          = {2015},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{lin2017fpn,
  title         = {Feature Pyramid Networks for Object Detection},
  author        = {Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
  year          = {2017},
  eprint        = {1612.03144},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{yu2019dla,
  title         = {Deep Layer Aggregation},
  author        = {Fisher Yu and Dequan Wang and Evan Shelhamer and Trevor Darrell},
  year          = {2019},
  eprint        = {1707.06484},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{pflugfelder2018indepth,
  title         = {An In-Depth Analysis of Visual Tracking with Siamese Neural Networks},
  author        = {Roman Pflugfelder},
  year          = {2018},
  eprint        = {1707.00569},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{zhu2018dasiamrpn,
  title         = {Distractor-aware Siamese Networks for Visual Object Tracking},
  author        = {Zheng Zhu and Qiang Wang and Bo Li and Wei Wu and Junjie Yan and Weiming Hu},
  year          = {2018},
  eprint        = {1808.06048},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{li2020figsiam,
  author  = {Li, Daqun and Yu, Yi},
  journal = {IEEE Access},
  title   = {Foreground Information Guidance for Siamese Visual Tracking},
  year    = {2020},
  volume  = {8},
  number  = {},
  pages   = {55905-55914},
  doi     = {10.1109/ACCESS.2020.2982261}
}
@misc{li2018siamrpnpp,
  title         = {SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks},
  author        = {Bo Li and Wei Wu and Qiang Wang and Fangyi Zhang and Junliang Xing and Junjie Yan},
  year          = {2018},
  eprint        = {1812.11703},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{wang2018learningattentions,
  author    = {Wang, Qiang and Teng, Zhu and Xing, Junliang and Gao, Jin and Hu, Weiming and Maybank, Stephen},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {4854-4863},
  doi       = {10.1109/CVPR.2018.00510}
}
@article{marvastizadeh2021survey,
  author  = {Marvasti-Zadeh, Seyed Mojtaba and Cheng, Li and Ghanei-Yakhdan, Hossein and Kasaei, Shohreh},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  title   = {Deep Learning for Visual Tracking: A Comprehensive Survey},
  year    = {2021},
  volume  = {},
  number  = {},
  pages   = {1-26},
  doi     = {10.1109/TITS.2020.3046478}
}
@article{li2019siamrm,
  author   = {Li, Daqun
              and Yu, Yi
              and Chen, Xiaolin},
  title    = {Object tracking framework with Siamese network and re-detection mechanism},
  journal  = {EURASIP Journal on Wireless Communications and Networking},
  year     = {2019},
  month    = {Nov},
  day      = {29},
  volume   = {2019},
  number   = {1},
  pages    = {261},
  abstract = {To improve the deficient tracking ability of fully-convolutional Siamese networks (SiamFC) in complex scenes, an object tracking framework with Siamese network and re-detection mechanism (Siam-RM) is proposed. The mechanism adopts the Siamese instance search tracker (SINT) as the re-detection network. When multiple peaks appear on the response map of SiamFC, a more accurate re-detection network can re-determine the location of the object. Meanwhile, for the sake of adapting to various changes in appearance of the object, this paper employs a generative model to construct the templates of SiamFC. Furthermore, a method of template updating with high confidence is also used to prevent the template from being contaminated. Objective evaluation on the popular online tracking benchmark (OTB) shows that the tracking accuracy and the success rate of the proposed framework can reach 79.8{\%} and 63.8{\%}, respectively. Compared to SiamFC, the results of several representative video sequences demonstrate that our framework has higher accuracy and robustness in scenes with fast motion, occlusion, background clutter, and illumination variations.},
  issn     = {1687-1499},
  doi      = {10.1186/s13638-019-1579-x},
  url      = {https://doi.org/10.1186/s13638-019-1579-x}
}
@article{liang2019lssiam,
  author  = {Z. {Liang} and J. {Shen}},
  journal = {IEEE Transactions on Image Processing},
  title   = {Local Semantic Siamese Networks for Fast Tracking},
  year    = {2020},
  volume  = {29},
  number  = {},
  pages   = {3351-3364},
  doi     = {10.1109/TIP.2019.2959256}
}
@article{shuai2020multisiamrcnn,
  title   = {Multi-object tracking with Siamese track-RCNN},
  author  = {Shuai, Bing and Berneshawi, Andrew G and Modolo, Davide and Tighe, Joseph},
  journal = {arXiv preprint arXiv:2004.07786},
  year    = {2020}
}
@inproceedings{vaquero2021siammt,
  author    = {Vaquero, Lorenzo and Mucientes, Manuel and Brea, Víctor M.},
  booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
  title     = {SiamMT: Real-Time Arbitrary Multi-Object Tracking},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {707-714},
  doi       = {10.1109/ICPR48806.2021.9412625}
}
@misc{lu2020retinatrack,
  title         = {RetinaTrack: Online Single Stage Joint Detection and Tracking},
  author        = {Zhichao Lu and Vivek Rathod and Ronny Votel and Jonathan Huang},
  year          = {2020},
  eprint        = {2003.13870},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{salscheider2020featurenms,
  title         = {FeatureNMS: Non-Maximum Suppression by Learning Feature Embeddings},
  author        = {Niels Ole Salscheider},
  year          = {2020},
  eprint        = {2002.07662},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{luo2019bagoftricksreid,
  title         = {Bag of Tricks and A Strong Baseline for Deep Person Re-identification},
  author        = {Hao Luo and Youzhi Gu and Xingyu Liao and Shenqi Lai and Wei Jiang},
  year          = {2019},
  eprint        = {1903.07071},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{dai2017dcnn,
  author    = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Deformable Convolutional Networks},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {764-773},
  doi       = {10.1109/ICCV.2017.89}
}
@misc{jaderberg2016stn,
  title         = {Spatial Transformer Networks},
  author        = {Max Jaderberg and Karen Simonyan and Andrew Zisserman and Koray Kavukcuoglu},
  year          = {2016},
  eprint        = {1506.02025},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{holschneider1990atrousconv,
  author    = {Holschneider, M.
               and Kronland-Martinet, R.
               and Morlet, J.
               and Tchamitchian, Ph.},
  editor    = {Combes, Jean-Michel
               and Grossmann, Alexander
               and Tchamitchian, Philippe},
  title     = {A Real-Time Algorithm for Signal Analysis with the Help of the Wavelet Transform},
  booktitle = {Wavelets},
  year      = {1990},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {286--297},
  isbn      = {978-3-642-75988-8}
}
@misc{jeon2017activeconv,
  title         = {Active Convolution: Learning the Shape of Convolution for Image Classification},
  author        = {Yunho Jeon and Junmo Kim},
  year          = {2017},
  eprint        = {1703.09076},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{vaswani2017attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2017},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{bahdanau2016additiveattention,
  title         = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author        = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  year          = {2016},
  eprint        = {1409.0473},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{yu2021dsa,
  title         = {Deformable Siamese Attention Networks for Visual Object Tracking},
  author        = {Yuechen Yu and Yilei Xiong and Weilin Huang and Matthew R. Scott},
  year          = {2021},
  eprint        = {2004.06711},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{zhu2018mdcnn,
  title         = {Deformable ConvNets v2: More Deformable, Better Results},
  author        = {Xizhou Zhu and Han Hu and Stephen Lin and Jifeng Dai},
  year          = {2018},
  eprint        = {1811.11168},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{chen2001motdynamicgraph,
  author    = {Hwann-Tzong Chen and Horng-Horng Lin and Tyng-Luh Liu},
  booktitle = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},
  title     = {Multi-object tracking using dynamical graph matching},
  year      = {2001},
  volume    = {2},
  number    = {},
  pages     = {II-II},
  doi       = {10.1109/CVPR.2001.990962}
}
@misc{papakis2021gcnnmatch,
  title         = {GCNNMatch: Graph Convolutional Neural Networks for Multi-Object Tracking via Sinkhorn Normalization},
  author        = {Ioannis Papakis and Abhijit Sarkar and Anuj Karpatne},
  year          = {2021},
  eprint        = {2010.00067},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{cuan2018deepsiammot,
  author    = {Cuan, Bonan and Idrissi, Khalid and Garcia, Christonhe},
  booktitle = {2018 IEEE 20th International Workshop on Multimedia Signal Processing (MMSP)},
  title     = {Deep Siamese Network for Multiple Object Tracking},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {1-6},
  doi       = {10.1109/MMSP.2018.8547137}
}
@article{lee2019motfpsn,
  author  = {Lee, Sangyun and Kim, Euntai},
  journal = {IEEE Access},
  title   = {Multiple Object Tracking via Feature Pyramid Siamese Networks},
  year    = {2019},
  volume  = {7},
  number  = {},
  pages   = {8181-8194},
  doi     = {10.1109/ACCESS.2018.2889442}
}
@misc{levi2021rethinking,
  title         = {Rethinking preventing class-collapsing in metric learning with margin-based losses},
  author        = {Elad Levi and Tete Xiao and Xiaolong Wang and Trevor Darrell},
  year          = {2021},
  eprint        = {2006.05162},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{zhang2019learning,
  title         = {Learning the Model Update for Siamese Trackers},
  author        = {Lichao Zhang and Abel Gonzalez-Garcia and Joost van de Weijer and Martin Danelljan and Fahad Shahbaz Khan},
  year          = {2019},
  eprint        = {1908.00855},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{zhang2021fairmot,
  title     = {FairMOT: On the Fairness of Detection and Re-identification in Multiple Object Tracking},
  volume    = {129},
  issn      = {1573-1405},
  url       = {http://dx.doi.org/10.1007/s11263-021-01513-4},
  doi       = {10.1007/s11263-021-01513-4},
  number    = {11},
  journal   = {International Journal of Computer Vision},
  publisher = {Springer Science and Business Media LLC},
  author    = {Zhang, Yifu and Wang, Chunyu and Wang, Xinggang and Zeng, Wenjun and Liu, Wenyu},
  year      = {2021},
  month     = {Sep},
  pages     = {3069–3087}
}
@misc{zhou2019centernet,
  title         = {Objects as Points},
  author        = {Xingyi Zhou and Dequan Wang and Philipp Krähenbühl},
  year          = {2019},
  eprint        = {1904.07850},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{li2018spatialawaresiam,
  title    = {Hierarchical spatial-aware Siamese network for thermal infrared object tracking},
  journal  = {Knowledge-Based Systems},
  volume   = {166},
  pages    = {71-81},
  year     = {2019},
  issn     = {0950-7051},
  doi      = {https://doi.org/10.1016/j.knosys.2018.12.011},
  url      = {https://www.sciencedirect.com/science/article/pii/S0950705118305987},
  author   = {Xin Li and Qiao Liu and Nana Fan and Zhenyu He and Hongzhi Wang},
  keywords = {Thermal infrared tracking, Similarity verification, Siamese convolutional neural network, Spatial-aware}
}