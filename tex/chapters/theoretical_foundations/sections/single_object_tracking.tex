\section{Visual Object Tracking}
\label{sec:VisualObjectTracking}

In this section, we will focus on the \emph{main topic} of this thesis: \textbf{tracking objects visually}. For simplicity, we will first introduce works within the single object tracking category. \sectionstr{}~\ref{sec:MultiObjectTracking} will continue with multiple object tracking.

Many research activities have been devoted to dealing with tracking of objects where visual input is the only information a model is provided with. Since the field of computer vision as a whole has transitioned from manual feature engineering in combination with shallow machine learning algorithms to using deep neural networks in end-to-end style, \gls{vot} as a subfield is no exception. We are going to discuss primarily \gls{sota} approaches, and if we do mention older, maybe even obsolete methods, it will be only for the sake of reference and historical motivation. Furthermore, unless stated otherwise, we will assume a general object tracking model.

\subsection{Initial Deep Learning-Based Solutions}
\label{ssec:InitialDeepLearningBasedSolutions}

At a time of publishing~\cite{held2016goturn}, most generic object trackers required online training from scratch, without taking advantage of available datasets to at least provide a starting point by initial offline training. This was the incentive behind development of the famous \gls{goturn}~\cite{held2016goturn}. This approach used to be \gls{sota} in single object tracking, but nowadays it is considered obsolete. A major issue is that the object has to be located initially, and occlusion handling is not performed as well as management of abrupt changes in position. So it is common for the object to drift away. Nevertheless, it stands to reason that the notion of leveraging data for offline training has pervaded the \gls{vot} community ever since. Nowadays, it is scarce to find a tracker that is trained online purely from scratch.

Given an initial state in a form of a \gls{bbox} belonging to the first frame (a search region), the network then crops a new region in the next frame and tries to find the location of the target object within this region. It practically performs a comparison of the current search region given the predicted target location from the previous frame. The previous frame is cropped and scaled so that it is centered on the target object. A small padding is made to allow for contextual information (see \figstr{}~\ref{fig:GOTURNFramework}). A key concept to highlight is that \gls{goturn} addresses the tracking as a box regression problem. In contrast, as will be presented later, similarity learning performs even better (section~\ref{ssec:TrackingUsingSiameseNetworks}).

\begin{figure}[t]
    \centerline{\includegraphics[width=0.7\linewidth]{figures/theoretical_foundations/goturn_framework.pdf}}
    \caption[\Gls{goturn} architecture]{The architecture of \gls{goturn} showing the input as a search region from the current frame where the target from the previous frame is to be searched for. The network essentially learns to localize the target within the cropped region. \externalsrc{\cite{held2016goturn}}}
    \label{fig:GOTURNFramework}
\end{figure}

\subsection{Fully Convolutional Tracking}
\label{ssec:FullyConvolutionalTracking}

We will start with a discussion about approaches that are so-called \emph{fully convolutional}. Transfer learning, e.g., exploiting an already pre-trained \gls{cnn} model to extract visual features, often comes with one drawback: the model accepts only a fixed input size. Although newer architectures can handle variable input size, this trait is more prevalent in object detection and segmentation than in basic task of image classification. A common approach is to resize the image to the required shape, but this may significantly distort important features. Using fully connected layers demands known dimensions in advance, which is complicated to acquire when dealing with input of diverse shape. Convolutional layers are invariant to input size, therefore an avoidance of fully connected layers may provide an answer. An efficient solution to replace fully connected layers utilizes $1 \times 1$ convolutions was notably propagated in \modelname{Network In Network} model~\cite{lin2014netinnet}. $1 \times 1$ filters were also used in the \modelname{Inception} architecture for dimensionality reduction and at the same time to increase the dimensionality of feature maps~\cite{szegedy2015inception}.

The \glspl{cnn} provide valuable spatial clues about the image content. As we touched upon in \sectionstr{}~\ref{ssec:ConvolutionalNeuralNetworks}, convolutional layers placed at the bottom (beginning) of the model tend to capture elementary details useful for discriminating between targets of similar appearance, whilst top layers (at the end) seize more abstract information regarding separate object categories. Thus, interclass variations are thoroughly captured in the top layers, and intraclass variations conversely in the bottom layers (see \figstr{}~\ref{fig:FullyCNNTrackingFeatureMaps}). This concept led the authors of~\cite{wang2015votcnn} to propose a fully convolutional visual object tracker that exploits different layers of the pre-trained VGG network~\cite{simonyan2015verydeepcnn} (see \figstr{}~\ref{fig:FullyCNNTrackingSNetGNet}). Thus, the model responsible for extracting visual features is no longer treated as a black-box. An in-depth study was conducted on the properties of \gls{cnn} features of the offline pre-trained model for the task of classification of ImageNet dataset~\cite{deng2009imagenet}. It was found out, as suggested above, that characterization from different perspectives is provided by convolutional layers at different levels.

The motivation for the utilization of different convolutional layers was that existing appearance-based tracking methods adopted either generative or discriminative models for separating the foreground from the background~\cite{wang2015votcnn}. To denote the distinction between the two, assume a classification task with the goal of estimating a function $f: X \to Y$, or $\probgiven{Y}{X}$, where $X$ represents the data and $Y$ the associated labels. A generative classifier would estimate parameters $\probgiven{X}{Y}$ and $\prob{Y}$ (in essence, the joint probability $\prob{X, Y}$) from the training data and then use the Bayes' rule to compute $\probgiven{Y}{X}$. On the other hand, a discriminative classifier would estimate parameters of $\probgiven{Y}{X}$ from the training data directly~\cite{ng2002discriminative}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.13\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/theoretical_foundations/fully_cnn_tracking_feature_maps_1.pdf}
        \caption[]{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.39\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/theoretical_foundations/fully_cnn_tracking_feature_maps_2.pdf}
        \caption[]{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.39\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/theoretical_foundations/fully_cnn_tracking_feature_maps_3.pdf}
        \caption[]{}
    \end{subfigure}
    \caption[Fully convolutional tracking]{A \gls{cnn} model trained on image classification task carries spatial information. \imgpartdesc{a} Input image with an associated ground truth mask. \imgpartdesc{b} Visualization of feature maps from convolutional layers placed at the bottom of the model, capturing differences between foreground and background of a particular object instance. \imgpartdesc{c} As opposed to the previous group of images, a more holistic, abstract view on the object category itself is provided by feature maps from top convolutional layers. The top row in the \imgpartdesc{b} and \imgpartdesc{c} represents feature maps, whereas the bottom row represents the corresponding saliency map with spatial information of the category. \externalsrc{\cite{wang2015votcnn}}}
    \label{fig:FullyCNNTrackingFeatureMaps}
\end{figure}

Authors of~\cite{wang2015votcnn} put together a list of three observations that summarize properties of the fully convolutional nature of a tracker proposed by them.
\begin{itemize}
    \item Despite a large receptive field of \gls{cnn} feature maps, few of them are activated and they are sparsely distributed, localized, and correlated to the regions of semantic objects.
    \item The majority of the feature maps can be considered noisy or irrelevant when discriminating a specific target object (foreground) from the background.
    \item Different layers encode different types of features (related to the intraclass or interclass variations discussed at the beginning).
\end{itemize}
The proposed architecture in accordance to the aforementioned observation is described in \figstr{}~\ref{fig:FullyCNNTrackingFeatureMaps} and \figstr{}~\ref{fig:FullyCNNTrackingSNetGNet}.

\begin{figure}[t]
    \centerline{\includegraphics[width=\linewidth]{figures/theoretical_foundations/fully_cnn_tracking_snet_gnet.pdf}}
    \caption[Architecture of fully convolutional tracking]{For a given image, a feature map selection is performed on two different layers of the VGG network~\cite{simonyan2015verydeepcnn} to select the most relevant ones. This selection is handled as a heat map regression problem. A general network (GNet) captures the category information, whilst the specific network (SNet) captures visual traits based on which the foreground is separated from the background. Two heat maps are produced by these networks forming a basis for subsequent target localization. Both of these networks have to be initialized in the first frame. Afterward, when a new frame is to be processed, a region of interest is centered at the last target location, and the whole process repeats. \externalsrc{\cite{wang2015votcnn}}}
    \label{fig:FullyCNNTrackingSNetGNet}
\end{figure}

\subsection{Tracking Using Siamese Networks}
\label{ssec:TrackingUsingSiameseNetworks}

Even though \glspl{cnn} condense valuable visual information into low dimensional space upon which a tracker may be built, it is still not sufficient in many situations during object tracking. The object representation from convolutional layers trained on image classification is not robust enough for dramatic visual changes and occlusion of varying intensity. As we discussed in \sectionstr{}~\ref{sec:LatentSpacesAndEmbeddings} dedicated to metric spaces and embeddings, an object representation supporting \gls{reid} requires different types of models, one of which is a \emph{Siamese network} (\sectionstr{}~\ref{ssec:SiameseAndTripletNetworks}). We have already mentioned our intention of utilizing custom metric space for tracking, and~\cite{bertinetto2016siamfc} were among the first ones to successfully demonstrate it.

We would like to remark that this branch of trackers formed the basis of our research. Its importance reached such a high level that we even composed an up-to-date comprehensive survey paper~\cite{ondrasovic2021siamese} solely focused on Siamese trackers and their fundamentals. At the time of writing these lines, this paper already has $2$ international citations.

Authors of~\cite{bertinetto2016siamfc} approved of the idea that visual feature extraction using \glspl{cnn} is pertinent to the robustness of the tracking algorithm, yet they advocated to train the visual model to a more general task of similarity learning rather than just classification. This observation and its further implementation was the main contribution of their work, achieving \gls{sota} performance back then. Broadly speaking, they trained a \emph{fully convolutional} Siamese network to locate an \emph{exemplar} image within a larger \emph{search} image (see \figstr{}~\ref{fig:FullyCNNSiamTrackingArch}). The model got the name \siamfc{}. We mentioned this to make the comparison easier because a lot of follow-up work has been done, producing models such as \sasiam{}~\cite{he2018twofoldsiam}, \siamrpn{}~\cite{li2018siamrpn}, \siammask{}~\cite{wang2019siammask}, \siammaske{}~\cite{chen2019rotbboxes}, and so forth.

Let $\gamma$ be a transformation that extracts visual features from the input, and $g$ be the function that combines two representations produced by the function $\gamma$. Siamese networks apply this identical transformation $\gamma$ to both inputs, search image $x$ and exemplar image $z$, and then combine the result as
\begin{equation}
    \func{f}{x, z} = \func{g}{\func{\gamma}{x}, \func{\gamma}{z}}.
\end{equation}
As such, when trivial distance or similarity measure is computed by the function $g$, then $\gamma$ can be deemed as the already introduced embedding.

\begin{figure}[t]
    \centerline{\includegraphics[width=0.6\linewidth]{figures/theoretical_foundations/fully_cnn_siam_tracking_architecture.pdf}}
    \caption[\siamfc{} architecture]{The fully-convolutional Siamese architecture produces a scalar-valued score map, the size of which depends on the size of the search image. The similarity function is computed for all sub-windows within the search image and stored in a 2D score map, rather than just a pure 1D embedding vector. This computation requires only one evaluation. In this image, the red and blue pixels in the output score map represent similarity values for the two sub-windows on the input. Best viewed in color. \externalsrc{\cite{bertinetto2016siamfc}}}
    \label{fig:FullyCNNSiamTrackingArch}
\end{figure}

The use of the Siamese architecture spawned a lot of follow-up work, and we will describe some that serve our purposes. A team of authors in~\cite{he2018twofoldsiam} made the following observation: features learned in an image classification task (denoted as semantic features) complement features learned in a similarity matching task (denoted as appearance features). They also suitably commented that the key to designing a high-performance tracker is to harness expressive features that are simultaneously discriminative and generalized. In light of this, they developed a model consisting of a semantic and an appearance branch (see \figstr{}~\ref{fig:TwofoldSiameseNetArchitecture}), with each branch being represented by a standard similarity-learning Siamese network (as in \siamfc{}~\cite{bertinetto2016siamfc}). However, an important distinction is that these two branches were trained separately, making them effectively heterogeneous to avoid any sharing of information. They reported that both branches were less powerful when trained jointly than when trained separately. The rationale behind such a decision was that each branch provides different features produced at different levels of abstraction, yet they complement each other. The merge of their respective outputs happens only during the testing time. Nowadays, joint training is prevalent, especially due to its effectiveness. Given the advantage of the hindsight, there are more important aspects of Siamese trackers to address in order to reap even greater benefits in terms of accuracy, for example, feature fusion.

\begin{figure}[t]
    \centerline{\includegraphics[width=\linewidth]{figures/theoretical_foundations/twofold_siamese_net_architecture.pdf}}
    \caption[\sasiam{} architecture]{The architecture of the \sasiam{} network. The \anet{} represents the appearance network and the \snet{} represents the semantic network. For the reason that this work builds on \siamfc{} model~\cite{bertinetto2016siamfc}, structures connected by dotted lines are exactly the same as in the \siamfc{} model. The last two convolutional layers provide features that are extracted afterward. The attention model determines the weight of each feature channel by simultaneous consideration of exemplar and context information. The fusion module is trivial and uses just $1 \times 1$ convolutions. As shown on the right, combining branches is allowed only when testing. \externalsrc{\cite{he2018twofoldsiam}}}
    \label{fig:TwofoldSiameseNetArchitecture}
\end{figure}

The \sasiam{} receives an input as a pair of image patches (see \figstr{}~\ref{fig:TwofoldSiameseNetArchitecture}) cropped from the initial (target, exemplar) frame and the current (search) frame. Let $z$, $z^s$ and $X$ be the image of exemplar, exemplar including the surrounding context and the search region, respectively. Dimensions of $x^s$ and $X$ are identical, $W_s \times H_s \times 3$. Dimensions of the exemplar $z$ located in the exact center of the region of $z^s$ are $W_t \times H_t \times 3$, such that $W_t < W_s$ and $H_t < H_s$. The appearance branch (\anet{}) takes $\rbrackets{z, X}$ as input and essentially clones the entire \siamfc{} network. Let $\func{f_a}{\cdot}$ denote the visual features extracted by the \anet{}. Then, the response map of this branch is given by
\begin{equation}
    \func{h_a}{z, X} = \func{corr}{\func{f_a}{z}, \func{f_a}{X}},
\end{equation}
where $\func{corr}{\cdot}$ is the correlation operation. The training of this branch is rather straightforward thanks to the abundance of training pairs $\rbrackets{z_i, X_i}$ with their related response map $Y_i$, for $i = 1, \dots, N$, with $N$ being the no. of chosen training samples. The parameters $\theta_a$ of the \anet{} model are optimized from scratch by minimizing the logistic loss function $\func{\mathcal{L}}{\cdot}$:
\begin{equation}
    \label{eq:ANetModelTraining}
    \underset{\theta_a}{\argmin}
    \frac{1}{N}
    \cbrackets{
        \func{\mathcal{L}}{
            \func{h_a}{z_i, X_i; \theta_a},
            Y_i
        }
    }.
\end{equation}

Analogically, the semantic branch (\snet{}) assumes as input a pair $\rbrackets{z^s, X}$. Contrary to the \anet{}, this model is pre-trained for the image classification task and its weights are frozen during the training phase. These last two convolutional layers of this model are of primary interest as their features provide abstraction at distinct levels. However, spatial resolutions are not alike. Let $\func{f_s}{\cdot}$ be the concatenated multilevel features. For the correlation operation ($\func{corr}{\cdot}$) to be usable, a special fusion module is introduced, implemented by a simple $1 \times 1$ convolution layer. The fusion operation is applied to features within the same layer, and this fused feature vector will be referred to as $\func{g}{\func{f_s}{X}}$.

Semantic features of a higher level are robust to appearance variation. This contributes to the generalization ability of the tracker but exacerbates its discriminative abilities. To circumvent this, the attention module is presented. The reasoning is that individual feature channels have varying importance for object tracking as far as different exemplars are concerned. The goal is then to assign a degree of importance (weight) to each channel for each exemplar. Still, the exemplar information is not sufficient, so the context must be supplied, too. The proposed attention module thus processes the feature map of $z^s$ instead of just $z$. Although the use of multilevel features in conjunction with the attention module delivers substantial progress for the semantic branch, it would be counterproductive for the appearance branch. Appearance features extracted from different convolutional layers lack sufficient differences in terms of expressiveness, as these features are dense whereas high-level semantic features are very sparse.

The attention module operates channel-wise. Assume the $i^{\text{th}}$ channel from some convolutional feature map with spatial dimensions of $22 \times 22$ is being processed. The tracking target is contained in the center, covering a region of $6 \times 6$. The entire feature map is divided into a grid of $3 \times 3$ cells. Within each grid, a max-pooling operation is applied followed by a simple neural network consisting of just $1$ hidden layer. The weight coefficient $\zeta_i$ for the particular $i^{\text{th}}$ channel is generated by the sigmoid activation function (see \figstr{}~\ref{fig:TwofoldSiameseNetAttentionModule}). The attention module incurs negligible computational overhead as it's only active during the target processing on the first frame. Later on, the weight coefficient is used to scale each feature map according to its importance. The response map for the semantic branch is produced as
\begin{equation}
    \func{h_s}{z^s, X} =
    \func{corr}{
        \func{g}{\zeta \cdot \func{f_s}{z}},
        \func{g}{\func{f_s}{X}}
    },
\end{equation}
where the no. of elements of $\zeta$ is equal to the no. of channels in $\func{f_s}{z}$, and the operator $\cdot$ indicates an element-wise multiplication.

\begin{figure}[t]
    \centerline{\includegraphics[width=0.6\linewidth]{figures/theoretical_foundations/twofold_siamese_net_attention_module.pdf}}
    \caption[\snet{} attention module]{The attention module of the \snet{} network. \externalsrc{\cite{he2018twofoldsiam}}}
    \label{fig:TwofoldSiameseNetAttentionModule}
\end{figure}

When training the \snet{} branch, only the fusion and the attention modules are updated. No fine-tuning techniques are taken advantage of, regardless of the potential improvement of the semantic branch alone. Authors informed about such experiments, and they resulted in diminished overall performance thanks to \anet{} and \snet{} becoming less heterogeneous. Let $\rbrackets{\subsup{z}{i}{s}, X_i}$ with their corresponding ground-truth response map $Y_i$, for $i = 1, \dots, N$, be the $N$ chosen training samples. The parameters $\theta_s$ of the \snet{} model are fitted using the logistic loss function $\func{\mathcal{L}}{\cdot}$ (equal to equation~\ref{eq:ANetModelTraining})
\begin{equation}
    \label{eq:SNetModelTraining}
    \underset{\theta_s}{\argmin}
    \frac{1}{N}
    \cbrackets{
        \func{\mathcal{L}}{
            \func{h_a}{\subsup{z}{i}{s}, X_i; \theta_a},
            Y_i
        }
    }.
\end{equation}
The inference phase involves computation of the overall heat map for which a weighted average of the two produced heat maps is used as shown below:
\begin{equation}
    \func{h}{z, z^s, X} = \lambda \func{h_a}{z, X} + \rbrackets{1 - \lambda} \func{h_s}{z^s, X}.
\end{equation}
This introduces another hyperparameter $\lambda$ (where $0 < \lambda < 1$) that can in practice be reliably estimated from the validation dataset.

The series of Siamese-based architectures for tracking continued with the idea of using the \gls{rpn}~\cite{li2018siamrpn} (see \sectionstr{}~\ref{ssec:FasterRCNN} for the same concept applied in object detection). Under the flag of end-to-end training, the \siamrpn{} model consists of a Siamese subnetwork for feature extraction (again, a duplicate of the \siamfc{}~\cite{bertinetto2016siamfc}) and \gls{rpn} as another subnetwork encompassing both classification and regression branch (see \figstr{}~\ref{fig:SiamRPNNetArchitecture}). The notable contribution is that the proposed framework is formulated as a local one-shot detection task in the inference phase (the first work to make such a step) (see \figstr{}~\ref{fig:SiamRPNOneShotDetection}). The template branch encodes the object appearance information for further foreground/background discrimination. Analogically, the \gls{bbox} from the first frame is the only exemplar for one-shot detection in the inference phase.

The region proposal subnetwork contains a pair-wise correlation section as well as a supervision section. Let $k$ denote the number of anchors. Then, the model has to output $2k$ channels for the classification and $4k$ channels for the regression. Following the established notation, the Siamese subnetwork produces feature maps $\func{\gamma}{z}$ and $\func{\gamma}{x}$. The pair-wise correlation splits $\func{\gamma}{z}$ into $\sbrackets{\func{\gamma}{z}}_{cls}$ and $\sbrackets{\func{\gamma}{z}}_{reg}$ while increasing the no. of channels (\figstr{}~\ref{fig:SiamRPNNetArchitecture}). Conversely, $\func{\gamma}{x}$ is also split into $\sbrackets{\func{\gamma}{x}}_{cls}$ and $\sbrackets{\func{\gamma}{x}}_{reg}$, but the no. of channels remains unchanged. The correlation, when computed on both branches, is given by
\begin{equation}
    \begin{aligned}
        \subsup{A}{w \times h \times 2k}{cls} & =
        \sbrackets{\func{\gamma}{x}}_{cls} \star \sbrackets{\func{\gamma}{z}}_{cls}, \\
        \subsup{A}{w \times h \times 4k}{reg} & =
        \sbrackets{\func{\gamma}{x}}_{reg} \star \sbrackets{\func{\gamma}{z}}_{reg},
    \end{aligned}
\end{equation}
where the template feature maps $\sbrackets{\func{\gamma}{z}}_{cls}$ and $\sbrackets{\func{\gamma}{z}}_{reg}$ stand in place of kernels in the convolution operation signified by the $\star$ character.

\begin{figure}[t]
    \centerline{\includegraphics[width=\linewidth]{figures/theoretical_foundations/siam_rpn_architecture.pdf}}
    \caption[\siamrpn{} architecture]{The pipeline starts with the original \siamfc{} network followed by the \gls{rpn} which has two branches: classification and regression. The output of the two branches is obtained using a pair-wise correlation. Foreground/background classification and the box regression are given by the $17 \times 17 \times 2k$ and $17 \times 17 \times 4k$ feature maps, respectively. \externalsrc{\cite{li2018siamrpn}}}
    \label{fig:SiamRPNNetArchitecture}
\end{figure}

The noteworthy formulation of tracking as one-shot detection was proposed as follows. In general terms, the goal is to minimize the average loss $\mathcal{L}$ of a predictor function $\func{\psi}{x; W}$ by finding its parameters $W$. When computed over a dataset of $N$ samples $x_i$ with corresponding labels $y_i$, $\forall i = 1, \dots, N$, it is given by
\begin{equation}
    \label{eq:SiamRPNOneShotGeneral}
    \underset{W}{\argmin}
    \cbrackets{
        \frac{1}{N}
        \sum_{i = 1}^{N}
        \func{\mathcal{L}}{
            \func{\psi}{x_i; W},
            y_i
        }
    }.
\end{equation}
\emph{One-shot learning} aims to learn $W$ when only a single template $z$ is available.
Discriminative one-shot learning tackles a major challenge of \emph{learning to learn}, by finding a mechanism to integrate category information into the learner~\cite{bertinetto2016oneshot}. If we consider a meta-learning feed-forward function $\omega$ that maps $\rbrackets{z_i; W'}$ to $W$, then the problem can be stated as
\begin{equation}
    \label{eq:SiamRPNOneShotMetaLearning}
    \underset{W'}{\argmin}
    \cbrackets{
        \frac{1}{N}
        \sum_{i = 1}^{N}
        \func{\mathcal{L}}{
            \func{\psi}{x_i; \func{\omega}{z_i, W'}},
            y_i
        }
    }.
\end{equation}
In this setting, this objective function can be re-written in terms of the Siamese subnetwork feature extraction $\gamma$ and region proposal subnetwork $\Psi$ as
\begin{equation}
    \label{eq:SiamRPNOneShoCombo}
    \underset{W}{\argmin}
    \cbrackets{
        \frac{1}{N}
        \sum_{i = 1}^{N}
        \func{\mathcal{L}}{
            \func{\Psi}{\func{\gamma}{x_i; W}; \func{\gamma}{z_i; W}},
            y_i
        }
    }.
\end{equation}
The template branch provides training parameters to predict the kernel for the detection task, a typical example of the \emph{learning to learn} process. The template branch, therefore, embeds necessary category information into the kernel that is subsequently utilized for detection (\figstr{}~\ref{fig:SiamRPNOneShotDetection}).

\begin{figure}[t]
    \centerline{\includegraphics[width=0.7\linewidth]{figures/theoretical_foundations/siam_rpn_one_shot_detection.pdf}}
    \caption[Tracking as one-shot detection]{Tracking is framed as one-shot detection here. First, the template branch predicts the weights of the kernels for the \gls{rpn} using the first frame. Later on, only the detection branch is retained so the framework can be thought of as a local detection network. \externalsrc{\cite{li2018siamrpn}}}
    \label{fig:SiamRPNOneShotDetection}
\end{figure}

Later on, a fork of publications emerged with an endeavor to improve the tracking performance by estimating not only a regular axis-aligned \gls{bbox}, but a rotated box, too. Put into perspective, the rotated \gls{bbox}, as opposed to an ordinary, axis-aligned, contains the minimal amount of background pixels~\cite{chen2019rotbboxes}. Thus, datasets with rotated \glspl{bbox} provide tighter enclosed boxes. Additionally, the orientation information may be useful for solving different computer vision problems, such as action classification.

Inspiration from techniques for the problem of object segmentation yielded another approach where the tracking process was assisted with additional semi-supervised object segmentation~\cite{wang2019siammask}. The relevant contribution is the augmentation of the training loss with a binary segmentation task. Further, once trained, the model (dubbed \siammask{}) relies exclusively upon a single \gls{bbox} initialization and operates online while producing rotated \glspl{bbox} instead of axis-aligned ones together with class-agnostic object segmentation masks. Notwithstanding its convenience, a single rectangle frequently fails to represent the object appropriately, hence the motivation to generate additional segmentation masks.

As always, the \siamfc{}~\cite{bertinetto2016siamfc} was employed as the fundamental building block. However, a notable alternation consisted of the use of a depth-wise cross-correlation layer instead of a simple cross-correlation layer. The latter one compresses all the information into one channel, impeding the potential to encode richer information about the target object. As a reminder, the original model used $6 \times 6 \times 128$ and $22 \times 22 \times 128$ tensors to produce a $17 \times 17 \times 1$ response map (\figstr{}~\ref{fig:FullyCNNSiamTrackingArch}). Here, a multi-channel response maps are utilized (\figstr{}~\ref{fig:SiamMaskArchitecture}).

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.68\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/theoretical_foundations/siam_mask_architecture_3_branch.pdf}
        \caption[]{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/theoretical_foundations/siam_mask_architecture_2_branch_head.pdf}
        \caption[]{}
    \end{subfigure}
    \caption[\siammask{} architecture]{A schematic illustration of the two \siammask{} variants: \imgpartdesc{a} a three-branch architecture (full), \imgpartdesc{b} a two-branch architecture (head). Depth-wise correlation layers are adopted again. \externalsrc{\cite{wang2019siammask}}}
    \label{fig:SiamMaskArchitecture}
\end{figure}

An incremental improvement of \siammask{} model came when~\cite{chen2019rotbboxes} proposed a novel, efficient algorithm for the estimation of the \gls{bbox} rotation when the object segmentation mask is given. Particularly, a mask produced by the \siammask{} model, as this work builds on top of~\cite{wang2019siammask}, under the derived name \siammaske{}. In addition, their approach can be used to generate a rotated box ground truth from any segmentation datasets to train a rotation angle regression model. To estimate the rotation angle, they adopted the least-squared scheme as part of the ellipse fitting algorithm.

The idea to employ fully convolutional networks seems to pertain to the modern computer vision community. Besides a simpler model, the fully convolutional design often leads to a reduced number of hyperparameters. One such an architecture (a descendant of the famous \siamfc{}~\cite{bertinetto2016siamfc} model) has been recently proposed, named \siamcar{}~\cite{guo2019siamcar}. This approach relies on the decomposition of the task of \gls{vot} into two subproblems: classification for pixel category and regression for object \gls{bbox} at the given pixel. The leading concept of the article is that this tracker operates in an end-to-end, per-pixel manner. The authors managed to avoid the use of anchors as well as region proposals, hence reducing the need for human intervention. The use of the two aforementioned traits commonly leads to sensitivity to dimensions and aspect ratios of the anchor boxes, which requires expertise on hyperparameter tuning for successful tracking.

\begin{figure}[t]
    \centerline{\includegraphics[width=\linewidth]{figures/theoretical_foundations/siamcar_architecture.pdf}}
    \caption[\siamcar{} architecture]{\siamcar{} architecture. The left side consists of the original \siamfc{}~\cite{bertinetto2016siamfc} model, with a simple amendment of using depth-wise correlation for multi-channel response map extraction. The right side depicts the subnetworks for foreground/background classification and \gls{bbox} regression. \externalsrc{\cite{guo2019siamcar}}}
    \label{fig:SiamCARArchitecture}
\end{figure}

An indispensable part of localization are low-level features like edges, corners, and so on, whereas high-level features strengthen the representational power from the semantic point of view, which is crucial for discrimination. Authors fused low-level and high-level features from the last $3$ residual blocks of the \gls{resnet}-50 backbone, forming a unity after concatenation.

An important observation was made that locations further away from the object center may aggravate the predicted box as they can be considered of low-quality. To diminish the effect of such locations, another branch alongside the classification branch to suppress the outliers is introduced, based on the concept of \emph{centerness}, borrowed from the~\cite{tian2019fcos}. This branch outputs a feature map where each point indicates the \emph{centerness} score for the corresponding location. This concept was also utilized within the base architecture we tested our experiments on. We can say that \emph{centerness} is a very general concept, and practically it represents a weighting mechanism to penalize areas within the \gls{roi} that most likely do not contain the target object.

\subsubsection{Conclusions Made In the Survey Paper}

Due to space limitations, in this short section we summarize the most important conclusions and observations that we made in our survey paper~\cite{ondrasovic2021siamese} when researching Siamese-based visual object trackers.

In the referred survey, we aimed to identify and elaborate on the most significant challenges the Siamese trackers face. The objective was to answer what design decisions the authors had made and what problems they had attempted to address. This treatise could be thought of as an in-depth analysis of the core principles on which Siamese trackers operate together with discussion of the underlying motivation. In addition, we also provided an up-to-date qualitative and quantitative comparison of the prominent Siamese trackers on established benchmarks, since the last survey that involved thorough discusssions of Siamese trackers was published in $2018$~\cite{pflugfelder2018indepth}. Last but not the least, we discussed the current trends in developing Siamese trackers at the time of writing the article as well.

We have to emphasize that Siamese trackers are a research direction in \gls{vot} with great potential. In practical terms, they belong to the fastest trackers with the ``accuracy-to-speed'' ratio being their primary strength. Contrary to the initial expectation, we realized that fast trackers were also among the most accurate ones (with some existing exceptions). Simply put, high processing speed is an inherent property of Siamese trackers. Nevertheless, there are existing drawbacks that require research attention. The presence of distractors is in our paper often mentioned as one of the leading causes of problems for this type of trackers. Our quantitative evaluations indicate that trackers where the presence of semantic background is explicitly treated often yield the top performance. Siamese metric learning is powerful enough to encompass numerous visual variations, but in case there are distractors present, then additional steps conditionally executed seem to contribute positively. To name a few, there are the explicit distractor-awareness~\cite{zhu2018dasiamrpn}, custom sampling strategies for foreground/background discrimination~\cite{li2020figsiam}, or conditional object re-detection~\cite{li2019siamrm}. Besides, there is a plethora of examples where \gls{rpn} was used for object proposals even for \gls{sot} in Siamese trackers. We venture to claim that the top-performing trackers exploited the above-mention \gls{rpn} head, e.g., \cite{li2018siamrpn, zhu2018dasiamrpn, li2018siamrpnpp}. A comprehensive survey concerning deep visual object tracking by Marvasti-Zadeh~\etal{}~\cite{marvastizadeh2021survey} also reached similar conclusion.

The utilization of cross-correlation has a great share of the leading performance in terms of their effectiveness. But the original single-channel formulation from~\cite{bertinetto2016siamfc} has been improved into a multi-channel cross-correlation that has been in use up to date. It was argued that a single channel did not capture sufficient information~\cite{guo2019siamcar}, thus multi-channel cross-correlation layers were used instead~\cite{li2018siamrpnpp}. On top of that, nce multiple channels are present, we observed an emerging trend in using various attention mechanisms to aid the feature selection~\cite{wang2018learningattentions}. As we will see further, multi-channel cross-correlation was also exploited in our work from a practical standpoint.

Speaking of cross-correlation, its core principle of performing a ``learned template matching'' using the exemplar and the search region, it raises the question whether and how the exemplar template should change during the training. Several works have remarked that incorporating memory or template updating strategies could potentially enhance the tracker performance, e.g,~\cite{bertinetto2016siamfc, liang2019lssiam}. It seems that relying solely upon the exemplar image from the initial frame may have detrimental effects as the object undergoes severe visual deformations, so the tracker may eventually loose its track.

Even though our discussed survey focused on \gls{sot}, there are emerging works where Siamese architectures such as \siamfc{}~\cite{bertinetto2016siamfc} were integrated into a \gls{mot} pipeline. For instance, Shuai~\etal{}~\cite{shuai2020multisiamrcnn} proposed one such framework that can simultaneously handle object tracking, detection, and \gls{reid}. In addition, the formulation allows the use of any Siamese tracker, which is a great contribution. Another trivial, yet very effective extension of the often-mentioned \siamfc{} tracker was to utiliation $n$ exemplars to produce $n$ response maps and, therefore, to perform tracking of $n$ objects at the same time~\cite{vaquero2021siammt}. We believe this paradigm of tracking is yet to uncover its full potential. As a matter of fact, our research ended up with working with \siammot{}~\cite{shuai2021siammot} (Section~\ref{sec:SiamMOT}) architecture, which we will introduce in great detail later on. It is a multi-object tracker that practically encompasses some of the best approaches we have discussed so far into an end-to-end framework, such as Siamese tracker (multi-channel cross-correlation), \gls{rpn} head, centerness, feature fusion, and much more.
