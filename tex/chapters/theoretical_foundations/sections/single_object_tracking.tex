\section{Single Object Tracking}
\label{sec:SingleObjectTracking}

% ##############################################################################
\subsection{Initial Deep Learning-Based Solutions}
\label{ssec:InitialDeepLearningBasedSolutions}

At the time of publishing the work of Held~\etal{}~\cite{held2016goturn}, most generic object trackers required online training from scratch, without taking advantage of available datasets to at least provide a starting point by initial offline training. This was the incentive behind the development of the famous \gls{goturn}~\cite{held2016goturn}. This approach used to be \gls{sota} in single-object tracking, but nowadays it is considered obsolete. A major issue is that the object has to be located initially, and occlusion handling is not performed as well as management of abrupt changes in position. So it is common for the object to drift away. Nevertheless, it stands to reason that the notion of leveraging data for offline training has pervaded the \gls{vot} community ever since. Nowadays, it is scarce to find a tracker that is trained online purely from scratch.

Given an initial state in a form of a \gls{bbox} belonging to the first frame (a search region), the network then crops a new region in the next frame and tries to find the location of the target object within this region. It practically performs a comparison of the current search region given the predicted target location from the previous frame (\figtext{}~\ref{fig:GOTURNFramework}). A key concept to highlight is that \gls{goturn} addresses the tracking as a box regression problem.

% ------------------------------------------------------------------------------
\begin{figure}[t]
    \centerline{\includegraphics[width=0.7\linewidth]{figures/theoretical_foundations/goturn_framework.pdf}}
    \caption[\Gls{goturn} architecture]{The architecture of \gls{goturn} showing the input as a search region from the current frame where the target from the previous frame is to be searched for. The network essentially learns to localize the target within the cropped region. \externalsrc{\cite{held2016goturn}}}
    \label{fig:GOTURNFramework}
\end{figure}
% ------------------------------------------------------------------------------

% ##############################################################################
\subsection{Fully Convolutional Tracking}
\label{ssec:FullyConvolutionalTracking}

Transfer learning, \egtext{}, exploiting an already pre-trained \gls{cnn} model to extract visual features, often comes with one drawback: the model accepts only a fixed input size. Although newer architectures can handle variable input size, this trait is more prevalent in object detection and segmentation than in the basic task of image classification. A common approach is to resize the image to the required shape, but this may significantly distort important features. Using fully connected layers demands known dimensions in advance, which is complicated to acquire when dealing with the input of diverse shapes. Convolutional layers are invariant to input size, therefore an avoidance of fully connected layers may provide an answer. An efficient solution to replace fully connected layers utilizes $1 \times 1$ convolutions was notably propagated in \modelname{Network In Network} model~\cite{lin2014netinnet}. $1 \times 1$ filters were also used in the \modelname{Inception} architecture for dimensionality reduction and at the same time to increase the dimensionality of feature maps~\cite{szegedy2015inception}.

The \glspl{cnn} provide valuable spatial clues about the image content. Thus, interclass variations are thoroughly captured in the top layers, and intraclass variations conversely in the bottom layers (\figtext{}~\ref{fig:FullyCNNTrackingFeatureMaps}). This concept led the authors of~\cite{wang2015votcnn} to propose a fully convolutional visual object tracker that exploits different layers of the pre-trained VGG network~\cite{simonyan2015verydeepcnn}. As a result, the model responsible for extracting visual features is no longer treated as a black box. An in-depth study was conducted on the properties of \gls{cnn} features of the offline pre-trained model for the task of classification of ImageNet dataset~\cite{deng2009imagenet}. It was found out, as suggested above, that characterization from different perspectives is provided by convolutional layers at different levels.

% ------------------------------------------------------------------------------
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.13\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/theoretical_foundations/fully_cnn_tracking_feature_maps_1.pdf}
        \caption[]{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.39\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/theoretical_foundations/fully_cnn_tracking_feature_maps_2.pdf}
        \caption[]{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.39\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/theoretical_foundations/fully_cnn_tracking_feature_maps_3.pdf}
        \caption[]{}
    \end{subfigure}
    \caption[Fully convolutional tracking]{\imgpartdesc{a} Input image with an associated ground truth mask. \imgpartdesc{b} Visualization of feature maps from convolutional layers placed at the bottom of the model, capturing differences between foreground and background of a particular object instance. \imgpartdesc{c} As opposed to the previous group of images, a more holistic, abstract view on the object category itself is provided by feature maps from top convolutional layers. The top row in the \imgpartdesc{b} and \imgpartdesc{c} represents feature maps, whereas the bottom row represents the corresponding saliency map with spatial information of the category. \externalsrc{\cite{wang2015votcnn}}}
    \label{fig:FullyCNNTrackingFeatureMaps}
\end{figure}
% ------------------------------------------------------------------------------

Authors of~\cite{wang2015votcnn} put together a list of three observations that summarize properties of the fully convolutional nature of a tracker proposed by them.
\begin{itemize}
    \item Despite a large receptive field of \gls{cnn} feature maps, few of them are activated and they are sparsely distributed and localized.
    \item The majority of the feature maps can be considered noisy or irrelevant when discriminating a specific target object (foreground) from the background.
    \item Different layers encode different types of features (related to the intraclass or interclass variations discussed at the beginning).
\end{itemize}

% ##############################################################################
\subsection{Tracking Using Siamese Networks}
\label{ssec:TrackingUsingSiameseNetworks}

Although \glspl{cnn} condense valuable visual information into low dimensional space, it is still not sufficient in many situations during object tracking. The object representation from convolutional layers trained on image classification is not robust enough for dramatic visual changes and occlusion of varying intensity. As we discussed in \sectiontext{}~\ref{sec:LatentSpacesAndEmbeddings} dedicated to metric spaces and embeddings, an object representation supporting \gls{reid} requires different types of models, \ietext{}, a \emph{Siamese neural network} (\sectiontext{}~\ref{ssec:SiameseAndTripletNetworks}). We have already mentioned our intention of utilizing custom metric space for tracking, and~\cite{bertinetto2016siamfc} were among the first ones to successfully demonstrate it.

Authors of~\cite{bertinetto2016siamfc} approved of the idea that visual feature extraction using \glspl{cnn} is pertinent to the robustness of the tracking algorithm, yet they advocated to train the visual model to a more general task of similarity learning rather than just classification. This observation and its further implementation was the main contribution of their work, achieving \gls{sota} performance back then. Broadly speaking, they trained a \emph{fully convolutional} Siamese network to locate an \emph{exemplar} (also referred to as a \emph{template} or \emph{target} in the literature, but for clarity, we will stick to \emph{exemplar} only) image within a larger \emph{search} image (\figtext{}~\ref{fig:FullyCNNSiamTrackingArch}). The model got the name \gls{siamfc}. We mentioned this to make the comparison easier because a lot of follow-up works have been done, producing models such as \gls{sasiam}~\cite{he2018twofoldsiam}, \gls{siamrpn}~\cite{li2018siamrpn}, \gls{siammask}~\cite{wang2019siammask}, \gls{siammaske}~\cite{chen2019rotbboxes}, and so forth.

Let $\gamma$ be a transformation that extracts visual features from the input, and $g$ be the function that combines two representations produced by the function $\gamma$. Siamese networks apply an identical transformation $\gamma$ to both inputs, search image $x$ and exemplar image $z$, and combine the result as
\begin{equation}
    \func{f}{x, z} = \func{g}{\func{\gamma}{x}, \func{\gamma}{z}}.
\end{equation}
As such, when trivial distance or similarity measure is computed by the function $g$, then $\gamma$ can be deemed as the already introduced embedding. The output of computing the cross-correlation operation is the response (confidence) map. During the training, however, the ground-truth response map is centered at the target object and the cosine function is used to create a $2$D penalty map. The reason is to focus the response to the center under the assumption that the object is always focused in the center of the current search region. In practice, this assumption holds most of the time, that's why adoption of the so-called \emph{cosine window} is prevalent in Siamese trackers (\figtext{}~\ref{fig:CosineWindow}).

% ------------------------------------------------------------------------------
\begin{figure}[!t]
    \centerline{\includegraphics[width=0.9\linewidth]{figures/theoretical_foundations/fully_cnn_siam_tracking_architecture.pdf}}
    \caption[\Gls{siamfc} architecture]{The fully convolutional Siamese architecture produces a scalar-valued score map. The similarity function is computed for all sub-windows within the search image and stored in a $2$D score map, rather than just a pure $1$D embedding vector. This computation requires only one evaluation. In this image, the red, green and yellow pixels in the output score map represent similarity values for the three sub-windows on the input.}
    \label{fig:FullyCNNSiamTrackingArch}
\end{figure}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\begin{figure}[!t]
    \centerline{\includegraphics[width=0.8\linewidth]{figures/theoretical_foundations/cosine_window.pdf}}
    \caption[Cosine window]{A visualization of $1$D (left) and $2$D (right) cosine window under the assumption that the response map is of size $17 \times 17$. Such a mask conveys the idea of putting the highest weight to the center with nonlinear, even reduction when moving away from the center.}
    \label{fig:CosineWindow}
\end{figure}
% ------------------------------------------------------------------------------

A team of authors in~\cite{he2018twofoldsiam} made the following observation: features learned in an image classification task (denoted as semantic features) complement features learned in a similarity matching task (denoted as appearance features). They also suitably commented that the key to designing a high-performance tracker is to utilize expressive features that are simultaneously discriminative and generalized. In light of this, they developed a model consisting of a semantic and an appearance branch, with each branch being represented by a standard similarity-learning Siamese network (as in \gls{siamfc}~\cite{bertinetto2016siamfc}). However, an important distinction is that these two branches were trained separately, making them effectively heterogeneous to avoid any sharing of information. They reported that both branches were less powerful when trained jointly than when trained separately. The reasoning was that each branch provides different features produced at different levels of abstraction, yet they complement each other. The merge of their respective outputs happens only during the testing time. Nowadays, joint training is prevalent, especially due to its effectiveness. Given the advantage of hindsight, there are more important aspects of Siamese trackers to address to reap even greater benefits in terms of accuracy, for example, feature fusion.

The \gls{sasiam} receives an input as a pair of image patches cropped from the initial (exemplar) frame and the current (search) frame. Let $z$, $z^s$ and $X$ be the image of exemplar, exemplar including the surrounding context and the search region, respectively. Dimensions of $x^s$ and $X$ are identical, $W_s \times H_s \times 3$. Dimensions of the exemplar $z$ located in the exact center of the region of $z^s$ are $W_t \times H_t \times 3$, such that $W_t < W_s$ and $H_t < H_s$. The appearance branch (\gls{sasiamanet}) takes $\rbrackets{z, X}$ as input and essentially clones the entire \gls{siamfc} network. Let $\func{f_a}{\cdot}$ denote the visual features extracted by the \gls{sasiamanet}. Then, the response map of this branch is given by
\begin{equation}
    \label{eq:SiameseCrossCorrelation}
    \func{h_a}{z, X} = \func{corr}{\func{f_a}{z}, \func{f_a}{X}},
\end{equation}
where $\func{corr}{\cdot}$ is the correlation operation. The training of this branch is rather straightforward thanks to the abundance of training pairs $\rbrackets{z_i, X_i}$ with their related response map $Y_i$, for $i = 1, \dots, N$, with $N$ being the no. of chosen training samples. Analogically, the semantic branch (\gls{sasiamsnet}) assumes as input a pair $\rbrackets{z^s, X}$. Contrary to the \gls{sasiamanet}, this model is pre-trained for the image classification task and its weights are frozen during the training phase. These last two convolutional layers of this model are of primary interest as their features provide abstraction at distinct levels. However, spatial resolutions are not alike. Let $\func{f_s}{\cdot}$ be the concatenated multilevel features. For the correlation operation ($\func{corr}{\cdot}$) to be usable, a special fusion module is introduced, implemented by a simple $1 \times 1$ convolution layer. The fusion operation is applied to features within the same layer, and this fused feature vector will be referred to as $\func{g}{\func{f_s}{X}}$.

Semantic features of a higher level are robust to appearance variation. This contributes to the generalization ability of the tracker but exacerbates its discriminative abilities. To circumvent this, the attention module is presented. The reasoning is that individual feature channels have varying importance for object tracking as far as different exemplars are concerned. The goal is then to assign a degree of importance (weight) to each channel for each exemplar. Still, the exemplar information is not sufficient, so the context must be supplied, too. The proposed attention module thus processes the feature map of $z^s$ instead of just $z$. The attention module operates channel-wise and incurs negligible computational overhead as it's only active during the target processing on the first frame. Later on, the weight coefficient is used to scale each feature map according to its importance.

% ------------------------------------------------------------------------------
\begin{figure}[t]
    \centerline{\includegraphics[width=0.6\linewidth]{figures/theoretical_foundations/twofold_siamese_net_attention_module.pdf}}
    \caption[\gls{sasiamsnet} attention module]{The attention module of the \gls{sasiamsnet} network. \externalsrc{\cite{he2018twofoldsiam}}}
    \label{fig:TwofoldSiameseNetAttentionModule}
\end{figure}
% ------------------------------------------------------------------------------

When training the \gls{sasiamsnet} branch, only the fusion and the attention modules are updated. No fine-tuning techniques are taken advantage of, regardless of the potential improvement of the semantic branch alone. Authors informed about such experiments, and they resulted in diminished overall performance thanks to \gls{sasiamanet} and \gls{sasiamsnet} becoming less heterogeneous. The inference phase involves computation of the overall heat map for which a weighted average of the two produced heat maps.

The series of Siamese-based architectures for tracking continued with the idea of using the \gls{rpn}~\cite{li2018siamrpn} (see \sectiontext{}~\ref{ssec:FasterRCNN} for the same concept applied in object detection). Under the flag of end-to-end training, the \gls{siamrpn} model consists of a Siamese subnetwork for feature extraction (again, a duplicate of the \gls{siamfc}~\cite{bertinetto2016siamfc}) and \gls{rpn} as another subnetwork encompassing both classification and regression branch (\figtext{}~\ref{fig:SiamRPNNetArchitecture}). The notable contribution is that the proposed framework is formulated as a local one-shot detection task in the inference phase (the first work to make such a step). The template branch encodes the object appearance information for further foreground/background discrimination. Analogically, the \gls{bbox} from the first frame is the only exemplar for one-shot detection in the inference phase.

The region proposal subnetwork contains a pair-wise correlation section as well as a supervision section. Let $k$ denote the number of anchors. Then, the model has to output $2k$ channels for the classification and $4k$ channels for the regression. Following the established notation, the Siamese subnetwork produces feature maps $\func{\gamma}{z}$ and $\func{\gamma}{x}$. The pair-wise correlation splits $\func{\gamma}{z}$ into $\sbrackets{\func{\gamma}{z}}_{cls}$ and $\sbrackets{\func{\gamma}{z}}_{reg}$ while increasing the no. of channels (\figtext{}~\ref{fig:SiamRPNNetArchitecture}). Conversely, $\func{\gamma}{x}$ is also split into $\sbrackets{\func{\gamma}{x}}_{cls}$ and $\sbrackets{\func{\gamma}{x}}_{reg}$, but the no. of channels remains unchanged. The correlation, when computed on both branches, is given by
\begin{equation}
    \begin{aligned}
        \subsup{A}{w \times h \times 2k}{cls} & =
        \sbrackets{\func{\gamma}{x}}_{cls} \star \sbrackets{\func{\gamma}{z}}_{cls}, \\
        \subsup{A}{w \times h \times 4k}{reg} & =
        \sbrackets{\func{\gamma}{x}}_{reg} \star \sbrackets{\func{\gamma}{z}}_{reg},
    \end{aligned}
\end{equation}
where the template feature maps $\sbrackets{\func{\gamma}{z}}_{cls}$ and $\sbrackets{\func{\gamma}{z}}_{reg}$ stand in place of kernels in the convolution operation signified by the $\star$ character.

% ------------------------------------------------------------------------------
\begin{figure}[t]
    \centerline{\includegraphics[width=\linewidth]{figures/theoretical_foundations/siam_rpn_architecture.pdf}}
    \caption[\Gls{siamrpn} architecture]{The pipeline starts with the original \gls{siamfc} network followed by the \gls{rpn} which has two branches: classification and regression. The output of the two branches is obtained using a pair-wise correlation. Foreground/background classification and the box regression are given by the $17 \times 17 \times 2k$ and $17 \times 17 \times 4k$ feature maps, respectively. \externalsrc{\cite{li2018siamrpn}}}
    \label{fig:SiamRPNNetArchitecture}
\end{figure}
% ------------------------------------------------------------------------------

The noteworthy formulation of tracking as one-shot detection was proposed as follows. In general terms, the goal is to minimize the average loss $\mathcal{L}$ of a predictor function $\func{\psi}{x; W}$ by finding its parameters $W$. When computed over a dataset of $N$ samples $x_i$ with corresponding labels $y_i$, $\forall i = 1, \dots, N$, it is given by
\begin{equation}
    \label{eq:SiamRPNOneShotGeneral}
    \underset{W}{\argmin}
    \cbrackets{
        \frac{1}{N}
        \sum_{i = 1}^{N}
        \func{\mathcal{L}}{
            \func{\psi}{x_i; W},
            y_i
        }
    }.
\end{equation}
\emph{One-shot learning} aims to learn $W$ when only a single exemplar $z$ is available, tackling a major challenge of \emph{learning to learn}~\cite{bertinetto2016oneshot}. If we consider a meta-learning feed-forward function $\omega$ that maps $\rbrackets{z_i; W'}$ to $W$, then the problem can be stated as
\begin{equation}
    \label{eq:SiamRPNOneShotMetaLearning}
    \underset{W'}{\argmin}
    \cbrackets{
        \frac{1}{N}
        \sum_{i = 1}^{N}
        \func{\mathcal{L}}{
            \func{\psi}{x_i; \func{\omega}{z_i, W'}},
            y_i
        }
    }.
\end{equation}
In this setting, this objective function can be re-written in terms of the Siamese subnetwork feature extraction $\gamma$ and region proposal subnetwork $\Psi$ as
\begin{equation}
    \label{eq:SiamRPNOneShoCombo}
    \underset{W}{\argmin}
    \cbrackets{
        \frac{1}{N}
        \sum_{i = 1}^{N}
        \func{\mathcal{L}}{
            \func{\Psi}{\func{\gamma}{x_i; W}; \func{\gamma}{z_i; W}},
            y_i
        }
    }.
\end{equation}
The template branch provides training parameters to predict the kernel for the detection task. The template branch embeds necessary category information into the kernel that is subsequently utilized for detection.

Later on, a fork of publications emerged with an endeavor to improve the tracking performance by estimating not only a regular axis-aligned \gls{bbox} but a rotated box, too. Put into perspective, the rotated \gls{bbox}, as opposed to an ordinary, axis-aligned, contains the minimal amount of background pixels~\cite{chen2019rotbboxes}. Thus, datasets with rotated \glspl{bbox} provide tighter enclosed rectangles.

Inspiration from object segmentation yielded another approach where the tracking process was assisted with additional semi-supervised object segmentation~\cite{wang2019siammask}. The relevant contribution is the augmentation of the training loss with a binary segmentation task. Further, once trained, the model (dubbed \gls{siammask}) relies exclusively upon a single \gls{bbox} initialization and operates online while producing rotated \glspl{bbox} instead of axis-aligned ones together with class-agnostic object segmentation masks.

Again, the \gls{siamfc}~\cite{bertinetto2016siamfc} served as the fundamental building block. However, a notable modification consisted of the use of a depth-wise cross-correlation layer instead of a simple cross-correlation layer. The latter compresses all the information into a single channel, impeding the potential to encode richer information about the object. As a reminder, the original model used $6 \times 6 \times 128$ and $22 \times 22 \times 128$ tensors to produce a $17 \times 17 \times 1$ response map (\figtext{}~\ref{fig:FullyCNNSiamTrackingArch}). Here, a multi-channel response maps are utilized (\figtext{}~\ref{fig:SiamMaskArchitecture}).

% ------------------------------------------------------------------------------
\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.68\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/theoretical_foundations/siam_mask_architecture_3_branch.pdf}
        \caption[]{}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/theoretical_foundations/siam_mask_architecture_2_branch_head.pdf}
        \caption[]{}
    \end{subfigure}
    \caption[\Gls{siammask} architecture]{Illustration of the two \gls{siammask} variants: \imgpartdesc{a} a three-branch architecture (full), \imgpartdesc{b} a two-branch architecture (head), both with depth-wise correlation layers. \externalsrc{\cite{wang2019siammask}}}
    \label{fig:SiamMaskArchitecture}
\end{figure}
% ------------------------------------------------------------------------------

An incremental improvement of \gls{siammask} model came when~\cite{chen2019rotbboxes} proposed a novel, efficient algorithm for the estimation of the \gls{bbox} rotation when the object segmentation mask is given. Particularly, a mask produced by the \gls{siammask} model, as this work builds on top of~\cite{wang2019siammask}, under the derived name \gls{siammaske}. In addition, their approach can be used to generate a rotated box ground truth from any segmentation datasets to train a rotation angle regression model. To estimate the rotation angle, they adopted the least-squared scheme as part of the ellipse fitting algorithm.

The idea to employ fully convolutional networks seems to pertain to the modern computer vision community. Besides a simpler model, the fully convolutional design often leads to a reduced number of hyperparameters. One such an architecture (a descendant of the famous \gls{siamfc}~\cite{bertinetto2016siamfc} model) has been recently proposed, named \gls{siamcar}~\cite{guo2019siamcar}. This approach relies on the decomposition of the task of \gls{vot} into two subproblems: classification for pixel category and regression for object \gls{bbox} at the given pixel. The leading concept of the article is that this tracker operates in an end-to-end, per-pixel manner. The authors managed to avoid the use of anchors as well as region proposals, hence reducing the need for human intervention. The use of the two aforementioned traits commonly leads to sensitivity to dimensions and aspect ratios of the anchor boxes, which requires expertise in hyperparameter tuning.

% ------------------------------------------------------------------------------
\begin{figure}[t]
    \centerline{\includegraphics[width=\linewidth]{figures/theoretical_foundations/siamcar_architecture.pdf}}
    \caption[\Gls{siamcar} architecture]{\Gls{siamcar} architecture. The left side consists of the original \gls{siamfc}~\cite{bertinetto2016siamfc} model, with a simple amendment of using depth-wise correlation for multi-channel response map extraction. The right side depicts the subnetworks for foreground/background classification and \gls{bbox} regression. \externalsrc{\cite{guo2019siamcar}}}
    \label{fig:SiamCARArchitecture}
\end{figure}
% ------------------------------------------------------------------------------

An indispensable part of localization is low-level features like edges, corners, and so on, whereas high-level features strengthen the representational power from the semantic point of view, which is crucial for discrimination. Authors fused low-level and high-level features from the last $3$ residual blocks of the \gls{resnet}-50 backbone, forming a unity after concatenation.

An important observation was made that locations further away from the object center may aggravate the predicted box as they can be considered low-quality. To diminish the effect of such locations, another branch alongside the classification branch to suppress the outliers is introduced, based on the concept of \emph{centerness}, borrowed from the~\cite{tian2019fcos}. This branch outputs a feature map where each point indicates the \emph{centerness} score for the corresponding location. We can say that \emph{centerness} is a very general concept, and practically it represents a weighting mechanism to penalize areas within the \gls{roi} that most likely do not contain the target object.

\subsubsection{Conclusions Made In the Survey Paper}

We would like to remark that this branch of trackers formed the basis of our research. Its importance reached such a high level that we even composed an up-to-date comprehensive survey paper~\cite{ondrasovic2021siamese} solely focused on Siamese trackers and their fundamentals. Due to space limitations, in this short section, we summarize the most important conclusions and observations that we made in the aforementioned paper.

In the referred survey, we aimed to identify and elaborate on the most significant challenges the Siamese trackers face. The objective was to answer what design decisions the authors had made and what problems they had attempted to address. That treatise could be thought of as an in-depth analysis of the core principles on which Siamese trackers operate together with a discussion of the underlying motivation. In addition, we also provided an up-to-date qualitative and quantitative comparison of the prominent Siamese trackers on established benchmarks, since the last survey that involved a thorough discussion of Siamese trackers was published in $2018$~\cite{pflugfelder2018indepth}. Last but not least, we discussed the current trends in developing Siamese trackers at the time of writing the article as well.

We have to emphasize that Siamese trackers are a research direction in \gls{vot} with great potential. In practical terms, they belong to the fastest trackers with the ``accuracy-to-speed'' ratio being their primary strength. Contrary to the initial expectation, we realized that fast trackers were also among the most accurate ones (with some existing exceptions). Simply put, high processing speed is an inherent property of Siamese trackers. Nevertheless, there are existing drawbacks that require research attention. The presence of distractors (similar interference) is in our paper often mentioned as one of the leading causes of problems for this type of trackers. Our quantitative evaluations indicate that trackers where the presence of semantic background is explicitly treated often yield the top performance. Siamese metric learning is powerful enough to encompass numerous visual variations, but in case there are distractors present, then additional steps conditionally executed seem to contribute positively. To name a few, there are the explicit distractor-awareness~\cite{zhu2018dasiamrpn}, custom sampling strategies for foreground/background discrimination~\cite{li2020figsiam}, or conditional object re-detection~\cite{li2019siamrm}. Besides, there are a plethora of examples where \gls{rpn} was used for object proposals even for \gls{sot} in Siamese trackers. We venture to claim that the top-performing trackers exploited the above-mentioned \gls{rpn} head, \egtext{}, \cite{li2018siamrpn, zhu2018dasiamrpn, li2018siamrpnpp}. A comprehensive survey concerning deep visual object tracking by Marvasti-Zadeh~\etal{}~\cite{marvastizadeh2021survey} also reached a similar conclusion.

The utilization of cross-correlation has a great share of the leading performance in terms of its effectiveness. But the original single-channel formulation from~\cite{bertinetto2016siamfc} has been improved into a multi-channel, depthwise cross-correlation that has been in use up to date. It was argued that a single channel did not capture sufficient information~\cite{guo2019siamcar}, thus multi-channel cross-correlation layers were used instead~\cite{li2018siamrpnpp}. On top of that, since multiple channels are present, we observed an emerging trend in using various attention mechanisms to aid the feature selection~\cite{wang2018learningattentions}.

Speaking of cross-correlation in terms of its core principle of performing a ``learned template matching'' using the exemplar and the search region, it raises the question of whether and how the exemplar template should change during the training. Several works have remarked that incorporating memory or template updating strategies could potentially enhance the tracker performance, e.g,~\cite{bertinetto2016siamfc, liang2019lssiam}. It seems that relying solely upon the exemplar image from the initial frame may have detrimental effects as the object undergoes severe visual deformations, so the tracker may eventually lose its track.

Even though our discussed survey focused on \gls{sot}, there are emerging works where Siamese architectures were integrated into a \gls{mot} pipeline.
