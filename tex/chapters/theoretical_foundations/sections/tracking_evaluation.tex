\section{Evaluating Visual Multiple Object Tracking}
\label{sec:EvaluatingMOT}

When it comes to evaluating \gls{mot} performance, there is still no consensus on how to approach the evaluation and subsequent comparison of multiple-object trackers. There is one established metric, however, called \gls{clear} metric~\cite{bernardin2008clearmot}, that we will employ to quantitatively assess the performance of a \gls{mot} system. The reasons are:
\begin{itemize}
    \item This metric is still considered a reasonably effective and intuitive metric to use, despite multiple proposals for improvements~\cite{wen2020uadetrac}.
    \item Numerous works in object tracking, especially tracking of people, report statistics from the \gls{mot} challenges that historically have utilized this metric.
    \item There exist libraries (\egtext{},~\cite{webpymotmetrics}) allowing an evaluation of a \gls{mot} tracker inference.
\end{itemize}

Bernardin~\etal{}~\cite{bernardin2008clearmot}, the authors of the \gls{clear} metric, designed few criteria that performance metrics should meet. Therefore, a useful metric:
\begin{enumerate}
    \item allows assessing the tracker's precision regarding how well it is capable of determining the exact object location,
    \item reflects the tracker's ability to track objects consistently, \ietext{}, to correctly trace object trajectories such that one and only one trajectory is established per object,
    \item has as few free parameters as possible,
    \item is clear and easy to interpret while emphasizing an intuitive human understanding of the tracking process,
    \item is general enough so that comparison of different types of trackers is possible,
    \item contains expressive values rich in information yet not abundant in quantity.
\end{enumerate}

Let $t$ denote a time for a specific frame. For each frame $t$, the multiple-object tracker produces a set of hypotheses $\cbrackets{h_1, h_2, \dots, h_m}$ for a set of visible objects $\cbrackets{o_1, o_2, \dots, o_n}$. The evaluation procedure can be briefly described in the following pseudocode.

For each time frame $t$:
\begin{enumerate}
    \item Establish the best possible correspondence between hypotheses $h_i$ and objects $o_j$, where $i = 1, 2, \dots m$ and $j = 1, 2, \dots, n$.
    \item For each determined correspondence between object and hypothesis:
          \begin{enumerate}
              \item quantify the error in estimation of the object's position.
          \end{enumerate}
    \item Perform accumulation of all errors (\figtext{}~\ref{fig:CLEARHypotheses}) in the found correspondences:
          \begin{enumerate}
              \item count false negatives (misses), \ietext{}, objects without assigned hypothesis,
              \item count ala false positives, \ietext{}, hypotheses for which there was no object,
              \item count mismatch errors (swaps of object IDs), \ietext{}, situations in which the hypothesis for a given object changed compared to the previous frame.
          \end{enumerate}
\end{enumerate}

% ------------------------------------------------------------------------------
\begin{figure}[t]
    \centerline{\includegraphics[width=0.55\linewidth]{figures/theoretical_foundations/clear_hypotheses_status.pdf}}
    \caption[\gls{clear} hypotheses]{With a demonstration of a correct tracker inference at the top, the \gls{clear} metric distinguishes between three fundamental types of errors, misses (false negatives), false positives and ID switches, shown in this order respectively. \externalsrc{\cite{bernardin2008clearmot}}}
    \label{fig:CLEARHypotheses}
\end{figure}
% ------------------------------------------------------------------------------

% ##############################################################################
\subsection{Establishing Correspondences}

The correspondence between a hypothesis $h_i$ and an object $o_j$ should not be made unless their distance (denoted as $d_{i,j}$) is within a specific threshold $T$. The measure of distance has to be defined for each task, but the \gls{iou} distance or Euclidean distance of \gls{bbox} centroids are most commonly used. From now on, we define object-hypothesis correspondence to be valid as long as $d_{i,j} < T$.

The value of $T$ is critical and greatly influences the outcome. Evaluating tracking performance bears the burden of having parameters that are difficult to generalize and the process of setting their values is often accompanied by experimentation. For example, conceptually speaking, there is, by all means, a boundary (the threshold $T$) beyond which we can no longer speak of an error in position estimation, but we should rather claim that the tracker has drifted away and is tracking a completely different object.

% ##############################################################################
\subsection{Tracking Consistency}

To properly examine the tracker in terms of how consistent it is at tracking objects, one has to detect conflicting predictions for the given object over time. Bernardin~\etal{}~\cite{bernardin2008clearmot} remarked that such procedures need to decide what the ``best'' mapping is. For instance, assuming an object $o_j$ and a hypothesis $h_i$, the ``optimal'' matching may be based on the initial correspondence made for $o_j$ or the most frequent correspondence made throughout the whole sequence. If any violation is encountered, it is then treated as a discrepancy.

However, there are several issues. Consider scenarios depicted in \figtext{}~\ref{fig:SeqLevelMostCommonCorrespondenceProb}. The authors raised their concerns regarding the objectivity of such evaluation and proposed a slightly different method. They only count mismatch errors once at the time frame where the change occurs and consider the remaining intermediate correspondences as correct.

Let $M_t = \cbrackets{\rbrackets{h_i, o_j}}$ be the set of mappings made up to time $t$, such that $M_0 = \cbrackets{\cdot}$. Once a new correspondence is made at the next step at time $t + 1$ between the hypothesis $h_k$ and the object $o_j$ that conflicts the already established identity by the pair $\rbrackets{h_i, o_j}$ in $M_t$, this contradition is then counted as a mismatch error and $\rbrackets{h_i, o_j}$ is replaced by $\rbrackets{h_k, o_j}$ in $M_{t + 1}$. Consequently, mapping that is constructed this way enhances decision-making when facing multiple competing hypotheses for the same object. The implicit assumption is that the previously assigned hypothesis is more likely to be correct that the new one, even if the distance metric alone would indicate otherwise (\figtext{}~\ref{fig:ObjectHypothesisReInit}).

% ------------------------------------------------------------------------------
\begin{figure}[t]
    \centerline{\includegraphics[width=0.5\linewidth]{figures/theoretical_foundations/seq_based_correspondence_issues.pdf}}
    \caption[Sequence-based correspondence mismatches]{Illustration of the inherent ``unfairness'' when relying on sequence-level ``best'' object-hypothesis mapping induced by the most frequent correspondence. As shown in the case $1$, the correct hypothesis is the $h_2$, and thus only $2$ errors are incurred for the first mismatch. The case $2$ is practically identical, the $h_2$ also represents the most common assignment. However, $4$ errors are accumulated for the alleged mismatch for $h_1$. \externalsrc{\cite{bernardin2008clearmot}}}
    \label{fig:SeqLevelMostCommonCorrespondenceProb}
\end{figure}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\begin{figure}[t]
    \centerline{\includegraphics[width=0.5\linewidth]{figures/theoretical_foundations/object_hypothesis_reinit.pdf}}
    \caption[Object-hypothesis re-initialization]{Track reinitialization. At time $t$, the identity of the object $o_1$ is accounted for by the hypothesis $h_1$. At time $t + 1$, the object disappears and the track is temporarily lost. At time $t + 2$, the tracker is responsible for reinstantiating the object identity. During evaluation, the underlying assumption is that the previous hypothesis should be the correct one, even if the new hypothesis is closer according to the used distance function. \externalsrc{\cite{bernardin2008clearmot}}}
    \label{fig:ObjectHypothesisReInit}
\end{figure}
% ------------------------------------------------------------------------------

% ##############################################################################
\subsection{Mapping Procedure}

Let $M_0 = \cbrackets{\cdot}$. For each time frame $t$:
\begin{enumerate}
    \item Verify if mappings $\rbrackets{h_i, o_j}$ in $M_{t - 1}$ are still valid. A pair is deemed valid as long as the hypothesis $h_i$ exists at time $t$, the object $o_j$ is still visible while the distance between the two does not exceed $T$. If these conditions hold, establish a correspondence.
    \item If there are objects for which no correspondence has been made so far, then a suitable matching hypothesis is searched for. This step involves one-to-one matching for pairs the distance of which does not exceed the threshold $T$. The matching procedure is formulated as a minimum cost assignment problem. In case there happens to be a correspondence that contradicts a mapping $\sbrackets{h_i, o_j}$ as part of $M_{t - 1}$, then replace the previous pair $\sbrackets{h_i, o_j}$ with $\sbrackets{h_k, o_j}$ and treat such an occurrence as a mismatch error. For simplicity, let $mme_t$ be the number of the mismatch errors for the frame $t$.
    \item The two previous steps guarantee that a complete set of matching pairs has been generated for the current time $t$. At this point, we may start calculating values that will be utilized later for computing the final metrics. So, let $c_t$ be the number of matches found for time $t$. For each such match, compute the distance between the object $o_j$ and the corresponding hypothesis, denoted by $d_{ti}$.
    \item Every hypothesis that is not part of any pair up to this point is reckoned as false positive. Likewise, all the remaining objects are marked as misses. Thus, let $fp_t$ and $m_t$ be the number of false positives and misses, respectively. For future reference, let us define $g_t$ as the number of ground-truth objects visible at time $t$.
\end{enumerate}

% ##############################################################################
\subsection{Performance Metrics}

In light of the previously described procedure, here we present the two most relevant performance metrics by which the tracking performance can be intuitively expressed, namely the ``tracking precision'' and ``tracking accuracy''.

The role of tracking precision is to measure the alignment between the predicted object position and the ground-truth position only for the positive sample. For that reason, precision is not influenced by the ability (or lack thereof) of the tracker to detect objects. Therefore, the \gls{motp} metric can be defined as
\begin{equation}
    \label{eq:MOTPMetric}
    \text{MOTP} = \frac{\sum_{\forall t} \sum_{\forall i} d_{ti}}{\sum_{\forall t} c_t},
\end{equation}
representing the total error in the estimated position for the pairs where the object-hypothesis relationship was correctly determined averaged over the total number of such matches made.

Conversely, the accuracy metric attempts to reflect the number of mistakes the tracker made in terms of misses, false positives, object mismatches. Given this description, the \gls{mota} metric can be expressed as
\begin{equation}
    \label{eq:MOTAMetric}
    \text{MOTA} = 1 - \frac{\sum_{\forall t} \rbrackets{m_t + fp_t + mme_t}}{\sum_{\forall t} g_t},
\end{equation}
the possible values of which lie within the interval $\sbrackets{-\infty, 1}$.

We would like to emphasize that the errors have to be first summed up across all the frames before computing the ratios rather than evaluating the ratio locally. Independent computation of the given ratios would lead to non-intuitive outcome (\figtext{}~\ref{fig:LocalGlobalRatioEval}).

% ------------------------------------------------------------------------------
\begin{figure}[t]
    \centerline{\includegraphics[width=0.6\linewidth]{figures/theoretical_foundations/local_vs_global_ratio_evaluation.pdf}}
    \caption[Local vs. global ratio evaluation]{Computing of error ratios needs to be performed on a global level, rather than on a local, frame level. Assume a sequence consisting of $8$ frames. Moreover, assume that objects $o_1, \dots, o_4$ are visible on the frames from $t_1$ to $t_4$, but none of them is being tracked. The situation changes at frame $t_4$ where only the object $o_4$ is being tracked properly by the hypothesis $h_1$. As a result, in frames $t_1, \dots, t_4$, the resulting miss rate is $100\%$, whereas in frames $t_5, \dots, t_8$ it is exactly $0\%$. Applying arithmetic average to these values yields a global miss rate of $\frac{1}{8} \rbrackets{4 \cdot 100 + 4 \cdot 0} = \frac{1}{2}$, or, $50\%$. Conversely, performing summation prior to quantifying the final global ratio produces far more intuitive result of $16$ out of $20$ misses, or the miss rate of $80\%$. \externalsrc{\cite{bernardin2008clearmot}}}
    \label{fig:LocalGlobalRatioEval}
\end{figure}
% ------------------------------------------------------------------------------


% ##############################################################################
\subsection{Other Performance Metrics}

Besides the two primary performance metrics discussed in the previous section, there are also partial metrics that are worth evaluating to get a better grasp of the tracker's performance. For our purposes, we computed metrics outlined in \tabletext{}~\ref{tab:OtherCLEARMetrics}.

\begin{table}[t]
    \centering
    \begin{tabular}{p{0.3\linewidth}p{0.65\linewidth}}
        \toprule
        \textbf{Metric Name}     & \textbf{Description}                                                 \\
        \midrule
        no. of frames            & Total number of frames.                                              \\
        no. of matches           & Total number matches.                                                \\
        no. of switches          & Total number of track ID switches.                                   \\
        no. of false positives   & Total number of false positives (false alarms).                      \\
        no. of misses            & Total number of misses (false negatives).                            \\
        no of detections         & Total number of detected objects including matches and switches.     \\
        no. of objects           & Total number of unique object appearances over all frames.           \\
        no. of predictions       & Total number of unique prediction appearances over all frames.       \\
        no. of fragmentations    & Total number of switches from tracked to not tracked.                \\
        no. of mostly tracked    & Number of objects tracked for at least $80\%$ of lifespan.           \\
        no. of partially tracked & Number of objects tracked with lifespan from $20\%$ to $80\%$.       \\
        precision                & Number of detected objects over sum of detected and false positives. \\
        recall                   & Number of detections over number of objects.                         \\
        IDF1                     & ID measures: global min-cost F1 score.                               \\
        \gls{motp}               & Multiple object tracker precision.                                   \\
        \gls{mota}               & Multiple object tracker accuracy.                                    \\
        \bottomrule
    \end{tabular}
    \caption{Other important \gls{clear} metrics that we adopted for evaluation of our experiments with various \gls{mot} approaches.}
    \label{tab:OtherCLEARMetrics}
\end{table}
