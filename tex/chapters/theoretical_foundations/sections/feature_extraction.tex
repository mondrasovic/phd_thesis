\section{Feature Extraction and Feature Fusion}
\label{sec:FeatureExtractionFusion}

It goes without saying that efficient capabilities of deep learning models of extracting robust features pertinent to the task at hand are of great significance. As we have observed in our survey of Siamese trackers~\cite{ondrasovic2021siamese}, incremental improvements in feature extraction were often the major contribution of numerous works that granted the authors a competitive performance against other \gls{sota} frameworks. With this in mind, we consider feature extraction a necessary part of any deep learning model design. As we will see further, the model with which we performed majority of our experiments exploited top feature extraction approaches. Thus, it is also suitable to provide a brief background to the most influential methods of feature extraction and feature fusion.

\subsection{Residual Neural Network}
\label{ssec:ResidualNeuralNet}

Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers. Typical ResNet models are implemented with double- or triple- layer skips that contain nonlinearities (ReLU) and batch normalization in between.

There are two main reasons to add skip connections: to avoid the problem of vanishing gradients

\subsection{Feature Pyramid Network}
\label{ssec:FeaturePyramidNetwork}

\subsection{Deep Layer Aggregation}
\label{ssec:DeepLayerAggregation}
